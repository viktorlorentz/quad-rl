method: bayes
metric:
  goal: minimize
  name: position_tracking
parameters:
  total_timesteps:
    distribution: int_uniform
    min: 1000000
    max: 5000000
  policy_kwargs:
    parameters:
      activation_fn:
        distribution: categorical
        values:
          - ReLU
          - Tanh
      net_arch:
        distribution: categorical
        values:
          - {"pi": [64, 64], "vf": [64, 64]}
          - {"pi": [128, 128], "vf": [128, 128]}
          - {"pi": [64, 64, 64], "vf": [64, 64, 64]}
  reward_coefficients:
    parameters:
      alive_reward:
        distribution: uniform
        min: 4.0
        max: 20.0
      linear_velocity:
        distribution: uniform
        min: 0.0
        max: 7.0
      distance:
        distribution: uniform
        min: 5.0
        max: 30.0
      velocity_towards_target:
        distribution: uniform
        min: 0.0
        max: 10.0
program: scripts/train_agent.py
