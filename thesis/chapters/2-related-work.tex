\chapter{Related Work}
This chapter reviews prior work on the control of multirotor \gls{uav}s transporting cable-suspended payloads. We organize the discussion by first examining traditional model-based control strategies, which laid the foundation for understanding the dynamics and cooperative planning. Building upon these classical approaches, we then highlight recent advances in \gls{rl} for single-\gls{uav} control and payload transport, before considering simulation frameworks that enable high-throughput algorithm development. Finally, we turn to \Gls{marl} \autocite{littman1994markov}, which offers decentralized learning solutions for multi-\gls{uav} swarm coordination and collaborative payload transport. A recent survey by \cite{estevez_review_2024} provides a comprehensive overview of various methods for aerial cable-suspended transport with quadrotors, which we build upon here.

\section{Traditional Model-Based Control for Multi-\gls{uav} Payload Transport}
Early work on cooperative payload transport relied on physics-based models and carefully designed feedback controllers. These approaches provided essential insights into how multiple \gls{uav}s interact with a shared cable-suspended payload, forming the baseline for later \gls{rl}-based and \gls{marl} methods.

Early work formulated the coupled dynamics of multiple quadrotors and their suspended payload, enabling coordinated planning and stabilization beyond quasi-static assumptions \cite{sreenath_dynamics_2013}. Similarly, \cite{villa_cooperative_2021} proposed a virtual-structure formation with adaptive dynamic compensators, achieving robust rod-shaped payload positioning under modeling errors and wind disturbances, critically, without requiring inter-\gls{uav} communication. These works demonstrate that carefully derived models and controllers can ensure stable transport, but they also highlight the complexity of explicitly modeling every disturbance and interaction.

Building on these foundational models, \cite{lee_geometric_2018} derived full rigid-body dynamics on nonlinear configuration spaces and designed controllers that guarantee payload pose tracking with provable stability. In parallel, \cite{tognon_aerial_2018} showed that internal cable tension enables decentralized stabilization of any payload orientation, whereas zero-tension schemes render the payload's attitude uncontrollable. To handle parameter uncertainties, \cite{tagliabue_robust_2017} shaped each drone's apparent dynamics, demonstrating stable collaborative transport even under worst-case conditions. These contributions collectively illustrate that traditional controllers can achieve robust performance, but they often require precise knowledge of vehicle and payload parameters.

More recent work has pushed toward online optimization techniques. \cite{sun_nonlinear_2023} developed a \gls{mpc} framework that plans full six degree of freedom payload trajectories while respecting thrust, collision, and cable tension limits for teams of up to ten \glspl{uav}. In a similar direction, \cite{li_nonlinear_2023} optimized payload trajectories and inter-drone spacing within an \gls{mpc} framework, validating performance on both simulation and experiments. To address real-time computational demands, \cite{jackson_scalable_2020} introduced a distributed trajectory optimization approach, parallelizing computation across agents to enable real-time replanning in cluttered environments. These \gls{mpc}-based methods demonstrate how traditional control can be scaled to larger teams, but they still rely on accurate models and often incur significant computational overhead.

Analyses of equilibrium and stability help inform both controller design and the motivation for learning-based methods. For example, \cite{gabellieri_equilibria_2023} analyzed equilibrium conditions and stability for two-\gls{uav} beam transport without communication, showing that internal forces induced by cable angles enhance attitude robustness under uncertain parameters. \cite{wahba_kinodynamic_2024} integrated sampling-based kinodynamic planning with a dynamics-aware optimizer to navigate \gls{uav} teams through confined spaces, achieving lower tracking errors and reduced energy consumption compared to payload-only planners. Most recently, \cite{Wang2025SafeAA} improved the agility and robustness of \gls{uav} teams carrying cable-suspended payloads, while avoiding dependence on explicit state estimation of the payload. These studies underscore both the progress and limitations of purely model-based approaches, motivating the exploration of learning-based techniques that can adapt online to uncertainties.

\section{\gls{rl} Approaches for \gls{uav} Control}
While traditional model-based controllers offer provable guarantees, they can struggle with highly nonlinear dynamics, unmodeled disturbances, and complex payload interactions. \gls{rl} (\gls{rl}) methods are gaining traction for \gls{uav} control because they can adapt to complex and uncertain dynamics while reducing the need for explicit modeling. In the context of payload transport, \gls{rl} offers the potential to learn control policies that implicitly account for cable dynamics and multi-agent coupling, paving the way toward more flexible and robust systems. We first review general single-\gls{uav} \gls{rl} methods and then focus on sim-to-real \gls{rl} strategies for micro-\gls{uav} platforms.

\subsection{Single-Agent \gls{rl} for \gls{uav} Flight Control}

Initial applications of \gls{rl} to \gls{uav} control focused on single-agent scenarios to validate that learning-based controllers can match or exceed traditional PID and model-based methods. For example, \cite{Koch2018ReinforcementLF} evaluated Deep Deterministic Policy Gradient (DDPG) \cite{Lillicrap2015ContinuousCW}, Trust Region Policy Optimization (TRPO) \cite{Schulman2015TrustRP}, and \gls{ppo} \cite{schulman2017proximal} for inner-loop attitude control in a quadrotor simulation. Their results demonstrated that \gls{rl}-based controllers can achieve stability and responsiveness on par with or better than classical PID controllers. Building on this insight, \cite{Hwangbo2017ControlOA} proposed a neural-network-based controller trained via \gls{rl} to stabilize a quadrotor from harsh initial conditions, like being thrown upside-down. The resulting policy achieved accurate step-response behavior and robust real-world stabilization in both simulation and hardware experiments, showing that \gls{rl} can handle aggressive maneuvers.

In highly dynamic scenarios at the edge of quadrotor agility, \cite{Song2023ReachingTL} compared optimal control and \gls{rl} in autonomous drone racing. They found that a \gls{rl}-based policy could execute aggressive maneuvers near the vehicle's limits and achieve competitive lap times, illustrating that learned policies can capture complex dynamics more effectively than hand-tuned controllers. More recently, \cite{xing_multi-task_2024} developed a multitask \gls{rl} framework enabling a single quadrotor to execute high-speed stabilization, velocity tracking, and autonomous racing tasks using a unified policy, illustrating the versatility and efficiency of learned controllers in complex scenarios. 

These single-agent studies set the stage for extending \gls{rl} methods to payload transport, where the coupling between vehicle and payload introduces additional challenges.

\subsection{Sim-to-Real \gls{rl} for Micro-\gls{uav} Control}
Sim-to-real transfer remains a challenge in \gls{rl} for \gls{uav} platforms. In \cite{molchanov_sim--multi-real_2019} it is demonstrated that domain randomization over physical parameters enables a single policy trained in simulation to generalize zero-shot to quadrotors of varying sizes. Despite these advances, sim-to-real discrepancies under real-world conditions still pose significant challenges in practice, especially for micro-\glspl{uav} like the Crazyflie 2.1, which have low inertia and constrained onboard computation. These limitations have led to the development of specialized \gls{rl} strategies designed to bridge the sim-to-real gap on micro-\glspl{uav}.

Abstracting actions as \gls{ctbr} transfers better than low-level rotor \gls{pwm} or thrust commands \cite{kaufmann_benchmark_2022}. Leveraging such insights, \cite{Eschmann2024} used an asymmetric actor-critic framework, where the training process has access to privileged simulator information, while the deployed policy uses only realistic observations. This approach yielded zero-shot transfer to Crazyflie hardware within seconds of simulated learning. \cite{chen_what_2024} also managed to transfer by combining careful reward shaping and curriculum schedules, stabilizing training, and ensuring safe initial flights on the real quadrotor.

Table~\ref{tab:rl_comparison} summarizes these and other recent \gls{rl}-based approaches for Crazyflie control. The table highlights how observation spaces, action representations, and sim-to-real techniques vary across studies, and how they achieved robust flight control under real-world conditions.
\begin{table*}[!b]
  \centering
  \caption[Summary of \gls{rl} approaches]{Summary of recent \gls{rl} approaches for Crazyflie control. In the table, $\mathbf{p}$ denotes position, $\mathbf{e}$ the position errors, $\mathbf{v}$ velocity, $\mathbf{R}$ a rotation matrix (orientation), and $\boldsymbol{\omega}$ angular velocity. For details, refer to the original paper. We group \gls{pwm} and motor thrust commands as \gls{srt}.}
  \label{tab:rl_comparison}
  \scriptsize
  % full text width with flexible last column
  \rowcolors{2}{gray!20}{white}
  \begin{tabularx}{\textwidth}{p{0.7cm} p{3.0cm} p{1.3cm} p{3.5cm} X}
    \toprule
    \rowcolor{white}
    \textbf{Paper} & \textbf{Observation $\mathcal{O}$} & \textbf{Action $\mathcal{A}$} & \textbf{Reward $r$} & \textbf{Sim-to-Real} \\
    \midrule
    \cite{molchanov_sim--multi-real_2019} & 
    ($\mathbf e_p,\mathbf e_v,\mathrm{vec}(\mathbf{R}),\mathbf e_\omega\,)\in\mathbb R^{18} $ & 
    \gls{srt}  & 
      \(\displaystyle
    r_t=-c_t,\quad
    c_t=\bigl(\|\mathbf e_p\|^2+\alpha_v\|\mathbf e_v\|^2+\alpha_\omega\|\mathbf e_\omega\|^2+\alpha_a\|a\|^2+\alpha_R\cos^{-1}\!\tfrac{\mathrm{tr}(\mathbf{R})-1}{2}\bigr)\Delta t
    \) 
    &
     Domain randomization across most physical parameters\newline
      Motor delay model \& motor noise\newline
      Sensor noise injection\newline
      Normalized thrust input\newline
      Sim-to-sim verification\newline
      Real-world deployment on multiple platforms  \\
    \cite{gronauer_using_2022} & 
    $\bigl(x_t,\mathbf{e}_t,a_{t-1}\bigr)\!\in\!\mathbb{R}^{20}$, \newline 
    $x_t=(\mathbf{p}_t,\mathbf{v}_t,\mathbf{q}_t,\boldsymbol{\omega}_t)\!\in\!\mathbb{R}^{13}$ \newline 
    (history $H{=}2$) & 
    \gls{srt} \& \gls{ctbr}\newline 
    (\gls{pwm} / AttRate / Att) & 
    $\displaystyle r_t= -\!\Bigl(\|\mathbf{e}_t\|^{2} + 10^{-4}\|a_t\|^{2} + 10^{-3}\|a_t\!-\!a_{t-1}\|^{2} + 10^{-3}\|\boldsymbol{\omega}_t\|^{2}\Bigr) + r_f,$ \newline 
    $r_f = -100$ if terminated &
    Domain randomization ($k_F$, $\Delta t$, $m$, $I$, $T_m$, $k_m$) \newline
    Gaussian \& uniform sensor noise; OU actuator noise \newline
    Motor delay model \newline
    Bayesian optimisation of sim parameters (1 h real flight data)\\
    \cite{chen_what_2024} & 
    $\bigl(e_W^{1:10},\,\mathbf v,\,\mathrm{vec}(\mathbf{R})\bigr)\!\in\!\mathbb R^{42}$
    ($e_W^{k}$: rel.\ pos.\ to next $10$ trajectory ref pts.) & 
    \gls{ctbr} & 
    $r_t = r_{\text{track}} - \lambda\|a_t - a_{t-1}\|^2$, \; $\lambda=0.4$ \newline 
    ($r_{\text{track}} \propto -\|e_W\|$) \newline
    also evaluated other rewards
    & 
   
    System identification of $m,I,k_f,T_m$ \newline
    Selective domain randomization (only sensitive params $m, k_f$) \newline
    Motor delay model \newline
    Smoothness penalty on $\Delta a$ \newline
    Large-batch \gls{ppo} training
 \\
    \cite{ma2024skilltransfer} & 
    $(\mathbf e,\mathbf q,[r,p,y],\mathbf v,\boldsymbol\omega,
    \mathbf a_{t-1})$  & 
    \gls{srt} (\gls{pwm}) & 
    $\displaystyle
    r_t=2-2.5\|\mathbf e\|
    -1.5\|[r,p]\|-0.05\|\mathbf v\|
    -0.05\|\boldsymbol\omega\|-0.1\|\mathbf a_t\|$ & 
    Residual-dynamics skill discovery with orthogonality constraints\newline
    Real-data fine-tuning\\
    \cite{Eschmann2024} & 
  $(\mathbf p,\mathbf v,\mathrm{vec}(\mathbf{R}),\boldsymbol{\omega}, \newline a_{t-1:t-H})\!\in\!\mathbb R^{18+4H}$ & 
  \gls{srt} (RPM)& 
  $\displaystyle r_t=-\!\bigl(w_p\|\mathbf p\|^{2}+w_v\|\mathbf v\|^{2}+w_R\phi(\mathbf{R})+w_\omega\|\boldsymbol{\omega}\|^{2}+w_a\|\mathbf a_t\|^{2}\bigr)+c_{\text{alive}}$ & 
  Zero-shot transfer without domain randomization \newline
  Motor delay model\newline
  Action-history observations\newline
  Asymmetric actor-critic with privileged critic\newline
  Force/torque disturbances\newline
  Gaussian sensor noise \\
  \cite{huang_quadswarm_2023} & 
  $(\mathbf e_i,\mathbf v_i,\mathrm{vec}(\mathbf{R}_i),\boldsymbol\omega_i, \newline 
  \{\delta\tilde{\mathbf p}_{ij},\tilde{\mathbf v}_{ij}\}_{j=1}^{Q})$ & 
  \gls{srt} & 
  $\displaystyle r_t = r_{\text{pos}}+r_{\text{vel}}+r_{\text{ori}}+r_{\text{spin}}+r_{\text{act}}+r_{\delta\text{act}}+r_{\text{rot}}+r_{\text{yaw}} \;+\; r_{\text{collision}}$ 
 
   $r_t \;=\;
  \alpha_{\text{pos}}\|\delta\mathbf p_i\|^{2}
  +\alpha_{\text{vel}}\|\mathbf v_i\|^{2}
  +\alpha_{\text{ori}}\,\mathbf{R}_{22}
  +\alpha_{\text{spin}}\|\boldsymbol{\omega}_i\|^{2}
  +\alpha_{\text{act}}\|\mathbf f_i\|^{2}
  +\alpha_{\delta\text{act}}\|\mathbf f_i(t)-\mathbf f_i(t-1)\|^{2}
  +\alpha_{\text{rot}}\frac{\operatorname{tr}(\mathbf{R}_i)-1}{2}
  +\alpha_{\text{yaw}}\,\mathbf{R}_{00}
  +\alpha_{\text{col}}\mathbf 1_{\text{collision}}$ 
  & 
  Swarm of quadrotors \newline
  Domain randomization (mass, inertia, thrust, etc.) \newline
  Motor delay model \& OU motor noise \newline
  Uniform/Gaussian sensor noise injection \newline
  Simplified collision \& downwash models \newline
  Zero-shot transfer demonstrated on Crazyflie swarms \\ 
    \bottomrule
  \end{tabularx}
\end{table*}

\subsection{Single \gls{uav} \gls{rl} for Payload Transport and Aerial Manipulation}
Extending single-\gls{uav} \gls{rl} methods to payload transport, researchers have sought to address the additional complexity introduced by cable-suspended loads. \cite{belkhale_model-based_2021} introduced model-based meta-\gls{rl} that adapts quickly to unknown payload dynamics, updating its model mid-flight to maintain stability. By contrast, \cite{hua_new_2022} combined a Lyapunov-based nonlinear controller with \gls{rl}, resulting in a hybrid controller that rapidly converges and robustly manages payload disturbances. 
These single-\gls{uav} payload studies pave the way toward multi-\gls{uav} \gls{rl} solutions by demonstrating that \gls{rl} can implicitly handle the coupling between vehicle and payload dynamics.

\section{Simulation Frameworks for Multi-\gls{uav} Payload Transport}
While \gls{rl} methods promise adaptability, they require large amounts of simulated data for training. Simulation frameworks thus play a central role in developing control algorithms and enabling \gls{rl} workflows. A comprehensive survey in \cite{Dimmig2023SurveyOS} reviews a wide variety of existing simulators, highlighting how \gls{uav} simulators accelerate algorithm development.

Specialized payload-transport simulators, such as RotorTM \cite{Li2022RotorTMAF}, accurately model cable-suspended dynamics but rely on CPU-based physics, resulting in slower runtimes that hinder sample-intensive \gls{rl} training. To address this bottleneck, GPU-accelerated frameworks have emerged. The Aerial Gym Simulator, built on NVIDIA Isaac Gym, runs thousands of multirotor instances in parallel with GPU-based physics and sensor rendering, dramatically increasing throughput \cite{aerial_gym_simulator}. Extending these capabilities, the simulator OmniDrones offers GPU-parallelized collaborative simulations of multirotor systems with rigid-link payloads, enabling multi-agent payload transport scenarios \cite{xu_omnidrones_2024}. While OmniDrones cannot yet model flexible cable dynamics and incurs some CPU-GPU synchronization overhead as multi-\gls{uav} cable-transport tasks grow in complexity, it represents a critical step toward high-throughput training of \gls{marl} policies. These simulation frameworks, by bridging the gap between physics fidelity and computational efficiency, accelerate the development of both single-agent and multi-agent \gls{rl} methods for payload transport.


\section{\gls{marl} for Multi-\gls{uav} Cooperative Tasks}

\gls{marl} has emerged as a powerful framework for multi-agent cooperative tasks, enabling agents to learn coordinated behaviors in dynamic environments. The key challenge in \gls{marl} is addressing non-stationarity. As each agent learns, the environment dynamics from any other agent's perspective change. To address this, early cooperative \gls{marl} approaches leveraged a \gls{ctde}  paradigm to stabilize learning.

Multi-Agent Deep Deterministic Policy Gradient (MADDPG) employs a shared critic during training to stabilize inter-agent learning and coordinate policies \cite{Lowe2017MultiAgentAF}. However, relying on centralized critics introduces scalability and communication overhead as the number of agents increases. More recent work has shown that \gls{ppo}-based \gls{marl} methods can achieve comparable coordination with simpler architectures. For instance, \gls{mappo} extends \gls{ppo} by using a centralized value function during training and has demonstrated performance on par with earlier \gls{ctde} methods across standard benchmarks \cite{yu_surprising_2022}. Findings from \cite{witt_is_2020} show that \gls{ippo}, where each agent trains its own \gls{ppo} instance without any shared critic or centralized state, often matches or even outperforms \gls{mappo} on tasks such as the StarCraft Multi-Agent Challenge (SMAC). Benchmarking studies confirm that \gls{mappo} nearly matches or exceeds other \gls{marl} algorithms, while maintaining a minimal and straightforward architecture \cite{Papoudakis2020BenchmarkingMD}.

Real-world \gls{marl} implementations have demonstrated the viability of decentralized learning for collaborative robotics tasks similar to multi-\gls{uav} payload transport. For example, \cite{Chen2022TowardsHBA} developed decentralized policies for multi-robot manipulation under uncertainty. \cite{Pandit2024LearningDM} demonstrated cooperative payload transport across uneven terrain without retraining, and \cite{Ji2021ReinforcementLF} enabled formation control with obstacle avoidance using only local observations. Other works, such as \cite{Chen2025DecentralizedNO}, applied decentralized \gls{marl} to coordinate multiple robots towing loads with complex dynamics like quadruped platforms. These results highlight the suitability of \gls{ppo}-based \gls{marl} methods (e.g., \gls{ippo} and \gls{mappo}) for multi-\gls{uav} payload transport, since they can scale to larger teams and adapt to dynamic interactions without requiring explicit global state estimation during execution.

\subsection{Multi-\gls{uav} \gls{marl} for Swarm Coordination}
In parallel to payload-specific \gls{marl} research, there has been significant progress in multi-\gls{uav} swarm coordination, which provides valuable insights for multi-\gls{uav} payload transport. \cite{huang_collision_2024} introduced attention-based neural networks enabling collision avoidance and navigation in \gls{uav} swarms, transferring policies trained with up to 32 agents directly to real hardware. \cite{xie_multi-uav_2024} addressed formation flight with obstacles through a two-stage \gls{rl} pipeline employing attention mechanisms, and validated swarm performance through real-world experiments. These swarm coordination advances, particularly in learning decentralized behaviors based on local observations, suggest strategies for coordinating multiple \gls{uav}s carrying a shared payload.

Lightweight perception-driven policies have also emerged. \cite{diao_efficient_2024} developed a LiDAR-based multi-agent navigation policy trained in dense simulation environments, demonstrating successful real-time collision-free flight on hardware with limited computation capabilities. \cite{zhao_deep_2024} applied deep \gls{rl} to collaborative pursuit and evasion scenarios, enabling coordinated multi-\gls{uav} target herding and obstacle avoidance. All of these works emphasize the importance of decentralized execution, local perception, and scalable architectures. These are properties that are equally critical when \gls{uav} teams carry cable-suspended payloads and must adapt quickly to disturbances and other agents' actions.

\subsection{Multi-\gls{uav} Collaborative Transport via \gls{marl}}
Cooperative payload transport has been well studied under traditional control frameworks, but \gls{marl}-based approaches to this problem remain relatively new. Insights from single-\gls{uav} \gls{rl} methods, such as rapid adaptability and robustness to payload disturbances, naturally motivate extensions to multi-\gls{uav} scenarios. In these settings, \gls{marl} can facilitate decentralized, cooperative strategies that adjust in real time to the complex, coupled dynamics of cable-suspended loads.

For instance, \cite{Lin2024PayloadTW} proposed a centralized \gls{rl} method for two \glspl{uav} jointly carrying a cable-suspended payload, demonstrating stable transport in simulation. Building on the idea of learned coordination, \cite{Estevez2024Reinforcement} introduced an \gls{rl}-based trajectory planning approach for multi-\gls{uav} load transportation, optimizing the transport path while ensuring both stability and safety under uncertainty. Complementing these algorithmic advances, \cite{xu_omnidrones_2024} presented OmniDrones, a GPU-parallelized simulation framework that supports payload configurations via rigid links. Although OmniDrones does not yet model flexible cables and has not reported any real-world hardware transfers, it shows promise for the payload transport domain by enabling high-throughput training of multi-\gls{uav} policies.

While these studies collectively establish the feasibility of \gls{marl} for multi-\gls{uav} payload transport, several challenges remain. Key questions include how to scale learned policies to teams, how to maintain stability under agile movements, more complex payload dynamics, and how to achieve reliable sim-to-real transfer on micro-\gls{uav} platforms like the Crazyflie.

In summary, the progression from traditional model-based control, through single-agent \gls{rl} and high-throughput simulation frameworks, to emerging \gls{marl} methods highlights a clear trajectory toward increasingly agile, flexible, and scalable strategies for multi-\gls{uav} payload transport.