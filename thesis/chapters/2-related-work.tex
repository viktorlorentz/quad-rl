\chapter{Related Work}
This chapter reviews prior work on the control of multirotor \gls{uav}s transporting cable-suspended payloads. We categorize existing methods into traditional model-based control strategies, reinforcement learning (\gls{rl}) approaches, and multi-agent reinforcement learning (\gls{marl}) frameworks. The review \cite{estevez_review_2024} rececently showed a wide range of methods for aerial cable suspended transport of payloads with quadrotors. 

There is still limited work on \gls{marl} for multi \gls{uav} payload transport, but we highlight recent advances in \gls{rl} for single \gls{uav} control and payload transport and aerial manipulation, as well as multi \gls{uav} swarm coordination. We also discuss the challenges of transferring \gls{rl} policies from simulation to real hardware, particularly for micro UAVs like the Crazyflie.

\section{Traditional Model-Based Control for Multi-\gls{uav} Payload Transport}
Early works on cooperative payload transport relied on physics-based models and carefully designed feedback controllers. \cite{sreenath_dynamics_2013} formulated the coupled dynamics of multiple quadrotors and their suspended payload, enabling coordinated planning and stabilization beyond quasi static assumptions. \cite{villa_cooperative_2021} proposed a virtual structure formation with adaptive dynamic compensators to achieve robust rod-shaped payload positiotning transport under modeling errors and wind disturbances, critically without inter \gls{uav} communication.

\cite{lee_geometric_2018} derived full rigid body dynamics on nonlinear configuration spaces and designed controllers guaranteeing payload pose tracking with provable stability. \cite{tognon_aerial_2018} showed that internal cable tension enables decentralized stabilization of any payload orientation, whereas zero tension schemes render attitude uncontrollable. \cite{tagliabue_robust_2017} shaped each drone’s apparent dynamics to handle parametric uncertainties, demonstrating stable collaborative transport even under worst case conditions.

\cite{sun_nonlinear_2023} developed an MPC that plans full six degree of freedom payload trajectories while respecting thrust, collision, and cable tension limits for teams of up to ten UAVs. \cite{li_nonlinear_2023} optimized payload trajectory and inter drone spacing within an MPC framework, validating performance through simulation and experiments. \cite{jackson_scalable_2020} introduced distributed trajectory optimization, parallelizing computation across agents to enable real time re planning in cluttered environments.

\cite{gabellieri_equilibria_2023} analyzed equilibrium and stability of two \gls{uav} beam transport without communication, showing that internal forces induced by cable angles enhance attitude robustness under uncertain parameters. \cite{wahba_kinodynamic_2024} integrated sampling based kinodynamic planning with a dynamics aware optimizer to navigate \gls{uav} teams through confined spaces, achieving lower tracking errors and reduced energy consumption compared to payload only planners. \cite{Wang2025SafeAA} improved on the level of agility and robustness of \gls{uav} teams carrying cable-suspended payloads while avoiding dependence on state estimation of the payload.

\todo{maybe seperate traj planning and control?}

\section{Reinforcement Learning Approaches for \gls{uav} Control}
\gls{rl} methods are gaining traction for \gls{uav} control, offering adaptability to complex and uncertain dynamics while reducing dependence on explicit models. Recent \gls{rl} approaches for \gls{uav} control are categorized into two areas: general single-\gls{uav} \gls{rl} and micro-\gls{uav} control, particularly focusing on the Crazyflie platform.

\subsection{Single-Agent Reinforcement Learning for \gls{uav} Flight Control}

Initial applications of reinforcement learning (RL) to \gls{uav} control focused on single-agent scenarios. In \cite{Koch2018ReinforcementLF}, Deep Deterministic Policy Gradient (DDPG), Trust Region Policy Optimization (TRPO), and Proximal Policy Optimization (PPO) were evaluated for inner-loop attitude control in a quadrotor simulation, demonstrating that RL-based controllers can match or exceed traditional PID controllers in terms of stability and responsiveness. Similarly, \cite{Hwangbo2017ControlOA} proposed a neural-network–based controller trained via RL to stabilize a quadrotor from harsh initial conditions (e.g., thrown upside-down at 5 m/s). The resulting policy achieved accurate step-response behavior and robust real-world stabilization in both simulation and hardware experiments.  

In highly dynamic settings at the limit of quadrotor agility, \cite{Song2023ReachingTL} compared optimal control and RL in autonomous drone racing, demonstrating that an RL-based policy can execute aggressive maneuvers near the vehicles limits and achieve competitive lap times.  

\subsection{Sim-to-Real Reinforcement Learning for Micro-\gls{uav} Control}
The Crazyflie 2.1 quadrotor used in this work is a miniature quadrotor platform with strict size, weight, and onboard computation constraints. These limitations pose significant challenges for deploying reinforcement-learned policies trained in simulation. Prior work has addressed these challenges through various strategies, including domain randomization, action abstraction, and asymmetric actor-critic frameworks.

\cite{molchanov_sim--multi-real_2019} demonstrated that domain randomization of physical parameters (e.g., motor characteristics and sensor noise) improves policy robustness under real-world variability. \cite{kaufmann_benchmark_2022} found that abstracting actions as \gls{ctbr} transfers better than low-level single rotor PWM or thrust commands. \cite{eschmann_learning_2024} leveraged privileged simulator information during training while enforcing realistic observation constraints at test time, yielding zero-shot transfer to the Crazyflie hardware within seconds of simulated learning. \cite{chen_what_2024} employed carefully designed reward shaping and curriculum schedules to stabilize training and ensure safe initial flights on the real quadrotor.

Table~\ref{tab:rl_comparison} summarizes several recent \gls{rl}-based approaches for control of the Crazyflie.
\todo{fix table}

\begin{table*}[!b]
\centering
\caption{Summary of recent reinforcement learning approaches for Crazyflie control. In the table, $\mathbf{p}$ denotes position, $\mathbf{v}$ velocity, $R$ a rotation matrix (orientation), and $\boldsymbol{\omega}$ angular velocity. Subscript $_{err}$ indicates error relative to a target state. $\mathbf{a}$ is the action vector (e.g., motor commands), and $\Delta \mathbf{a}$ its successive difference. We denote $\phi_R$ as the orientation error angle. We group PWM and motor thrust commands as \gls{srt}.}
\label{tab:rl_comparison}
\scriptsize
\begin{tabular}{p{2.0cm}<{\raggedright} p{2.5cm}<{\raggedright} p{2.0cm}<{\raggedright} p{3.0cm}<{\raggedright} p{2.8cm}<{\raggedright} p{2.7cm}<{\raggedright}}
\hline
\textbf{Paper} & \textbf{Observation $\mathcal{O}$} & \textbf{Action $\mathcal{A}$} & \textbf{Reward $r$} & \textbf{Sim-to-Real} & \textbf{Remarks} \\
\hline
Molchanov et al.\ (2019) \cite{molchanov_sim--multi-real_2019} & $e_p,\ e_v,\ e_\omega,\ R,\ \boldsymbol{\Omega}$ (rotor speeds) & \gls{srt} & { $-\bigl(w_p\|e_p\|^2 + w_v\|e_v\|^2 + w_\omega\|e_\omega\|^2 + w_u\|\mathbf{a}\|^2 + w_R\,\phi_R\bigr)$} & { Dynamics randomization; sensor \& thrust noise injection; onboard MCU inference.} & { First learned low-level controller; robust hover matching PID on Crazyflie 2.0.} \\[1ex]

Gronauer et al.\ (2022) \cite{gronauer_using_2022} & $\mathbf{x}$ (IMU attitude, alt.), $\mathbf{p}_{err}$, $\mathbf{a}_{t-1}$ & \gls{srt} (PWM commands) & { $-\bigl(|x_{err}|+|y_{err}|+|z_{err}|\bigr)$ per step; bonus on loop complete.} & { Randomized mass, inertia, thrust-to-weight, motor latencies; IMU/actuator noise; Bayesian sim tuning; onboard inference.} & { Compared low-level PWM vs. high-level rate policies; high-level more robust; zero-shot transfer onboard.} \\[1ex]

Chen et al. (2024) \cite{chen_what_2024} & $\mathbf{p}_{err}, \mathbf{v}, R, \boldsymbol{\omega}$ & \gls{ctbr} & { $r_{track} - \lambda\,\|\Delta\mathbf{a}\|$} & { System ID calibration; randomized uncalibrated params; matched inner PID mixer; large-batch training.} & { \gls{ppo} controller; >50\% lower error vs. prior RL; 70\% vs. MPC; tracks aggressive maneuvers.} \\[1ex]

Ma et al.\ (2024) \cite{ma2024skilltransfer} & $\mathbf{p},\mathbf{v},\ \mathbf{q},\ \boldsymbol{\omega},\ \mathbf{a}_{t-1},\,\int e\,dt$ & \gls{ctbr}& { $-\bigl(\|\mathbf{p}_{err}\|^2+\|\mathbf{v}_{err}\|^2+\phi_R^2+\|\mathbf{a}\|^2\bigr)$} & { Latent skill learning in sim; real adaptation via limited real-data finetuning.} & { 30\% improvement in real tasks through sim-to-real skill adaptation.} \\[1ex]

Eschmann et al.\ (2024) \cite{eschmann_learning_2024} & $\mathbf{p},\,R,\,\mathbf{v},\,\boldsymbol{\omega},\,\{\mathbf{a}_{t-k}\}$ & \gls{srt}(motor RPMs) & { $-\bigl(\|e_p\|^2 + \|\mathbf{v}\|^2\bigr)$; term.\ on crash.} & { Mixer noise; observation delay; async MCU inference.} & { <20 s sim train; MCU deployment; competitive with MPC.} \\[1ex]

Huang et al.\ (2023) \cite{huang_quadswarm_2023} & $e_p,e_v,\,\mathbf{q},\,\boldsymbol{\omega}$ & \gls{srt}& { Position/yaw error $- \alpha\|\mathbf{v}\|^2$.} & { Disturbance observer; wind randomization; onboard inference.} & { 34-48\% lower error under wind vs. adaptive MPC.} \\
\hline
\end{tabular}
\end{table*}

\subsection{Single \gls{uav} RL for Payload Transport and Aerial Manipulation}
\cite{belkhale_model-based_2021} introduced meta reinforcement learning that adapts quickly to unknown payload dynamics, updating its model mid flight to maintain stability. \cite{hua_new_2022} combined a Lyapunov based nonlinear controller with RL, resulting in a hybrid controller that rapidly converges and robustly manages payload disturbances. \cite{xing_multi-task_2024} developed a multitask RL framework enabling a single quadrotor to execute high speed stabilization, velocity tracking, and autonomous racing tasks using a unified policy, illustrating versatility and efficiency improvements.

\section{Simulation Frameworks for Multi-\gls{uav} Payload Transport}

The survey \cite{Dimmig2023SurveyOS} gives a good overview of the wide variety of existing simulators and highlights the central role of \gls{uav} simulators in developing control algorithms and enabling \gls{rl} workflows. Specialized payload-transport simulators like RotorTM \cite{Li2022RotorTMAF} accurately model cable-suspended dynamics but rely on CPU-based physics, resulting in slower runtimes that hinder sample-intensive \gls{rl} training.

GPU-accelerated frameworks overcome this bottleneck. The Aerial Gym Simulator, built on NVIDIA Isaac Gym, runs thousands of multirotor instances in parallel with GPU based physics and sensor rendering, dramatically increasing throughput \cite{aerial_gym_simulator}. OmniDrones extends these capabilities to more scenarios like multi-agent payload transport with rigid links, offering GPU-parallelized collaborative simulations of multirotor systems \cite{xu2023omnidrones}. OmniDrones cannot model flexible cable dynamics and instead uses rigid-link payloads. It also might incurs some CPU-GPU synchronization overhead for high-level \gls{rl} logic, reducing throughput as multi-\gls{uav} cable-transport tasks grow in complexity and scale. 

\section{\gls{marl}}
\gls{marl} has emerged as a powerful framework for multi-agent cooperative tasks, enabling agents to learn coordinated behaviors in dynamic environments. The key challenge in \gls{marl} is addressing the non-stationarity introduced by multiple learning agents, which complicates the learning process.
Early approaches to cooperative \gls{marl} leveraged \gls{ctde} to address non-stationarity and coordination. Notably, Multi-Agent Deep Deterministic Policy Gradient (MADDPG) employs a shared critic during training to stabilize inter-agent learning and enable coordinated policies \cite{Lowe2017MultiAgentAF}. While MADDPG achieved strong cooperation, its reliance on centralized critics introduces scalability and complexity challenges as agent counts increase.

More recent work has shown that \gls{ppo}-based \gls{marl} methods can achieve comparable coordination with simpler architectures. \gls{mappo} extends \gls{ppo} by using a centralized value function during training, demonstrating performance on par with earlier CTDE methods across standard benchmarks \cite{yu_surprising_2022}. \gls{ippo} trains each agent with its own \gls{ppo} instance, foregoing any shared critic or centralized state. Surprisingly \gls{ippo} often matches or outperforms MAPPO on tasks such as the StarCraft Multi-Agent Challenge (SMAC) \cite{witt_is_2020}. Benchmarking studies confirm that MAPPO nearly matches or exceeds other \gls{marl} algorithms while maintaining a minimal and straightforward architecture \cite{Papoudakis2020BenchmarkingMD}.

Robust real-world \gls{marl} implementations underscore the viability of decentralized learning for collaborative robotics. Decentralized policies have enabled reliable multi-robot manipulation under uncertainty \cite{Chen2022TowardsHBA}, cooperative payload transport across uneven terrain without retraining \cite{Pandit2024LearningDM}, and formation control with obstacle avoidance using only local observations \cite{Ji2021ReinforcementLF}. Decentralized \gls{marl} has been used to coordinate multiple robots for cable-towed load navigation with complex robots like quadrupeds \cite{Chen2025DecentralizedNO}. These results highlight the suitability of \gls{ppo}-based methods, like \gls{ippo} and \gls{mappo}, for multi-UAV payload transport with cable-suspended loads.


\subsection{Multi \gls{uav} \gls{marl} for Swarm Coordination}
\cite{riviere_glas_2020} presented GLAS, using centralized demonstrations and a differentiable safety module to train decentralized swarm policies surpassing traditional collision avoidance algorithms. \cite{huang_collision_2024} introduced attention based neural networks enabling collision avoidance and navigation in \gls{uav} swarms, transferring policies trained with up to 32 agents directly to real hardware. \cite{xie_multi-uav_2024} addressed formation flight with obstacles through a two stage RL pipeline employing attention mechanisms, validating swarm performance through real world experiments.

\cite{diao_efficient_2024} developed a lightweight LiDAR based multi agent navigation policy trained in dense simulation environments, demonstrating successful real time collision free flight on hardware with limited computation capabilities. \cite{zhao_deep_2024} applied deep RL to collaborative pursuit and evasion scenarios, enabling coordinated multi \gls{uav} target herding and obstacle avoidance.

\subsection{Multi \gls{uav} Collaborative Transport via \gls{marl}}
Collaborative payload transport via multi \gls{uav} \gls{rl} remains a relatively new area of study. Single \gls{uav} \gls{rl} approaches like those by \cite{belkhale_model-based_2021} and \cite{hua_new_2022}, which provide rapid adaptability and robust payload handling, suggest promising extensions to multi \gls{uav} reinforcement learning (\gls{marl}). Such adaptations would allow decentralized, cooperative strategies capable of dynamically adjusting to uncertainties and disturbances inherent to cable suspended multi \gls{uav} payload transport scenarios. \cite{Lin2024PayloadTW} proposed a centralized \gls{rl} method for carrying a cable-suspended payload with two \glspl{uav}. \cite{Estevez2024Reinforcement} introduced a reinforcement learning based trajectory planning approach for multi-\gls{uav} load transportation, focusing on optimizing the transport path while ensuring stability and safety.