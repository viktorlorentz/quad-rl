\chapter{Related Work}
This chapter reviews prior work on the control of multirotor \gls{uav}s transporting cable-suspended payloads. We organize the discussion by first examining traditional model-based control strategies, which laid the foundation for understanding the dynamics and cooperative planning. Building upon these classical approaches, we then highlight recent advances in \gls{rl} for single-\gls{uav} control and payload transport, before considering simulation frameworks that enable high-throughput algorithm development. Finally, we turn to \Gls{marl} \autocite{littman1994markov}, which offers decentralized learning solutions for multi-\gls{uav} swarm coordination and collaborative payload transport. A recent survey by \cite{estevez_review_2024} provides a comprehensive overview of various methods for aerial cable-suspended transport with quadrotors, which we build upon here.

\section{Traditional Model-Based Control for Multi-\gls{uav} Payload Transport}
Early work on cooperative payload transport relied on physics-based models and carefully designed feedback controllers. These approaches provided essential insights into how multiple \gls{uav}s interact with a shared cable-suspended payload, forming the baseline for later \gls{rl}-based and \gls{marl} methods.

Early work formulated the coupled dynamics of multiple quadrotors and their suspended payload, enabling coordinated planning and stabilization beyond quasi-static assumptions \cite{sreenath_dynamics_2013}. Similarly, \cite{villa_cooperative_2021} proposed a virtual-structure formation with adaptive dynamic compensators, achieving robust rod-shaped payload positioning under modeling errors and wind disturbances—critically, without requiring inter-\gls{uav} communication. These works demonstrate that carefully derived models and controllers can ensure stable transport, but they also highlight the complexity of explicitly modeling every disturbance and interaction.

Building on these foundational models, \cite{lee_geometric_2018} derived full rigid-body dynamics on nonlinear configuration spaces and designed controllers that guarantee payload pose tracking with provable stability. In parallel, \cite{tognon_aerial_2018} showed that internal cable tension enables decentralized stabilization of any payload orientation, whereas zero-tension schemes render the payload's attitude uncontrollable. To handle parameter uncertainties, \cite{tagliabue_robust_2017} shaped each drone's apparent dynamics, demonstrating stable collaborative transport even under worst-case conditions. These contributions collectively illustrate that traditional controllers can achieve robust performance, but they often require precise knowledge of vehicle and payload parameters.

More recent work has pushed toward online optimization techniques. \cite{sun_nonlinear_2023} developed a \gls{mpc} framework that plans full six degree of freedom payload trajectories while respecting thrust, collision, and cable tension limits for teams of up to ten \glspl{uav}. In a similar direction, \cite{li_nonlinear_2023} optimized payload trajectories and inter-drone spacing within an \gls{mpc} framework, validating performance on both simulation and experiments. To address real-time computational demands, \cite{jackson_scalable_2020} introduced a distributed trajectory optimization approach, parallelizing computation across agents to enable real-time replanning in cluttered environments. These \gls{mpc}-based methods illustrate how traditional control can scale to larger teams, but they still rely on accurate models and often incur significant computation overhead.

Analyses of equilibrium and stability help inform both controller design and the motivation for learning-based methods. For example, \cite{gabellieri_equilibria_2023} analyzed equilibrium conditions and stability for two-\gls{uav} beam transport without communication, showing that internal forces induced by cable angles enhance attitude robustness under uncertain parameters. \cite{wahba_kinodynamic_2024} integrated sampling-based kinodynamic planning with a dynamics-aware optimizer to navigate \gls{uav} teams through confined spaces, achieving lower tracking errors and reduced energy consumption compared to payload-only planners. Most recently, \cite{Wang2025SafeAA} improved the agility and robustness of \gls{uav} teams carrying cable-suspended payloads, while avoiding dependence on explicit state estimation of the payload. These studies underscore both the progress and limitations of purely model-based approaches, motivating the exploration of learning-based techniques that can adapt online to uncertainties.

\section{Reinforcement Learning Approaches for \gls{uav} Control}
While traditional model-based controllers offer provable guarantees, they can struggle with highly nonlinear dynamics, unmodeled disturbances, and complex payload interactions. Reinforcement learning (\gls{rl}) methods are gaining traction for \gls{uav} control because they can adapt to complex and uncertain dynamics while reducing the need for explicit modeling. In the context of payload transport, \gls{rl} offers the potential to learn control policies that implicitly account for cable dynamics and multi-agent coupling, paving the way toward more flexible and robust systems. We first review general single-\gls{uav} \gls{rl} methods and then focus on sim-to-real \gls{rl} strategies for micro-\gls{uav} platforms.

\subsection{Single-Agent Reinforcement Learning for \gls{uav} Flight Control}

Initial applications of \gls{rl} to \gls{uav} control focused on single-agent scenarios to validate that learning-based controllers can match or exceed traditional PID and model-based methods. For example, \cite{Koch2018ReinforcementLF} evaluated Deep Deterministic Policy Gradient (DDPG)\cite{Lillicrap2015ContinuousCW}, Trust Region Policy Optimization (TRPO)\cite{Schulman2015TrustRP}, and \gls{ppo}\cite{schulman_proximal_2017} for inner-loop attitude control in a quadrotor simulation. Their results demonstrated that \gls{rl}-based controllers can achieve stability and responsiveness on par with or better than classical PID controllers. Building on this insight, \cite{Hwangbo2017ControlOA} proposed a neural-network based controller trained via \gls{rl} to stabilize a quadrotor from harsh initial conditions, like being thrown upside-down. The resulting policy achieved accurate step-response behavior and robust real-world stabilization in both simulation and hardware experiments, showing that \gls{rl} can handle aggressive maneuvers.

In highly dynamic scenarios at the edge of quadrotor agility, \cite{Song2023ReachingTL} compared optimal control and \gls{rl} in autonomous drone racing. They found that an \gls{rl}-based policy could execute aggressive maneuvers near the vehicle's limits and achieve competitive lap times, illustrating that learned policies can capture complex dynamics more effectively than hand-tuned controllers. More recently, \cite{xing_multi-task_2024} developed a multitask \gls{rl} framework enabling a single quadrotor to execute high-speed stabilization, velocity tracking, and autonomous racing tasks using a unified policy, illustrating the versatility and efficiency of learned controllers in complex scenarios. 

These single-agent studies set the stage for extending \gls{rl} methods to payload transport, where the coupling between vehicle and payload introduces additional challenges.

\subsection{Sim-to-Real Reinforcement Learning for Micro-\gls{uav} Control}
Sim-to-real transfer remains a challenge in reinforcement learning for \gls{uav} platforms. In \cite{molchanov_sim--multi-real_2019} it is demonstrated, that domain randomization over physical parameters enables a single policy trained in simulation to generalize zero-shot to quadrotors of varying sizes. Despite these advances, sim-to-real discrepancies under real-world conditions still pose significant challenges in practice, especially for micro-\glspl{uav} like the Crazyflie 2.1, which have low inertia and constrained onboard computation. These limitations have led to the development of specialized \gls{rl} strategies designed to bridge the sim-to-real gap on micro-\glspl{uav}.

Abstracting actions as \gls{ctbr} transfers better than low-level rotor PWM or thrust commands \cite{kaufmann_benchmark_2022}. Leveraging such insights, \cite{eschmann_learning_2024} used an asymmetric actor-critic framework, where the training process has access to privileged simulator information, while the deployed policy uses only realistic observations. This approach yielded zero-shot transfer to Crazyflie hardware within seconds of simulated learning. \cite{chen_what_2024} also managed to transfer by combining careful reward shaping and curriculum schedules, stabilizing training and ensuring safe initial flights on the real quadrotor.

Table~\ref{tab:rl_comparison} summarizes these and other recent \gls{rl}-based approaches for Crazyflie control. The table highlights how observation spaces, action representations, and sim-to-real techniques vary across studies, and how they achieved robust flight control under real-world conditions.
\todo{fix table.}
\begin{table*}[!b]
\centering
\caption[Summary of RL approaches]{TODO fix table. Summary of recent reinforcement learning approaches for Crazyflie control. In the table, $\mathbf{p}$ denotes position, $\mathbf{v}$ velocity, $R$ a rotation matrix (orientation), and $\boldsymbol{\omega}$ angular velocity. Subscript $_{err}$ indicates error relative to a target state. $\mathbf{a}$ is the action vector (e.g., motor commands), and $\Delta \mathbf{a}$ its successive difference. We denote $\phi_R$ as the orientation error angle. We group PWM and motor thrust commands as \gls{srt}.}
\label{tab:rl_comparison}
\scriptsize
\begin{tabular}{p{2.0cm}<{\raggedright} p{2.5cm}<{\raggedright} p{2.0cm}<{\raggedright} p{3.0cm}<{\raggedright} p{2.8cm}<{\raggedright} p{2.7cm}<{\raggedright}}
\hline
\textbf{Paper} & \textbf{Observation $\mathcal{O}$} & \textbf{Action $\mathcal{A}$} & \textbf{Reward $r$} & \textbf{Sim-to-Real} & \textbf{Remarks} \\
\hline
Molchanov et al.\ (2019) \cite{molchanov_sim--multi-real_2019} & $e_p,\ e_v,\ e_\omega,\ R,\ \boldsymbol{\Omega}$ (rotor speeds) & \gls{srt} & { $-\bigl(w_p\|e_p\|^2 + w_v\|e_v\|^2 + w_\omega\|e_\omega\|^2 + w_u\|\mathbf{a}\|^2 + w_R\,\phi_R\bigr)$} & { Dynamics randomization; sensor \& thrust noise injection; onboard MCU inference.} & { First learned low-level controller; robust hover matching PID on Crazyflie 2.0.} \\[1ex]

Gronauer et al.\ (2022) \cite{gronauer_using_2022} & $\mathbf{x}$ (IMU attitude, alt.), $\mathbf{p}_{err}$, $\mathbf{a}_{t-1}$ & \gls{srt} (PWM commands) & { $-\bigl(|x_{err}|+|y_{err}|+|z_{err}|\bigr)$ per step; bonus on loop complete.} & { Randomized mass, inertia, thrust-to-weight, motor latencies; IMU/actuator noise; Bayesian sim tuning; onboard inference.} & { Compared low-level PWM vs. high-level rate policies; high-level more robust; zero-shot transfer onboard.} \\[1ex]

Chen et al. (2024) \cite{chen_what_2024} & $\mathbf{p}_{err}, \mathbf{v}, R, \boldsymbol{\omega}$ & \gls{ctbr} & { $r_{track} - \lambda\,\|\Delta\mathbf{a}\|$} & { System ID calibration; randomized uncalibrated params; matched inner PID mixer; large-batch training.} & { \gls{ppo} controller; >50\% lower error vs. prior RL; 70\% vs. MPC; tracks aggressive maneuvers.} \\[1ex]

Ma et al.\ (2024) \cite{ma2024skilltransfer} & $\mathbf{p},\mathbf{v},\ \mathbf{q},\ \boldsymbol{\omega},\ \mathbf{a}_{t-1},\,\int e\,dt$ & \gls{ctbr}& { $-\bigl(\|\mathbf{p}_{err}\|^2+\|\mathbf{v}_{err}\|^2+\phi_R^2+\|\mathbf{a}\|^2\bigr)$} & { Latent skill learning in sim; real adaptation via limited real-data finetuning.} & { 30\% improvement in real tasks through sim-to-real skill adaptation.} \\[1ex]

Eschmann et al.\ (2024) \cite{eschmann_learning_2024} & $\mathbf{p},\,R,\,\mathbf{v},\,\boldsymbol{\omega},\,\{\mathbf{a}_{t-k}\}$ & \gls{srt}(motor RPMs) & { $-\bigl(\|e_p\|^2 + \|\mathbf{v}\|^2\bigr)$; term.\ on crash.} & { Mixer noise; observation delay; async MCU inference.} & { <20 s sim train; MCU deployment; competitive with MPC.} \\[1ex]

Huang et al.\ (2023) \cite{huang_quadswarm_2023} & $e_p,e_v,\,\mathbf{q},\,\boldsymbol{\omega}$ & \gls{srt}& { Position/yaw error $- \alpha\|\mathbf{v}\|^2$.} & { Disturbance observer; wind randomization; onboard inference.} & { 34–48\% lower error under wind vs. adaptive MPC.} \\
\hline
\end{tabular}
\end{table*}

\subsection{Single \gls{uav} RL for Payload Transport and Aerial Manipulation}
Extending single-\gls{uav} \gls{rl} methods to payload transport, researchers have sought to address the additional complexity introduced by cable-suspended loads. \cite{belkhale_model-based_2021} introduced model-based meta-\gls{rl} that adapts quickly to unknown payload dynamics, updating its model mid-flight to maintain stability. By contrast, \cite{hua_new_2022} combined a Lyapunov-based nonlinear controller with \gls{rl}, resulting in a hybrid controller that rapidly converges and robustly manages payload disturbances. 
These single-\gls{uav} payload studies pave the way toward multi-\gls{uav} \gls{rl} solutions by demonstrating that \gls{rl} can implicitly handle the coupling between vehicle and payload dynamics.

\section{Simulation Frameworks for Multi-\gls{uav} Payload Transport}
While \gls{rl} methods promise adaptability, they require large amounts of simulated data for training. Simulation frameworks thus play a central role in developing control algorithms and enabling \gls{rl} workflows. A comprehensive survey in \cite{Dimmig2023SurveyOS} reviews a wide variety of existing simulators, highlighting how \gls{uav} simulators accelerate algorithm development.

Specialized payload-transport simulators, such as RotorTM \cite{Li2022RotorTMAF}, accurately model cable-suspended dynamics but rely on CPU-based physics, resulting in slower runtimes that hinder sample-intensive \gls{rl} training. To address this bottleneck, GPU-accelerated frameworks have emerged. The Aerial Gym Simulator, built on NVIDIA Isaac Gym, runs thousands of multirotor instances in parallel with GPU-based physics and sensor rendering, dramatically increasing throughput \cite{aerial_gym_simulator}. Extending these capabilities, OmniDrones offers GPU-parallelized collaborative simulations of multirotor systems with rigid-link payloads, enabling multi-agent payload transport scenarios \cite{xu_omnidrones_2024}. While OmniDrones cannot yet model flexible cable dynamics and incurs some CPU-GPU synchronization overhead as multi-\gls{uav} cable-transport tasks grow in complexity, it represents a critical step toward high-throughput training of \gls{marl} policies. These simulation frameworks, by bridging the gap between physics fidelity and computational efficiency, accelerate the development of both single-agent and multi-agent \gls{rl} methods for payload transport.


\section{\gls{marl} for Multi-\gls{uav} Cooperative Tasks}

\gls{marl} has emerged as a powerful framework for multi-agent cooperative tasks, enabling agents to learn coordinated behaviors in dynamic environments. The key challenge in \gls{marl} is addressing non-stationarity. As each agent learns, the environment dynamics from any other agent's perspective change. To address this, early cooperative \gls{marl} approaches leveraged a \gls{ctde}  paradigm to stabilize learning.

\cite{Lowe2017MultiAgentAF} introduced Multi-Agent Deep Deterministic Policy Gradient (MADDPG), which employs a shared critic during training to stabilize inter-agent learning and coordinate policies. However, the reliance on centralized critics introduces scalability and communication overhead as the number of agents grows. More recent work has shown that \gls{ppo}-based \gls{marl} methods can achieve comparable coordination with simpler architectures. For instance, \gls{mappo} extends \gls{ppo} by using a centralized value function during training and has demonstrated performance on par with earlier \gls{ctde} methods across standard benchmarks \cite{yu_surprising_2022}. Findings from \cite{witt_is_2020} show that Independent PPO (\gls{ippo}), where each agent trains its own \gls{ppo} instance without any shared critic or centralized state, often matches or even outperforms \gls{mappo} on tasks such as the StarCraft Multi-Agent Challenge (SMAC). Benchmarking studies confirm that \gls{mappo} nearly matches or exceeds other \gls{marl} algorithms, while maintaining a minimal and straightforward architecture \cite{Papoudakis2020BenchmarkingMD}.

Real-world \gls{marl} implementations have demonstrated the viability of decentralized learning for collaborative robotics tasks similar to multi-\gls{uav} payload transport. For example, \cite{Chen2022TowardsHBA} developed decentralized policies for multi-robot manipulation under uncertainty. \cite{Pandit2024LearningDM} demonstrated cooperative payload transport across uneven terrain without retraining and \cite{Ji2021ReinforcementLF} enabled formation control with obstacle avoidance using only local observations. Other works, such as \cite{Chen2025DecentralizedNO}, applied decentralized \gls{marl} to coordinate multiple robots towing loads with complex dynamics like quadruped platforms. These results highlight the suitability of \gls{ppo}-based \gls{marl} methods (e.g., \gls{ippo} and \gls{mappo}) for multi-\gls{uav} payload transport, since they can scale to larger teams and adapt to dynamic interactions without requiring explicit global state estimation during execution.

\subsection{Multi-\gls{uav} \gls{marl} for Swarm Coordination}
In parallel to payload-specific \gls{marl} research, there has been significant progress in multi-\gls{uav} swarm coordination, which provides valuable insights for multi-\gls{uav} payload transport. \cite{riviere_glas_2020} presented GLAS, a framework that uses centralized demonstrations and a differentiable safety module to train decentralized swarm policies that surpass traditional collision avoidance algorithms. \cite{huang_collision_2024} introduced attention-based neural networks enabling collision avoidance and navigation in \gls{uav} swarms, transferring policies trained with up to 32 agents directly to real hardware. \cite{xie_multi-uav_2024} addressed formation flight with obstacles through a two-stage \gls{rl} pipeline employing attention mechanisms, and validated swarm performance through real-world experiments. These swarm coordination advances—particularly in learning decentralized behaviors based on local observations—suggest strategies for coordinating multiple \gls{uav}s carrying a shared payload.

Lightweight perception-driven policies have also emerged. \cite{diao_efficient_2024} developed a LiDAR-based multi-agent navigation policy trained in dense simulation environments, demonstrating successful real-time collision-free flight on hardware with limited computation capabilities. \cite{zhao_deep_2024} applied deep \gls{rl} to collaborative pursuit and evasion scenarios, enabling coordinated multi-\gls{uav} target herding and obstacle avoidance. All of these works emphasize the importance of decentralized execution, local perception, and scalable architectures—properties that are equally critical when \gls{uav} teams carry cable-suspended payloads and must adapt quickly to disturbances and other agents' actions.

\subsection{Multi-\gls{uav} Collaborative Transport via \gls{marl}}
Cooperative payload transport has been well studied under traditional control frameworks, but \gls{marl}-based approaches to this problem remain relatively new. Insights from single-\gls{uav} \gls{rl} methods, such as rapid adaptability and robustness to payload disturbances, naturally motivate extensions to multi-\gls{uav} scenarios. In these settings, \gls{marl} can facilitate decentralized, cooperative strategies that adjust in real time to the complex, coupled dynamics of cable-suspended loads.

For instance, \cite{Lin2024PayloadTW} proposed a centralized \gls{rl} method for two \glspl{uav} jointly carrying a cable-suspended payload, demonstrating stable transport in simulation. Building on the idea of learned coordination, \cite{Estevez2024Reinforcement} introduced an \gls{rl}-based trajectory planning approach for multi-\gls{uav} load transportation, optimizing the transport path while ensuring both stability and safety under uncertainty. Complementing these algorithmic advances, \cite{xu_omnidrones_2024} presented OmniDrones, a GPU-parallelized simulation framework that supports payload configurations via rigid links. Although OmniDrones does not yet model flexible cables and has not reported any real-world hardware transfers, it shows promise for the payload transport domain by enabling high-throughput training of multi-\gls{uav} policies.

While these studies collectively establish the feasibility of \gls{marl} for multi-\gls{uav} payload transport, several challenges remain. Key questions include how to scale learned policies to teams, how to maintain stability under agile movements, more complex payload dynamics, and how to achieve reliable sim-to-real transfer on micro-\gls{uav} platforms like the Crazyflie.

In summary, the progression from traditional model-based control, through single-agent \gls{rl} and high-throughput simulation frameworks, to emerging \gls{marl} methods highlights a clear trajectory toward increasingly agile, flexible, and scalable strategies for multi-\gls{uav} payload transport.