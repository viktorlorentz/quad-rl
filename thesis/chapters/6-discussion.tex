\chapter{Discussion}
This chapter brings together the central insights from our work on training and evaluating \gls{rl} agents for decentralized multi-quadrotor cable-suspended payload transport. We revisit our research objectives and contributions, summarize the most important findings, consider their broader significance, acknowledge the limitations we encountered, and suggest directions for future research to further advance the field.

\section{Key Findings}
In pursuit of our research objectives, we first formulated the decentralized multi \gls{uav} payload transport problem as a multi agent \gls{dec-pomdp} with local observation and action spaces enabling coordination through decentralized policies, then designed and implemented a high-performance GPU-parallel simulation pipeline to support scalable policy learning, investigated reward design by decomposing the overall objective into tracking and stability components, evaluated the learned policies under harsh initializations to assess resilience relative to classical baselines, and finally demonstrated that our decentralized framework maintains precise tracking and payload stability as the number of \glspl{uav} increases.

Concurrently, running training across thousands of parallel simulation environments effectively reduced training time and gradient variance, accelerating convergence to reliable policy performance. Our proposed training pipeline, implemented in JAX and MuJoCo, enables very capable and efficient training.

Randomizing the initial positions of payloads and quadrotors during training was crucial to train a policy to recover from harsh initial conditions. We found that applying domain randomization solely to the maximum thrust parameter, without randomizing parameters such as payload mass, quadrotor mass, and cable length, still leads to decent generalization in simulation. Extending each agent's observation to include the most recent action further enhanced the stability and smoothness of the control outputs.

The design of the reward function proved to be another key factor. By shaping reward components with exponential decay rather than fixed penalties, we prevented agents from terminating episodes prematurely and enabled them to recover gracefully from disturbances. Our experiments demonstrate that including a regularization term on the agent's actions is critical to producing smooth control signals and avoiding abrupt bang-bang behavior. This regularization also improves the ability to transfer policies from simulation to real hardware. 

Notably, policies trained exclusively on recovery and position holding transferred successfully to dynamic trajectory tracking. This result highlights the flexibility of the learned control strategies. Despite these advances, achieving a reliable sim-to-real transfer remains an open challenge.

\section{Implications}
Our findings underscore the potential of \gls{rl} for cooperative aerial transport tasks that are difficult to handle with traditional model-based control. The decentralized \gls{marl} framework we developed is capable of stabilizing suspended payloads, following precise trajectories, and rejecting external disturbances. Such capabilities are directly relevant to real-world applications, including aerial construction support, infrastructure inspection, and rapid delivery of emergency supplies.

At the same time, our results emphasize that accurate and robust state estimation is essential for any practical deployment. Our approach requires the position information of all quadrotors to be shared with low latency and high frequency. In many real-world environments, full state information is not available, external motion capture systems are impractical, and low-latency communication is challenging. Ensuring that learned policies remain resilient in the face of communication issues, sensor noise, and estimation errors is therefore a critical requirement for future work.

\section{Limitations}
Several important limitations must be mentioned in the conclusions of this study. First, learned policies sometimes fail under extreme conditions, for example, when payload oscillations grow large or quadrotor separation becomes excessive. Our current approach does not include obstacle avoidance or integrated motion planning and relies heavily on external infrastructure without providing formal safety guarantees. Each change in payload mass or quadrotor configuration presently requires retraining from scratch, which limits flexibility. Although we observed some generalization across payload masses and cable lengths in simulation, systematic validation on real platforms is still needed.

Moreover, our methods depend on knowing quantities such as payload mass and cable lengths in advance. Improving autonomy will require onboard estimation of these variables. Addressing the challenge of accurately estimating the full sytem parameters of cables and payload remains an important barrier to fully autonomous operation.

As mentioned above, our method requires high-frequency position updates from all quadrotors, which is not always feasible in real-world scenarios. The current approach also does not account for communication delays or packet loss, which can significantly impact performance in practice. Future work should explore ways to make the system more robust to these issues, for example, by incorporating predictive control or state estimation techniques, or by estimating the positions of other quadrotors solely from local observations of each quadrotor.

Finally, while our training pipeline is highly efficient, it still requires substantial computational resources for training. Future work should explore more sample-efficient algorithms and methods to reduce the reliance on extensive simulation data.

\section{Future Research Directions}
To address these challenges and enhance the real-world applicability of decentralized \gls{rl} for multi-quadrotor cable-suspended payload transport, we propose several avenues for future research. Enriching the agent's state representation with extended histories or estimates of latent variables such as cable length, payload mass, quadrotor mass, and relative positions could yield more robust control. Developing onboard estimation schemes that leverage \gls{imu} cameras or LiDAR sensors would remove dependence on external motion capture systems. These should be combined with motion and path planning algorithms to ensure safe operation in dynamic environments. To facilitate real-world deployment, inter-quadrotor communication must be investigated, including methods for handling communication delays and packet loss.

Expanding experiments to a wider range of payload shapes, masses, and cable configurations will help characterize the limits of scalability and generalization. Improving sim-to-real transfer by applying advanced domain randomization methods may reduce the remaining reality gap. Finally, incorporating online policy adaptation mechanisms and exploring alternative \gls{rl} algorithms, such as off-policy actor-critic methods, recurrent networks, attention mechanisms, or meta-learning approaches, could accelerate training and improve policy flexibility under changing conditions.

Pursuing these directions will bring us closer to realizing fully autonomous, reliable, and efficient multi-agent cable-suspended aerial transport systems based on \gls{rl}.