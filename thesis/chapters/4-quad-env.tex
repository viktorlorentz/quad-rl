\chapter{Training Multi-Agent Reinforcement Learning Policies for Crazyflie Quadrotors}

This chapter presents our methodology for training decentralized \gls{marl} policies that enable a team of Crazyflie quadrotors to cooperatively transport a cable-suspended payload. We begin by formulating the cooperative transport task as a \gls{dec-pomdp}, defining the state, action, and observation structures. We then describe our high-performance simulation and training pipeline, and finally present the modular reward design used to train our policies.
\section{Problem Formulation}

We model a team of $Q$ Crazyflie quadrotors collaboratively carrying a cable-suspended payload as the \gls{dec-pomdp} $(\mathcal{Q}, \mathcal{S}, \{\mathcal{A}^i\}_{i=1}^Q, P, r, \{\Omega^i\}_{i=1}^Q, O, \rho_0, \gamma)$, where $\mathcal{Q} = \{1,2,\dots,Q\}$ is the set of agents. At each discrete timestep $t$, the global state 
\begin{equation}
s_t = \bigl(\{\mathbf{p}^i_t,\mathbf{v}^i_t,\mathbf{R}^i_t,\boldsymbol{\omega}^i_t\}_{i=1}^Q,\; \mathbf{p}^P_t,\mathbf{v}^P_t\bigr)
\end{equation}
encapsulates each quadrotor's position $\mathbf{p}^i_t \in \mathbb{R}^3$, velocity $\mathbf{v}^i_t \in \mathbb{R}^3$, attitude $\mathbf{R}^i_t \in SO(3)$, angular velocity $\boldsymbol{\omega}^i_t \in \mathbb{R}^3$, and the payload's position $\mathbf{p}^P_t \in \mathbb{R}^3$ and velocity $\mathbf{v}^P_t \in \mathbb{R}^3$. The transition model $P(s_{t+1}\mid s_t, a_t)$ corresponds to a single simulation step in MuJoCo MJX, which approximates the continuous quadrotor and payload dynamics (including cables) over the fixed interval $\Delta t$.

The initial state is sampled from a distribution $\rho_0$. In our case $\rho_0$ defines harsh initial conditions, such that the policies must learn to recover from them.

Each agent $i$ selects an action 
\begin{equation}
a^i_t = (f^i_{1,t},f^i_{2,t},f^i_{3,t},f^i_{4,t}) \in \mathbb{R}^4
\end{equation}
where $f^i_{j,t}$ is the thrust of rotor~$j$. The joint action of all agents is $a_t = (a^1_t,\dots,a^Q_t)$. 

We define the payload position tracking error as
$
\mathbf{e}^P_t = \mathbf{p}^P_t - \mathbf{p}^P_{\mathrm{des},t},
$
so that $e^P_t = \|\mathbf{e}^P_t\|$ represents the tracking error at time $t$. The main objective is to minimize this error while maintaining stability in the system. This gives us a rough reward structure of:
\begin{equation}
r(s_t,a_t) \approx\; - e^P_t \;-\; \text{(stability penalties)}
\end{equation}
We penalize payload tracking error and include additional terms to discourage large payload swing or cable-tension imbalance.

Each agent must observe its own state as well as the payload's state and the position of the other agents to be able to avoid them. Therefore agent $i$ observes
\begin{equation}
    \label{eq:obs}
    \tilde{o}^i_t 
    = \Bigl(
      \hat{\mathbf{p}}^i_t,\;
      \hat{\mathbf{v}}^i_t,\;
      \hat{\mathbf{R}}^i_t,\;
      \hat{\boldsymbol{\omega}}^i_t,\;
      \mathbf{e}^P_t,\;
      \mathbf{v}^P_t,\;
      \{\hat{\mathbf{p}}^j_t\}_{j \in \mathcal{Q}\setminus\{i\}}
    \Bigr).
\end{equation}
\emph{Note that \(\tilde{o}^i_t\) is a simplified observation. The exact observation model is defined below.}

where $\hat{\mathbf{p}}^i_t,\hat{\mathbf{v}}^i_t,\hat{\mathbf{R}}^i_t,\hat{\boldsymbol{\omega}}^i_t$ are \gls{ekf} estimates of its own state and $\mathbf{e}^P_t,\mathbf{v}^P_t$ are the payload state from motion capture. To reduce notational complexity we will denote the observation component without $\hat{\cdot}$ as $\mathbf{p}^i_t,\mathbf{v}^i_t,\mathbf{R}^i_t,\boldsymbol{\omega}^i_t$. The observation function $O(o^i_t \mid s_t)$ models sensor noise and maps the global state $s_t$ to each $o^i_t$. Finally, $\gamma \in [0,1)$ is the discount factor.

Agents share a single policy conditioned on their current observation \(o^i_t\) and the previous action \(a^i_{t-1}\) and parameterized by \(\theta\). The actions are sampled as
\begin{equation}
  a^i_t \sim \pi_{\theta}\bigl(a^i_t \mid o^i_t,\;a^i_{t-1}\bigr)
  \quad\text{for }i=1,\dots,Q.
\end{equation}


For multiple agent scenarios we sample the actions independently for each agent, so that the joint action is given by $a_t = \bigl(a^1_t,\dots,a^Q_t\bigr)$.


We seek the shared parameters \(\theta\) that maximize the expected discounted return
\begin{equation}
\begin{aligned}
\text{maximize}\quad & J(\theta) = \mathbb{E}\Bigl[\sum_{t=0}^\infty \gamma^t\,r(s_t,a_t)\Bigr],\\
\text{subject to}\quad &s_0 \sim \rho_0,\\
&s_{t+1} \sim P\bigl(\cdot\mid s_t,a_t\bigr),\\
&o^i_t \sim O\bigl(\cdot\mid s_t\bigr),\\
&a^i_t \sim \pi_{\theta}\bigl(\cdot\mid o^i_t,a^i_{t-1}\bigr)
\quad\forall i.
\end{aligned}
\end{equation}
By maximizing \(J(\theta)\), the agents learn to jointly minimize payload tracking error \(e^P_t\) while maintaining stability throughout the cooperative transport task.

We have now formalized the problem as a \gls{dec-pomdp}, which allows us to apply decentralized \gls{marl} algorithms to learn policies that can handle the complexities of multi-agent coordination in cable-suspended payload transport tasks. The next section describes the CrazyMARL framework, which provides the necessary simulation and training infrastructure to implement this efficiently.
\todo{maybe add fig wit multiple agents and env...}

\section{CrazyMARL}
We present \textbf{CrazyMARL}, an end-to-end JAX-based pipeline for training \gls{marl} policies on teams of Crazyflie quadrotors. Our extendable framework implements both learning for a single quadrotor without payload, as well as single-vehicle and cooperative multi-agent cable-suspended payload transport tasks. At its core, the simulation leverages the high-performance MJX backend of the MuJoCo physics engine \cite{todorov_mujoco_2012}, interfaced through the Brax library for highly parallelized training. Our environments and algorithms are based on JaxMARL introduced by \autocite{flair2023jaxmarl}. We expose the simulator as a \texttt{JAXMARL.Mabrax} environment, providing a drop-in API for JAX-based RL algorithms and other research algorithm implementing the JaxMARL api.

\subsection{Simulation Environment}

Our framework provides a configurable environment generator for simulating multi-agent cable-suspended payload transport tasks, as well as swarm control of single quadrotors. The environment is designed to be modular and extensible, allowing for easy definition of new scenarios. The environment can be initialized with a configuration, defining the number of quadrotors, the payload mass, cable length, and other parameters. The environment supports both single-agent and multi-agent scenarios, where each agent is a quadrotor with its own observations and actions.
\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{figs/quad_env.png}
\caption[Simulation Environment]{Rendering of the MuJoCo simulation with three quadrotors carrying a cable-suspended payload.}
\label{fig:crazyflie_env}
\end{figure}

Figure~\ref{fig:crazyflie_env} shows a rendering of the simulation environment with three quadrotors carrying a cable-suspended payload. The environment is built upon the MuJoCo physics engine, which provides accurate and efficient simulation of rigid body dynamics, including contact and friction. The MJX solver is used to compute the physics updates in parallel on the GPU, allowing for high-performance training of reinforcement learning policies with up to 32k parallel environments. We can also visualize the simulation using the rendering capabilities of MuJoCo, which provides a realistic representation of the quadrotors, payload, and cables.

The quadrotors are modeled as rigid bodies with four rotors, each producing thrust and a small torque. We optionally reduce the quadrotors collision mesh to a rectangular prism in order to speed up collision detection. The payload is modeled as a rigid body sphere suspended by cables, which are represented as tendons in MuJoCo. The shape and attachment points of the payload are configurable. The cables connect the quadrotors to the payload and can be configured with different lengths and properties. The environment also supports obstacles, which were not yet further utilized in our work, but could be a good basis for future work on obstacle avoidance or dynamic interactions with the environment.

We implement the JaxMARL api for multi agent environments based on Gymnax \autocite{gymnax2022github}. This wraps our environment in a standardized interface that can be used with any compatible \gls{rl} algorithm. The wrapper adds an abstraction layer to the simulation. Each timestep we call a step function with a set of actions for each quadrotor and the environment returns the updated state which is then used to compute the rewards and observations for each agent.

\subsection{Observation Mapping}
In practice we slightly adjust the observation structure defined in Equation~\eqref{eq:obs}. Each agent \(i\) still receives a local observation vector \(\mathbf{o}^i_t\) that contains its own state, the payload's state, and the relative positions of other agents.  This ensures that each agent can act based on its own measurements and the shared payload information, while not directly observing other agents' internal states. To define the observation space, we first define the global observation vector \(\mathbf{o}_t\) that contains all relevant information about the system state at time \(t\). This also serves as the basis for computing rewards and termination conditions and allows for a centralized training setup.
\subsubsection{Global Observations}
At each control step \(t\), the simulation environment produces a high-dimensional state vector. We map this to a global observation vector 
\begin{equation}
\mathbf{o}_t \;=\; \bigl[\;\underbrace{\mathbf{e}^P_t}_{3},\;\underbrace{\mathbf{v}^P_t}_{3},\;\underbrace{\{\;\mathbf{r}^i_t,\;\mathrm{vec}(\mathbf{R}^i_t),\;\mathbf{v}^i_t,\;\boldsymbol{\omega}^i_t,\;\mathbf{a}^i_t\}_{i=1}^Q}_{Q(3+9+3+3+4)}\bigr] \;\in\;\mathbb{R}^{6 + 22Q}
\end{equation}
Here \(\mathbf{e}^P_t = (\mathbf{p}^P_{\mathrm{des},t}-\mathbf{p}^P_t)/\max(\|\mathbf{p}^P_{\mathrm{des},t}-\mathbf{p}^P_t\|,1)\) is the normalized payload tracking error and \(\mathbf{v}^P_t\) its velocity. Giving the error as observation instead of an absolute position, allows us to give a position setpoint and track trajectories. We normalize the error to have the policy automatically generalize to a target at any distance. For each quadrotor \(i\), \(\mathbf{r}^i_t=\mathbf{p}^i_t-\mathbf{p}^P_t\) denotes its position relative to the payload, \(\mathrm{vec}(R^i_t)\in\mathbb{R}^9\) the row-major flattening of its rotation matrix, \(\mathbf{v}^i_t\) is the linear velocity in world frame, \(\boldsymbol{\omega}^i_t\) its body-frame angular velocities and \(\mathbf{a}^i_{t-1}\) the agents previous action. By construction, this global observation vector contains all information required to compute rewards and termination conditions. It can be easily adjusted to include additional information or other mappings.

\subsubsection{Local Observations}
Since we want to train our policy in a \gls{ctde} fashion, we map the global observation vector to each agent's local observation space. During decentralized execution, each agent \(i\) receives only the subset of entries corresponding to the payload terms, its own dynamic state block, and the other quadrotors relative positions.  Concretely, agent \(i\) observes
\begin{equation}
\mathbf{o}^i_t 
\;=\;
\bigl[\,
\mathbf{e}^P_t,\;\mathbf{v}^P_t,\;\mathbf{r}^i_t,\;\mathrm{vec}(\mathbf{R}^i_t),\;\mathbf{v}^i_t,\;\boldsymbol{\omega}^i_t,\;\mathbf{a}^i_{t-1},\;
\{\mathbf{r}^j_t\}_{j \in \mathcal{Q}\setminus\{i\}}
\,\bigr]\;\in\;\mathbb{R}^{6 + 22 + Q - 1}\,,
\end{equation}
so that each policy conditions on the payload's error and velocity, the agent's full pose and velocity, and its previous action. This restriction enforces the Dec-POMDP structure, by hiding other agents' internal states, coordination must emerge through the shared reward alone.

The observation structure can also be used for a scenario without a payload, where the team observations containing payload position and velocity are omitted.
\subsubsection{Observation Noise}
To account for sensor imperfections, we can inject structured Gaussian noise into the global observation vector at each timestep. Concretely, given an observation $\mathbf{o}$, we draw a standard normal vector $\bm{\eta}\sim\mathcal{N}(\mathbf{0},\mathbf{I})$ of the same dimension and compute
\begin{equation}
    \mathbf{o}' = \mathbf{o} + \sigma_{\mathrm{obs}}\,\bm{\Lambda}\,\bm{\eta}\,,
\end{equation}
where $\sigma_{\mathrm{obs}}$ is a tunable noise amplitude and $\bm{\Lambda}\in\mathbb{R}^{6+22Q}$ is a diagonal scaling vector to tune the noise level for each observation component. We chose each scale factor by examining the typical range of values for its corresponding observation component and setting the noise magnitude accordingly. This ensures that each component of the observation is corrupted by noise proportional both to its typical magnitude and the global noise level.
\subsection{Action Space}

The policy of each agent \(i\) outputs a four-dimensional vector \(\mathbf{a}^i_t\in[-1,1]^4\), representing normalized thrust commands for its rotors.  These are mapped to physical rotor forces via
\begin{equation}
f^i_{j,t}
\;=\;
\frac{a^i_{j,t} + 1}{2}\;f_{\max}^i,
\qquad
j=1,\dots,4,
\end{equation}
where \(f_{\max}^i\) is the agent-specific maximum thrust, randomly perturbed at the start of each episode to improve robustness.  Optionally, zero-mean Gaussian noise proportional to \(f_{\max}^i\) may be added to each \(f^i_{j,t}\) to model actuator uncertainty.  The resulting thrust vector \(\mathbf{f}^i_t\) is then applied in the physics simulator.  

We always constrain \(\mathbf{a}^i_t\in[-1,1]^4\) and sample \(f_{\max}^i\) within a known bound to produce safe thrust profiles that transfer more effectively from simulation to the real Crazyflie platform.\todo{maybe talk about the motor delay model...}
\subsection{Randomization}
To train a robust policy that can handle a wide range of initial conditions and disturbances, we apply several randomization techniques during training.
\subsubsection{Initial-State Randomization}
\label{sec:reset}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{experiments/initial_states.pdf}
    \caption[Initial States]{Randomly sampled initial configurations for the payload and quadrotors. We show configurations for Q = 1, 2, 3 and 6 quadrotors. For visualization purposes we reduce the number of different runs (N) visualized. Here the payload position is sampled uniformly without target bias. The payload is positioned at the center of each configuration, while quadrotors are uniformly distributed in a spherical shell around it. A lot of configurations start with slack cables. Some configurations start on the gorunde.}
    \label{fig:reset_config}
\end{figure}
A key aspect of our approach is the initial-state randomization $s_0\sim\rho_0$, which generates diverse initial configurations for the payload and quadrotors. This helps the policy learn to handle a wide range of challenging scenarios, including starting from the ground, hovering, disturbance rejection, modeswitch of the payload cable and strong payload swing. The initial conditions are sampled at each reset of the environment, ensuring that the policy is exposed to a variety of situations during training. This is crucial for learning robust behaviors. 
At each reset, the payload center~$p$ is sampled uniformly around the target at $[0,0,1.5]$ in
\begin{equation}
p^P_{xy}\sim\mathcal{U}([-1.5,1.5]),\quad p^P_z\sim\mathcal{U}([-1,3]),
\end{equation}
where the $z$-coordinate is clipped to ensure the payload is above the ground, allowing us to adjust the ratio of ground-takeoff starting positions. We additionally add a subset initializations near the payload with $p^P\sim\mathcal{N}(\mathbf{0},0.02\mathbf{I})$. This ratio of target initializations is usually set to 20\% in training to help with learning accurate tracking.

We can then use the payload position $p^P$ as the center point and randomly sample $N$ quadrotors in a spherical shell around the payload. The radius $r_i$ of each quadrotor is sampled from a normal distribution with mean $\mu_r$ and standard deviation $\sigma_r$, and then clipped to cable length. The angles $\theta_i$ and $\phi_i$ are sampled from normal distributions with means $\mu_\theta$ and $\phi_{\mathrm{offset}}$ respectively, and standard deviations $\sigma_\theta$ and $\sigma_\phi$. The angles are then adjusted to ensure they are within the range $[0, 2\pi]$. The equations for sampling the quadrotor positions are as follows:
\begin{equation}
r_i = \mathrm{clip}\bigl(\mu_r+\sigma_r\varepsilon_i^{(r)},\,r_{\min},\,r_{\max}\bigr),\quad
\theta_i = \mu_\theta+\sigma_\theta\varepsilon_i^{(\theta)},\quad
\phi_i = \tfrac{2\pi(i-1)}{Q} + \phi_{\mathrm{offset}} + \sigma_\phi\varepsilon_i^{(\phi)},
\end{equation}
where we found these parameters to work well in practice across the different number of quadrotors. $c$ is the length of the cable, which is also the maximum radius for the quadrotors.
\begin{equation}
\begin{aligned}
\varepsilon_i^{(r)},\,\varepsilon_i^{(\theta)},\,\varepsilon_i^{(\phi)}
    &\sim \mathcal{N}(0,1), \quad
\phi_{\mathrm{offset}} \sim \mathcal{U}(-\pi,\pi),\\
\mu_r &= c,\quad
\sigma_r = \tfrac{c}{3},\quad
r_{\min} = 0.05,\quad
r_{\max} = c,\\
\mu_\theta &= \tfrac{\pi}{7},\quad
\sigma_\theta = \tfrac{\pi}{8},\quad
\sigma_\phi = \tfrac{\pi}{N+1}\,.
\end{aligned}
\end{equation}

These are converted to Cartesian positions with the $z$-coordinate clipped.
\begin{equation}
q_i = p + r_i
\begin{bmatrix}
\sin\theta_i\cos\phi_i\\
\sin\theta_i\sin\phi_i\\
\cos\theta_i
\end{bmatrix},
\end{equation}
We oversample configurations with this strategy and filter out all configurations where the quadrotors are initially in collision. This results in a even distribution of valid initial configurations with some very harsh initial conditions for any number of quadrotors as shown in \autoref{fig:reset_config}. This also includes starting from the ground. 

We also randomize the quadrotor attitude. Roll and pitch angles are drawn from $\mathcal{N}(0,10^\circ)$ and clipped to $\pm90^\circ$, while yaw is sampled uniformly from $[-180^\circ,180^\circ]$.

Each component of the linear velocity vector $\mathbf{v}$ is sampled from a zero-mean Gaussian with a standard deviation of $0.2\,m/s$. Each component of the angular velocity (body-rate) vector $\boldsymbol{\omega}$ is sampled similarly with a standard deviation of $20^\circ/s$.

This creates a wide range of initial conditions, including starting from the ground, hovering, and various disturbances. A lot of them are very challenging to recover from. The policy must learn this recovery behavior. The gaussian shaping of the randomization parameters is chosen to ensure that there are also sufficient easier initial conditions, such that the policy can learn the basic hover behavior.
\subsubsection{Domain Randomization}
To further improve robustness, we apply domain randomization techniques during training. For most experiments we focus on randomizing the quadrotor's maximum thrust $f_{\max}^i$ within a range of $[0.8,1.0]$ times the nominal value, with some small Gaussian offset for each motor. This not only ensures that the policy can adapt to variations in the quadrotor's  battery level and motor performance, but also brings a certain level of generalization to payload and quadrotor mass.

We also provide the option to randomize the payload mass, cable length, and other parameters, but these were not used in our experiments. Further randomization techniques should be explored in future work, to improve sim-to-real transfer and generalization.




\subsection{Reward Design}
\todo{update reward}
\todo{split quad reward -> payload reward}
The goal of our reward is to encourage flight behaviors that bring the payload to its target at a speed of roughly $ v_{\max} = 0.7m/s$, while simultaneously minimizing payload swing and vehicle tilt, maintaining a taut suspension cable, enforcing safe spacing between quadrotors, and promoting gentle, low-frequency thrust commands for robust sim-to-real transfer. Harsh penalties can induce “learning to terminate” behaviors or unstable gradients. In order to prevent this, we shape all error components with the bounded exponential function

\begin{equation}
\rho(x; s) \;=\;\exp\bigl(-s\,|x|\bigr)\;-\;0.1\,s\,|x|\,,\quad s>0,
\end{equation}
and inject a small constant \(\varepsilon>0\) wherever needed to prevent division by zero.

\subsubsection{Payload Tracking and Velocity Alignment}
Define the payload position error \(\mathbf{e}^P_t = \mathbf{p}^P_t - \mathbf{p}^P_{\mathrm{des},t}\) with norm \(d_t = \|\mathbf{e}^P_t\|\).  The payload velocity is \(\mathbf{v}^P_t\).  We scale velocity conditioned on the distance and set
\begin{equation}
v_{\mathrm{des}}(d_t)
= v_{\max}\bigl(1 - e^{-8\,d_t}\bigr),\quad
\end{equation}
and measure the velocity error
\begin{equation}
\Delta v_t
= \Bigl\|\mathbf{v}^P_t - v_{\mathrm{des}}(d_t)\,\frac{\mathbf{e}^P_t}{d_t + \varepsilon}\Bigr\|.
\end{equation}
The raw tracking reward
\begin{equation}
\tilde r_{\mathrm{track}}
= 2
\;+\;\rho\bigl(d_t; s_d\bigr)
\;+\;\rho\bigl(\Delta v_t; s_v\bigr)
\end{equation}
is clipped at zero to yield
\begin{equation}
r_{\mathrm{track}}=\max\{0,\tilde r_{\mathrm{track}}\},
\end{equation}
ensuring only positive incentive remains.

\subsubsection{Inter-Agent Safe-Distance}
For \(N>1\) quadrotors with planar positions \(\mathbf{p}^i_{t,xy}\), let
\begin{equation}
d_{ij} = \lVert\mathbf{p}^i_{t,xy}-\mathbf{p}^j_{t,xy}\rVert,\quad i\neq j.
\end{equation}
We reward the normalized sum
\begin{equation}
r_{\mathrm{safe}}
= \frac{1}{N(N-1)}\sum_{i\neq j}\mathrm{clip}\!\Bigl(\tfrac{d_{ij}-d_{\min}}{w_d},0,1\Bigr),
\end{equation}
which scales naturally with \(N\). The default \(d_{\min}=0.15\) and \(w_d=0.02\) ensure a minimum safe distance of 15 cm between quadrotors, with a linear penalty for closer approaches.

\subsubsection{Uprightness}
Tilt angles \(\theta^i_t\) between each quad's body-frame “up” axis and gravity are shaped by
\begin{equation}
r_{\mathrm{up}}
= \frac{1}{N}\sum_{i=1}^N \rho\bigl(\theta^i_t; s_\theta\bigr),
\end{equation}
discouraging large roll or pitch that induce instability and payload swing.

\subsubsection{Taut-String Maintenance}
Let \(d^i_t=\lVert\mathbf{p}^i_t-\mathbf{p}^P_t\rVert\) and \(h^i_t=(\mathbf{p}^i_t)_z-(\mathbf{p}^P_t)_z\).  A taut cable of length \(L\) is encouraged by maximizing both terms via
\begin{equation}
r_{\mathrm{taut}}
= \frac{1}{L}\Bigl(\frac{1}{N}\sum_{i=1}^N d^i_t + \frac{1}{N}\sum_{i=1}^N h^i_t\Bigr).
\end{equation}

\subsubsection{Rotor-Frame Velocity Regularization}
For each quad's body-frame angular velocity \(\boldsymbol{\omega}^i_t\) and linear velocity \(\mathbf{v}^i_t\), we define
\begin{equation}
r_{\omega}
= \frac{1}{N}\sum_{i=1}^N \rho\bigl(\lVert\boldsymbol{\omega}^i_t\rVert; s_\omega\bigr),
\quad
r_{v}
= \frac{1}{N}\sum_{i=1}^N \Bigl[\rho\bigl(\lVert\mathbf{v}^i_t\rVert; s_v\bigr)
- c_v\,\mathrm{clip}\bigl(\lVert\mathbf{v}^i_t\rVert - v_{\max},0,\infty\bigr)\Bigr].
\end{equation}

\subsubsection{Collision and Boundary Penalties}
With indicators \(\mathbb{I}_{\mathrm{coll}}\), \(\mathbb{I}_{\mathrm{oob}}\) and a grace factor \(g(t)=\mathrm{clip}(2t,0,1)\), penalties are
\begin{equation}
r_{\mathrm{coll}} = -\,g(t)\,\mathbb{I}_{\mathrm{coll}}, 
\quad
r_{\mathrm{oob}} = -\,g(t)\,\mathbb{I}_{\mathrm{oob}}.
\end{equation}

\subsubsection{Smoothness and Energy Cost}
\begin{figure}[ht]
    \centering
    
    \includegraphics[width=\textwidth]{experiments/actions_comparison.pdf}
    \caption[Policy comparison with/without action regularization]{Comparison of policy trained with and without the action regularization reward. The left plot shows the actions over time of the policy with regularization, while the right plot shows the actions of the policy without regularization. The regularized policy produces smoother and more stable actions, while the unregularized policy exhibits large fluctuations in thrust commands jumping between the action limits.}
    \label{fig:}
\end{figure}
Let \(\mathbf{a}_t,\mathbf{a}_{t-1}\) be consecutive normalized thrust vectors and \(\tilde f_{i,j}\) each rotor's thrust fraction.  We set
\begin{equation}
r_{\mathrm{smooth}}
= -\sum_{i,j} \bigl|a_{t,i,j}-a_{t-1,i,j}\bigr|,
\quad
r_{\mathrm{energy}}
= -\frac{1}{4N}\sum_{i=1}^N\sum_{j=1}^4\bigl[\exp\bigl(-30|\tilde f_{i,j}|\bigr)+\exp\bigl(90(\tilde f_{i,j}-1)\bigr)\bigr].
\end{equation}
The energy cost discourages thrust to be on the limits at 0 or 1, while the smoothness term penalizes large changes in thrust between consecutive timesteps, promoting stable flight.
\subsubsection{Aggregate Reward}
The stability incentive is the average of its five components:
\begin{equation}
r_{\mathrm{stab}}
= \frac{1}{5}\bigl(r_{\mathrm{safe}} + r_{\mathrm{up}} + r_{\mathrm{taut}} + r_{\omega} + r_{v}\bigr),
\end{equation}
and the total penalty is
\begin{equation}
r_{\mathrm{pen}}
= r_{\mathrm{coll}} + r_{\mathrm{oob}} + r_{\mathrm{smooth}} + r_{\mathrm{energy}}.
\end{equation}
Finally, we multiply tracking and stability to enforce simultaneous performance,
\begin{equation}
r_t
= r_{\mathrm{track}}\;r_{\mathrm{stab}}
\;+\;r_{\mathrm{pen}}.
\end{equation}
This product structure ensures the policy learns to track the payload while constantly maintaining stable formations, and the additive penalties discourage unsafe or aggressive behaviors without overwhelming the shaping incentives.

\subsection{Reward Design Considerations}
\todo{write reward design considerations}
\subsection{Integration with JAXMARL}

CrazyMARL integrates seamlessly with the JaxMARL ecosystem \cite{flair2023jaxmarl}.  We wrap each Brax environment in a \texttt{JAXMARL.Mabrax} adapter, exposing standard methods for rollout collection, batching, and replay buffer storage.  The same RL algorithms (e.g., PPO, QMIX) can be applied unchanged to single-agent or decentralized multi-agent tasks, enabling rapid experimentation across a broad class of cooperative aerial transport problems.


\section{Training}
\begin{figure}[ht]
    \centering
    
    \includegraphics[width=\textwidth]{experiments/train_metrics.pdf}
    \caption[Training metrics]{Average training metrics for one quadrotor with payload scenario. The top plot shows the average episode returns and the bottom plot shows the average episode lengths over environment steps and wall-clock time. The shaded area represents the standard deviation across 10 seeds. The maximum possible episode lenght here is 2048 environment steps, which corresponds to 8.2 seconds of flight time. In the end nearly all episodes are completing the episode indicating stable flight.}
    \label{fig:train_metrics}
\end{figure}
\todo{add training metrics comparison}
Our training pipeline follows the approach of \autocite{flair2023jaxmarl}, leveraging the JaxMARL framework as a basis to our training method. We use the \gls{ippo} algorithm, which extends \gls{ppo} to multi-agent settings by optimizing a joint policy across all agents while maintaining decentralized execution.

The decentralized policies are learned end-to-end via \gls{ippo} within our highly parallelized JAX/MJX simulation environment. At each iteration, a fixed number of synchronous actors collect trajectories in $N$ parallel environments over $T$ time steps, yielding $NT$ experiences per update. Collected rewards are bootstrapped with the learned critic to compute targets, and advantages are estimated using \gls{gae} with discount factor $\gamma$ and smoothing parameter $\lambda$. The overall loss combines the clipped policy objective
\begin{equation}
L_{\mathrm{PPO}}(\theta) = \mathbb{E}\!\bigl[\min\bigl(r_t(\theta)\,\hat{A}_t,\;\mathrm{clip}(r_t(\theta),1-\epsilon,1+\epsilon)\,\hat{A}_t\bigr)\bigr],
\end{equation}
a mean-squared value-function error, and an entropy regularizer to encourage sufficient exploration. Gradient norms are clipped to enhance numerical stability.

Since our environment has a certain level of complexity we always train on a high number of parallel environments (16384). This might not be very sampling efficient but allows us to collect a large number of experiences with sufficient variance of configurations in a short time, which is crucial for stable training of policies. A lower number of environments also usually trains a working policy, but the training progress has higher varience and it is more likely to get stuck on local minima for position tracking. On each environment we only step 32 steps per episode. Giving us 128 Minibatches of size 4096 for each episode. The training is performed on a single GPU, which allows us to leverage the high-performance capabilities of JAX and Brax for efficient training. Figure~\ref{fig:train_metrics} shows the training progess of the two quadrotor policy. \todo{maybe add comparison of few env traing to fig}

The training process and hyperparameters are virtually the same for all number of quadrotors, only the policy size is slightly adjusted for more quadrotors. We train the policy on a single RTX Ada 4000 GPU with 20GB of RAM. This gives us training speeds of up to 1 Million environment steps per second, depending on the number of parallel environments and Quadrotors. We trained the policies in our experiments on one Billion environment steps, which corresponds to roughly 10 minutes of training time for the single quadrotor task and 20 minutes for the dual quadrotor task and 60 minutes for the three quadrotor tasks. We are also able to train a single quadotor policy without payload in less than 3 minutes. \todo{check numbers again.}

\subsection{Policy Architecture}
Both actor and critic networks are instantiated as fully connected, feed-forward multilayer perceptrons. The actor network comprises an input layer matching the local observation dimension, three hidden layers of 64 units with tanh activations, and a linear output layer producing the action-mean vector. A learned log-standard deviation vector of the same dimensionality parameterizes a diagonal Gaussian policy. The critic network features three hidden layers of 128 tanh units each, terminating in a scalar output. All dense layers employ orthogonal weight initialization and zero biases. During training and rollouts, actions are sampled stochastically from the policy distribution, at evaluation time, the deterministic mean action is used.

\subsection{Hyperparameters}
We run a Bayesian optimization sweep, with early termination of runs, over the hyperparameters to find the best configuration for our task. We first conduct a coarse search over a wide range of values, then refine the search around the most promising regions. The final hyperparameters are selected based on the best performing policy with the highest reward. Table~\ref{tab:hyperparams} summarizes the key hyperparameters used in our experiments.

\begin{table}[ht]
  \centering
  \caption{Key hyperparameters for training.}
  \label{tab:hyperparams}
  \begin{tabular}{@{}ll@{}}
    \toprule
    Hyperparameter & Value \\
    \midrule
    Total environment timesteps & $1{,}000{,}000{,}000$ \\
    Number of environments & 16,384 \\
    Number of steps per update & 32 \\
    Number of minibatches & 128 \\
    Update epochs & 8 \\
    Anneal learning rate & false \\
    Learning rate & $4\times10^{-4}$ \\
    Activation function & \texttt{tanh} \\
    Entropy coefficient & 0.01 \\
    Discount factor ($\gamma$) & 0.995 \\
    GAE lambda ($\lambda$) & 0.95 \\

    Actor network architecture & [64, 64, 64] \\
    Critic network architecture & [128, 128, 128] \\
    \bottomrule
  \end{tabular}
\end{table}

% \subsubsection{Hardware Rollout}

% \subsubsection{Comparing Centralized PPO, IPPO, and MAPPO}
% We compare three variants of the \gls{ppo} algorithm: centralized \gls{ppo}, decentralized \gls{ippo}, and multi-agent \gls{mappo}. The centralized variant uses a shared critic for all agents, while \gls{ippo} and \gls{mappo} maintain decentralized policies with independent critics. 
% \begin{itemize}
%     \item 3 Plots with training (episode-length over steps) with 3 different seeds each
% \end{itemize}

