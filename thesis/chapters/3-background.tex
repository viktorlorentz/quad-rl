\chapter{Background}

\section{Multi-Agent Reinforcement Learning}

This section provides an overview of the theoretical background of \gls{marl} as applied in this work. Our work evaluates both centralized and decentralized training paradigms. In particular, we employ Proximal Policy Optimization (PPO) for centralized training, while also exploring decentralized approaches using Independent PPO (IPPO) and Multi-Agent PPO (MAPPO), which are detailed in subsequent sections.

\subsection{Fundamentals of Reinforcement Learning}
The term \gls{rl} is used for a wide range of algorithms and methods that learn to make decisions by interacting with an environment. The following will give a brief overview of the most important concepts and terms in \gls{rl}. In particular, this work focuses on model-free, on-policy, policy-optimization algorithms, where the policy used to collect data is the same one being updated. For a thorough treatment of the fundamentals see \cite{SuttonBarto2018}.
\subsubsection{Markov Decision Processes}
\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{images/rl_diagram.png}
\caption{TODO   Reinforcement learning framework. The agent interacts with the environment by taking actions and receiving rewards, while the environment transitions to new states based on the agent's actions.}
\label{fig:rl_diagram}
\end{figure}

\gls{rl} is a framework in which an agent learns to make decisions by interacting with its environment.  This interaction is commonly modeled as a \gls{mdp}, defined by the tuple
\[
(\mathcal{S}, \mathcal{A}, P, r, \gamma),
\]
where
\begin{itemize}
  \item \(\mathcal{S}\) is the set of all possible states,
  \item \(\mathcal{A}\) is the set of all possible actions,
  \item \(P(s'\!\mid\!s,a)\) is the probability of transitioning to \(s'\) from \(s\) when taking \(a\),
  \item \(r(s,a)\) is the immediate reward for taking action \(a\) in state \(s\),
  \item \(\gamma\in[0,1)\) is the discount factor, which down‐weights future rewards.
\end{itemize}

At each time step \(t\), the agent interacts with the environment as also shown in Figure~\ref{fig:rl_diagram}. The agent:
\begin{enumerate}
  \item observes \(s_t\),
  \item samples \(a_t\sim\pi_\theta(\cdot\mid s_t)\),
  \item receives \(r_t = r(s_t,a_t)\),
  \item transitions to \(s_{t+1}\sim P(\cdot\mid s_t,a_t)\).
\end{enumerate}
In practice the agent often does not have access to the full state \(s_t\) but only to a partial observation \(o_t\).  We will use the notation interchangeably, i.e.\ \(o_t\) or \(s_t\) depending on the context, unless otherwise specified.
\subsubsection{Policies}
As already mentioned, the agent's behavior is defined by a policy \(\pi_\theta\), which can be either deterministic or stochastic:
 
    \[
      a_t = \pi_{\theta}(s_t),
    \]
    where \(\pi_{\theta}\) is a deterministic function (e.g., a neural network) mapping state \(s_t\) to action \(a_t\).

    \[
      a_t \sim \pi_{\theta}(\,\cdot\mid s_t),
    \]
    where \(\pi_{\theta}(a\mid s_t)\) is the probability of taking action \(a\) in state \(s_t\).
We parameterize \(\pi_{\theta}\) with \(\theta\) and optimize it to maximize the expected return. Typically, the policy is represented by a neural network taking \(s_t\) as input.

\subsubsection{Reward and Return}
The agent's goal is to maximize the expected return, which is the total discounted reward over time. 
The reward function \(r_t(s_t,a_t)\) provides immediate feedback to the agent and usually depends on the current state \(s\), action taken by the policy \(a\) and the next state \(s'\). The return \(R_t\) at time step \(t\) is defined as the sum of discounted rewards:
\[
R_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k},
\]
where \(\gamma\) is the discount factor that determines the importance of future rewards. The expected return \(J(\pi)\) is defined as the expected value of the return:

\[
J(\pi) = \mathbb{E}\Bigl[\sum_{t=0}^\infty \gamma^t\,R_t\Bigr].
\]

At every time step \(t\), the agent observes the current state \(s_t\), selects an action \(a_t\) according to a policy \(\pi(a_t\mid s_t)\), receives an immediate reward \(r(s_t,a_t)\), and transitions to the next state \(s_{t+1}\) based on the dynamics \(P\). The goal is to find the optimal policy \(\pi^*\) that maximizes the expected total discounted reward if the agent acts according to it. This formulation can be explicitly stated as the following constrained optimization problem:
\begin{equation}\label{eq:RL_opt}
\begin{aligned}
\text{Find} \quad & \pi^* = \arg\max_{\pi} \mathbb{E}\left[J(\pi)\right] \\
\text{subject to} \quad & s_0 \sim \rho_0, \\
& a_t \sim \pi(\cdot \mid s_t), \quad \forall\ t \in T \\
& s_{t+1} \sim P(\cdot \mid s_t, a_t)
\end{aligned}
\end{equation}

\subsubsection{Value and Action‐Value Functions}

Given a fixed policy \(\pi_\theta\), we introduce two central quantities that measure the expected return from states and state–action pairs.

\begin{description}
  \item[State‐Value \(V^\pi\):]
    \[
      V^\pi(s)
      = \mathbb{E}\Bigl[\sum_{t=0}^{\infty}\gamma^t\,r_t
      \;\Big|\;s_0 = s\Bigr],
    \]
    the expected total discounted reward when starting in state \(s\) and thereafter following \(\pi_\theta\).
  
  \item[Action‐Value \(Q^\pi\):]
    \[
      Q^\pi(s,a)
      = \mathbb{E}\Bigl[\sum_{t=0}^{\infty}\gamma^t\,r_t
      \;\Big|\;s_0 = s,\;a_0 = a\Bigr],
    \]
    the expected total discounted reward when starting in \(s\), taking action \(a\), then following \(\pi_\theta\).
\end{description}

The Bellman equations decompose each function into reward received until the current state and the discounted value of the next state and action:

\begin{align}
\label{eq:BellmanV}
V^\pi(s)
&= \mathbb{E}_{a\sim\pi(\cdot\mid s)}\bigl[r(s,a)\bigr]
  + \gamma\,\mathbb{E}_{\substack{a\sim\pi(\cdot\mid s)\\s'\sim P(\cdot\mid s,a)}}\bigl[V^\pi(s')\bigr],\\[4pt]
\label{eq:BellmanQ}
Q^\pi(s,a)
&= \mathbb{E}_{s'\sim P(\cdot\mid s,a)}\bigl[r(s,a)\bigr]
  + \gamma\,\mathbb{E}_{\substack{s'\sim P(\cdot\mid s,a)\\a'\sim\pi(\cdot\mid s')}}\bigl[Q^\pi(s',a')\bigr].
\end{align}

The Bellman equations can be combined to give us the advantage function \(A^\pi\), which quantifies the relative value of taking action \(a\) in state \(s\) compared to the average value of the state:
\begin{align}
\label{eq:advantage}
A^\pi(s,a)
&= Q^\pi(s,a) - V^\pi(s)
\end{align}

\subsubsection{On‐Policy Policy‐Gradient Loop}  
On‐policy methods repeatedly collect fresh data under \(\pi_\theta\) to perform stochastic gradient ascent on \(J(\theta)\) to ultimately improve the policy towards \(\pi^*\). The gradient can be derived from the advantage function \(A^\pi\) as
\begin{align}
\label{eq:policy_gradient}
\nabla_\theta J(\theta)
&= \mathbb{E}\Bigl[\sum_{t=0}^\infty \gamma^t\,\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,A_t\Bigr],
\end{align}
In practice, there are different approaches to approximate the policy gradient \(\nabla_{\theta}J(\theta)\). In the next section, we will focus on the \gls{ppo} algorithm, which is a popular on‐policy method, using a surrogate objective function to optimize the policy.

\subsection{PPO}
\gls{ppo} was introduced by Schulman et al. in \cite{schulman2017proximal}. \gls{ppo} is a family of policy gradient methods that strives for simplicity, efficiency, and robustness. Its central idea is to iteratively improve a stochastic policy by alternating between sampling data from the environment and performing several epochs of first-order optimization on a surrogate objective function. It is an on-policy algorithm.

In \gls{ppo}, actions are sampled from a parameterized probability distribution. For continuous control tasks, the actor typically outputs the parameters of a Gaussian distribution (usually the mean and standard deviation), and actions are sampled as
\[
a_t \sim \mathcal{N}\big(\mu_\theta(s_t),\, \sigma_\theta(s_t)\big).
\]
This stochastic sampling is crucial as it enables the agent to explore a continuous range of actions, thereby promoting both exploration and robustness in learning.

\gls{ppo} builds on the actor-critic framework, where two components are optimized simultaneously. The actor represents the policy \(\pi_\theta\), which selects actions, while the critic estimates a value function \(V_\theta(s)\) that evaluates the quality of states under the current policy. The critic is used to compute an advantage function that quantifies the relative benefit of taking a specific action in a given state. A commonly used method for estimating advantages is \gls{gae}, which effectively reduces variance while introducing a manageable bias that aids learning stability. \gls{gae} computes the advantage as
\[
\hat{A}_t = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l},
\]
with the temporal difference error defined as
\[
\delta_t = r_t + \gamma V_\theta(s_{t+1}) - V_\theta(s_t).
\]
Here, \(r_t\) is the reward received at time \(t\), \(V_\theta(s_t)\) is the estimated value of state \(s_t\) under the current policy, \(\gamma \in [0,1]\) is the discount factor that weighs future rewards, and \(\lambda \in [0,1]\) is a parameter that adjusts the trade-off between bias and variance in the advantage estimates.

A key contribution of \gls{ppo} is the use of a clipped surrogate objective designed to restrict the size of policy updates. Let
\[
r_t(\theta) = \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)}
\]
denote the probability ratio between the new and the old policies. The clipped surrogate objective is then defined as:
\[
L^{CLIP}(\theta) = \mathbb{E}_t\!\left[\min\!\left(r_t(\theta)\hat{A}_t,\;\text{clip}\left(r_t(\theta),\,1-\epsilon,\,1+\epsilon\right)\hat{A}_t\right)\right],
\]
where \(\hat{A}_t\) is an estimator of the advantage function and \(\epsilon\) is a hyperparameter defining the clipping range. This objective penalizes overly large deviations from the previous policy, ensuring that updates remain conservative while still allowing for meaningful improvements.

In practice, \gls{ppo} combines the policy surrogate loss with additional terms, such as a value function loss and an entropy bonus, yielding a composite objective:
\[
L(\theta) = \mathbb{E}_t \left[ L^{CLIP}_t(\theta) - c_1\, \left(V_\theta(s_t) - V_t^{\text{target}}\right)^2 + c_2\, S\big[\pi_\theta\big](s_t) \right],
\]
where \(c_1\) and \(c_2\) are coefficients that weight the contributions of the value function error and the entropy bonus \(S\big[\pi_\theta\big](s_t)\), respectively. This final objective is optimized using stochastic gradient ascent over multiple epochs on the same batch of on-policy samples.

The overall procedure of \gls{ppo} in an actor-critic setting is summarized in Algorithm~\ref{alg:ppo}. In the algorithm, \(\theta\) denotes the current policy parameters, \(\theta_{\text{old}}\) are the parameters used for generating the on-policy data, \(N\) is the number of parallel actors, \(T\) is the number of timesteps per actor rollout (with \(NT\) total timesteps per batch), \(K\) is the number of epochs over the data, and \(M \le NT\) is the minibatch size.


\begin{algorithm}[H]
\caption{\gls{ppo}, Actor-Critic Style}
\label{alg:ppo}
\begin{algorithmic}[1]
\For{iteration = 1, 2, \dots}
    \For{actor = 1, 2, \dots, N}
        \State Run policy \(\pi_{\theta_{\text{old}}}\) in the environment for \(T\) timesteps.
        \State Compute advantage estimates \(\hat{A}_1, \dots, \hat{A}_T\).
    \EndFor
    \State Optimize the surrogate loss \(L(\theta)\) with respect to \(\theta\) using \(K\) epochs and minibatch size \(M \le NT\).
    \State Update \(\theta_{\text{old}} \leftarrow \theta\).
\EndFor
\end{algorithmic}
\end{algorithm}

Overall, \gls{ppo} strikes a favorable balance between simplicity and performance, making it one of the most widely adopted on-policy algorithms in modern reinforcement learning applications.
\subsection{IPPO}
In single-agent reinforcement learning, the environment is modeled as a \gls{mdp} with the tuple \((\mathcal{S}, \mathcal{A}, P, r, \gamma)\). In \gls{marl}, multiple agents interact with the environment simultaneously, which naturally leads to an extended framework. A common formalism is the Markov game (or stochastic game), which generalizes the MDP to handle multiple decision-makers. In this setting, the environment is defined by the tuple
\[
(\mathcal{S}, \{\mathcal{A}_i\}_{i=1}^n, P, \{r_i\}_{i=1}^n, \gamma),
\]
where:
\begin{itemize}
    \item \(\mathcal{S}\) is the set of states,
    \item \(\mathcal{A}_i\) is the set of actions available to agent \(i\), with the joint action space given by \(\mathcal{A} = \mathcal{A}_1 \times \cdots \times \mathcal{A}_n\),
    \item \(P(s' \mid s, \mathbf{a})\) denotes the probability of transitioning from state \(s\) to state \(s'\) given the joint action \(\mathbf{a} = (a_1, \dots, a_n)\),
    \item \(r_i(s, \mathbf{a})\) is the reward function for agent \(i\) (which can be different for each agent), and
    \item \(\gamma \in [0,1)\) is the common discount factor.
\end{itemize}

% In this multi-agent setting, the optimal policy for each agent generally depends on the policies of the other agents, which makes the learning process inherently non-stationary from an individual agent's perspective. Depending on whether the agents have aligned or conflicting interests, problems can be classified as cooperative, competitive, or general-sum games. In cases where each agent has only partial observability of the environment, the model is further extended to decentralized partially observable Markov decision processes (Dec-POMDPs).

\subsection{MAPPO}

\section{Quadrotor Control}
\subsection{Quadrotor Dynamics}
\subsection{Quadrotor Simulation}
\subsection{Quadrotor Classic Control}
\subsection{Quadrotor With Payloads}

\section{Problem Formulation}


