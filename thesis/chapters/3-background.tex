\chapter{Background}

\section{Multi-Agent Reinforcement Learning}

This section provides an overview of the theoretical background of Multi-Agent Reinforcement Learning (MARL) as applied in this work. Our work evaluates both centralized and decentralized training paradigms. In particular, we employ Proximal Policy Optimization (PPO) for centralized training, while also exploring decentralized approaches using Independent PPO (IPPO) and Multi-Agent PPO (MAPPO), which are detailed in subsequent sections.

\subsection{Fundamentals of Reinforcement Learning}

\gls{rl} is a framework in which an agent learns to make decisions by interacting with its environment. This interaction is commonly modeled as an \gls{mdp}, which is defined by the tuple
\[
(\mathcal{S}, \mathcal{A}, P, r, \gamma),
\]
where \(\mathcal{S}\) is the set of states, \(\mathcal{A}\) is the set of actions, \(P(s'\mid s,a)\) denotes the probability of transitioning from state \(s\) to state \(s'\) given action \(a\), \(r(s,a)\) represents the reward received when taking action \(a\) in state \(s\), and \(\gamma \in [0,1)\) is the discount factor, which reduces the weight of future rewards.

At every time step \(t\), the agent observes the current state \(s_t\), selects an action \(a_t\) according to a policy \(\pi(a_t\mid s_t)\), receives an immediate reward \(r(s_t,a_t)\), and transitions to the next state \(s_{t+1}\) based on the dynamics \(P\). The goal is to find the optimal policy \(\pi^*\) that maximizes the expected total discounted reward, i.e.,
\[
\pi^* = \arg\max_{\pi} J(\pi), \qquad\text{where}\qquad J(\pi) = \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t r(s_t, a_t)\right].
\]

This formulation can be explicitly stated as the following constrained optimization problem:
\begin{equation}\label{eq:RL_opt}
\begin{aligned}
\text{Find} \quad & \pi^* = \arg\max_{\pi} \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t r(s_t, a_t)\right] \\
\text{subject to} \quad & s_0 \sim \rho_0, \\
& a_t \sim \pi(\cdot \mid s_t), \quad t = 0,1,2,\ldots, \\
& s_{t+1} \sim P(\cdot \mid s_t, a_t), \quad t = 0,1,2,\ldots, \\
& \pi(a \mid s) \geq 0,\quad \forall\, s \in \mathcal{S},\, a \in \mathcal{A}, \\
& \sum_{a \in \mathcal{A}} \pi(a \mid s) = 1,\quad \forall\, s \in \mathcal{S}.
\end{aligned}
\end{equation}

Many \gls{rl} algorithms operate through the following iterative procedure:

\begin{enumerate}
    \item \textbf{Data Collection:} Execute the current policy \(\pi\) to collect trajectories of states, actions, and rewards.
    \item \textbf{Return Estimation:} For each time step, compute the cumulative future reward using:
    \begin{equation}
    R_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k}.
    \end{equation}
    \item \textbf{Value Function Computation:} Estimate the following:
    \begin{itemize}
        \item \emph{State-value function:}
        \begin{equation}
        V^\pi(s) = \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t r_t \,\bigg|\, s_0 = s\right].
        \end{equation}
        \item \emph{Action-value function (Q-function):}
        \begin{equation}
        Q^\pi(s, a) = \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t r_t \,\bigg|\, s_0 = s,\, a_0 = a\right].
        \end{equation}
    \end{itemize}
    \item \textbf{Advantage Estimation:} Determine the relative benefit of taking an action \(a_t\) in state \(s_t\) by computing:
    \begin{equation}
    \hat{A}_t = Q^\pi(s_t, a_t) - V^\pi(s_t).
    \end{equation}
    \item \textbf{Policy Update:} Adjust the policy parameters \(\theta\) by ascending the gradient:
    \begin{equation}
    \hat{g} = \mathbb{E}_t\left[\nabla_\theta \log \pi_\theta(a_t \mid s_t) \, \hat{A}_t\right].
    \end{equation}
    \item \textbf{Iteration:} With the updated policy, collect new data and repeat the process until the performance meets the desired criteria.
\end{enumerate}

The above steps outline the general framework of \gls{rl} algorithms. However, the specific implementation details can vary significantly between different algorithms. For instance, some methods may utilize function approximation to represent the policy and value functions, while others may rely on tabular representations. Additionally, the choice of exploration strategy, reward shaping, and experience replay can also influence the performance of the algorithm.

For a thorough treatment of these fundamentals, see \cite{SuttonBarto2018}.



\subsection{PPO}
\gls{ppo} was introduced by Schulman et al. in \cite{schulman2017proximal}. \gls{ppo} is a family of policy gradient methods that strives for simplicity, efficiency, and robustness. Its central idea is to iteratively improve a stochastic policy by alternating between sampling data from the environment and performing several epochs of first-order optimization on a surrogate objective function. It is an on-policy algorithm; that is, the trajectories used for the updates are generated by the current (or very recent) policy.

In \gls{ppo}, actions are sampled from a parameterized probability distribution. For continuous control tasks, the actor typically outputs the parameters of a Gaussian distribution (usually the mean and standard deviation), and actions are sampled as 
\[
a_t \sim \mathcal{N}\big(\mu_\theta(s_t),\, \sigma_\theta(s_t)\big).
\]
This stochastic sampling is crucial as it enables the agent to explore a continuous range of actions, thereby promoting both exploration and robustness in learning.

\gls{ppo} builds on the actor-critic framework, where two components are optimized simultaneously. The \emph{actor} represents the policy \(\pi_\theta\), which selects actions, while the \emph{critic} estimates a value function \(V_\theta(s)\) that evaluates the quality of states under the current policy. The critic is used to compute an advantage function that quantifies the relative benefit of taking a specific action in a given state. A commonly used method for estimating advantages is Generalized Advantage Estimation (GAE), which effectively reduces variance while introducing a manageable bias that aids learning stability. GAE computes the advantage as
\[
\hat{A}_t = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l},
\]
with the temporal difference error defined as
\[
\delta_t = r_t + \gamma V_\theta(s_{t+1}) - V_\theta(s_t).
\]
Here, \(r_t\) is the reward received at time \(t\), \(V_\theta(s_t)\) is the estimated value of state \(s_t\) under the current policy, \(\gamma \in [0,1]\) is the discount factor that weighs future rewards, and \(\lambda \in [0,1]\) is a parameter that adjusts the trade-off between bias and variance in the advantage estimates.

A key contribution of \gls{ppo} is the use of a \emph{clipped surrogate objective} designed to restrict the size of policy updates. Let
\[
r_t(\theta) = \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)}
\]
denote the probability ratio between the new and the old policies. The clipped surrogate objective is then defined as:
\[
L^{CLIP}(\theta) = \mathbb{E}_t\!\left[\min\!\left(r_t(\theta)\hat{A}_t,\;\text{clip}\left(r_t(\theta),\,1-\epsilon,\,1+\epsilon\right)\hat{A}_t\right)\right],
\]
where \(\hat{A}_t\) is an estimator of the advantage function and \(\epsilon\) is a hyperparameter defining the clipping range. This objective penalizes overly large deviations from the previous policy, ensuring that updates remain conservative while still allowing for meaningful improvements.

In practice, \gls{ppo} combines the policy surrogate loss with additional terms, such as a value function loss and an entropy bonus, yielding a composite objective:
\[
L(\theta) = \mathbb{E}_t \left[ L^{CLIP}_t(\theta) - c_1\, \left(V_\theta(s_t) - V_t^{\text{target}}\right)^2 + c_2\, S\big[\pi_\theta\big](s_t) \right],
\]
where \(c_1\) and \(c_2\) are coefficients that weight the contributions of the value function error and the entropy bonus \(S\big[\pi_\theta\big](s_t)\), respectively. This final objective is optimized using stochastic gradient ascent over multiple epochs on the same batch of on-policy samples.

The overall procedure of \gls{ppo} in an actor-critic setting is summarized in Algorithm~\ref{alg:ppo}. In the algorithm, \(\theta\) denotes the current policy parameters, \(\theta_{\text{old}}\) are the parameters used for generating the on-policy data, \(N\) is the number of parallel actors, \(T\) is the number of timesteps per actor rollout (with \(NT\) total timesteps per batch), \(K\) is the number of epochs over the data, and \(M \le NT\) is the minibatch size.


\begin{algorithm}[H]
\caption{\gls{ppo}, Actor-Critic Style}
\label{alg:ppo}
\begin{algorithmic}[1]
\For{iteration = 1, 2, \dots}
    \For{actor = 1, 2, \dots, N}
        \State Run policy \(\pi_{\theta_{\text{old}}}\) in the environment for \(T\) timesteps.
        \State Compute advantage estimates \(\hat{A}_1, \dots, \hat{A}_T\).
    \EndFor
    \State Optimize the surrogate loss \(L(\theta)\) with respect to \(\theta\) using \(K\) epochs and minibatch size \(M \le NT\).
    \State Update \(\theta_{\text{old}} \leftarrow \theta\).
\EndFor
\end{algorithmic}
\end{algorithm}

Overall, \gls{ppo} strikes a favorable balance between simplicity and performance, making it one of the most widely adopted on-policy algorithms in modern reinforcement learning applications.
\subsection{IPPO}
In single-agent reinforcement learning, the environment is modeled as a \gls{mdp} with the tuple \((\mathcal{S}, \mathcal{A}, P, r, \gamma)\). In \gls{marl}, multiple agents interact with the environment simultaneously, which naturally leads to an extended framework. A common formalism is the Markov game (or stochastic game), which generalizes the MDP to handle multiple decision-makers. In this setting, the environment is defined by the tuple
\[
(\mathcal{S}, \{\mathcal{A}_i\}_{i=1}^n, P, \{r_i\}_{i=1}^n, \gamma),
\]
where:
\begin{itemize}
    \item \(\mathcal{S}\) is the set of states,
    \item \(\mathcal{A}_i\) is the set of actions available to agent \(i\), with the joint action space given by \(\mathcal{A} = \mathcal{A}_1 \times \cdots \times \mathcal{A}_n\),
    \item \(P(s' \mid s, \mathbf{a})\) denotes the probability of transitioning from state \(s\) to state \(s'\) given the joint action \(\mathbf{a} = (a_1, \dots, a_n)\),
    \item \(r_i(s, \mathbf{a})\) is the reward function for agent \(i\) (which can be different for each agent), and
    \item \(\gamma \in [0,1)\) is the common discount factor.
\end{itemize}

% In this multi-agent setting, the optimal policy for each agent generally depends on the policies of the other agents, which makes the learning process inherently non-stationary from an individual agent's perspective. Depending on whether the agents have aligned or conflicting interests, problems can be classified as cooperative, competitive, or general-sum games. In cases where each agent has only partial observability of the environment, the model is further extended to decentralized partially observable Markov decision processes (Dec-POMDPs).

\subsection{MAPPO}

\section{Quadrotor Control}
\subsection{Quadrotor Dynamics}
\subsection{Quadrotor Simulation}
\subsection{Quadrotor Classic Control}
\subsection{Quadrotor With Payloads}

\section{Problem Formulation}


