\chapter{Background}
The methods developed in this thesis build on foundational concepts from \gls{rl}, \gls{marl}, and quadrotor dynamics. We first review model-free, on-policy policy-optimization algorithms, introducing \gls{mdp}s, policy gradients, and \gls{ppo}. We then extend these ideas to cooperative multi-agent tasks, comparing centralized critics with fully decentralized critics. Next, we present the mathematical model of a quadrotor \gls{uav}, covering coordinate frames, state representation, sensor fusion, continuous-time dynamics, and common action-space formulations ranging from velocity commands to direct thrust control. Finally, we explain how the quadrotor state is augmented to include cable-suspended payloads—first via a simple pendulum model and then through multi-link or tendon-based approximations—and how these formulations generalize to cooperative transport by multiple \glspl{uav}. These elements are essential for developing a deep \gls{rl} framework for decentralized multi-\gls{uav} cable-suspended payload transport.

\section{Fundamentals of Reinforcement Learning}
\gls{rl} refers to algorithms that learn to make decisions by interacting with an environment. This work focuses on model-free, on-policy, policy-optimization methods, where the policy used to collect data is the same one being updated. For a comprehensive treatment, see \cite{SuttonBarto2018}.

\subsection{Markov Decision Processes}
\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{images/rl_diagram.png}
\caption{TODO   Reinforcement learning framework. The agent interacts with the environment by taking actions and receiving rewards, while the environment transitions to new states based on the agent's actions.}
\label{fig:rl_diagram}
\end{figure}
\gls{rl} problems are commonly framed as \gls{mdp}s, defined by the tuple \((\mathcal{S}, \mathcal{A}, P, r, \gamma)\), where \(\mathcal{S}\) is the set of states, \(\mathcal{A}\) is the set of actions, \(P(s'\!\mid\!s,a)\) is the transition probability, \(r(s,a)\) is the immediate reward, and \(\gamma \in [0,1)\) is the discount factor. At each time step \(t\), the agent observes \(s_t\), samples and executes \(a_t \sim \pi_\theta(\cdot\mid s_t)\), receives \(r_t = r(s_t,a_t)\), and transitions to \(s_{t+1} \sim P(\cdot\mid s_t,a_t)\). This procedure is visualized in figure \ref{fig:rl_diagram}. In practice, the agent often has access only to a partial observation \(o_t\). We use \(o_t\) or \(s_t\) interchangeably depending on context.

\subsection{Policies}
The agent's behavior is defined by a policy \(\pi_\theta\), parameterized by \(\theta\). A deterministic policy outputs
$ 
a_t = \pi_{\theta}(s_t),
$
while a stochastic policy samples
$
a_t \sim \pi_{\theta}(\,\cdot\mid s_t).
$
We optimize \(\pi_\theta\) to maximize the expected return, typically representing it as a neural network mapping states to action distributions.

\subsection{Reward and Return}
The agent's goal is to maximize the expected return, which is the total discounted reward over time. 
The reward function \(r_t(s_t,a_t)\) provides immediate feedback to the agent and usually depends on the current state \(s\), action taken by the policy \(a\) and the next state \(s'\). The return \(R_t\) at time step \(t\) is defined as the sum of discounted rewards:
\[
R_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k},
\]
where \(\gamma\) is the discount factor that determines the importance of future rewards. The expected return \(J(\pi)\) is defined as the expected value of the return:

\[
J(\pi) = \mathbb{E}\Bigl[\sum_{t=0}^\infty \gamma^t\,R_t\Bigr].
\]
and the goal is to find \(\pi^* = \arg\max_\pi J(\pi)\). Formally,
\begin{equation}\label{eq:RL_opt}
\begin{aligned}
\text{maximize} \quad & \mathbb{E}\Bigl[\sum_{t=0}^\infty \gamma^t\,r(s_t,a_t)\Bigr] \\
\text{subject to} \quad & s_0 \sim \rho_0, \quad a_t \sim \pi(\cdot\mid s_t), \quad s_{t+1} \sim P(\cdot\mid s_t,a_t).
\end{aligned}
\end{equation}

\subsection{Value and Action-Value Functions}
Under a fixed policy \(\pi_\theta\), the state-value function \(V^\pi(s)\) and the action-value function \(Q^\pi(s,a)\) measure the expected discounted return:
\[
V^\pi(s) = \mathbb{E}\Bigl[\sum_{t=0}^\infty \gamma^t r_t \;\big|\; s_0 = s\Bigr], 
\qquad
Q^\pi(s,a) = \mathbb{E}\Bigl[\sum_{t=0}^\infty \gamma^t r_t \;\big|\; s_0 = s,\,a_0 = a\Bigr].
\]

Here, \(V^\pi(s)\) is the expected discounted return starting from state \(s\) under \(\pi_\theta\), and \(Q^\pi(s,a)\) is the expected discounted return starting from \(s\), taking action \(a\), then following \(\pi_\theta\).
The Bellman equations decompose these functions into immediate reward plus the discounted value of the subsequent state:
\begin{align}
V^\pi(s)
&= \mathbb{E}_{a\sim\pi(\cdot\mid s)}\bigl[r(s,a)\bigr]
  + \gamma\,\mathbb{E}_{s'\sim P(\cdot\mid s,a)}\bigl[V^\pi(s')\bigr],\\
Q^\pi(s,a)
&= \mathbb{E}_{s'\sim P(\cdot\mid s,a)}\bigl[r(s,a)\bigr]
  + \gamma\,\mathbb{E}_{a'\sim\pi(\cdot\mid s')}\bigl[Q^\pi(s',a')\bigr].
\end{align}
The advantage function \(A^\pi(s,a)\) quantifies the relative value of taking action \(a\) in state \(s\) compared to the average value of \(s\):
\[
A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s).
\]

\subsection{On-Policy Policy-Gradient Methods}
On-policy methods collect data under the current policy \(\pi_\theta\) and perform gradient ascent on \(J(\theta)\)to ultimately improve the policy towards \(\pi^*\). The policy gradient can be expressed using the advantage function:
\[
\nabla_\theta J(\theta)
= \mathbb{E}\Bigl[\sum_{t=0}^\infty \gamma^t\,\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,A_t\Bigr].
\]
In practice, there are different approaches to approximate the policy gradient \(\nabla_{\theta}J(\theta)\). In the next section, we will focus on the \gls{ppo} algorithm, which is a popular on-policy method, using a surrogate objective function to optimize the policy.
\subsection{Proximal Policy Optimization}
\gls{ppo} \cite{schulman2017proximal} alternates between sampling trajectories using the current policy and performing multiple epochs of stochastic gradient ascent on a clipped surrogate objective. For continuous control, the actor network outputs parameters of a Gaussian distribution,
\[
a_t \sim \mathcal{N}\bigl(\mu_\theta(s_t),\,\sigma_\theta(s_t)\bigr),
\]
Here, the actor network (parameterized by \(\theta\)) maps a state \(s_t\) to a distribution over actions, thereby defining the policy \(\pi_\theta(a_t \mid s_t)\). This stochastic sampling is crucial as it enables the agent to explore a continuous range of actions, promoting both exploration and robustness in learning.

\gls{ppo} builds on the actor-critic framework, where two neural networks are optimized simultaneously. The actor and the critic. The critic network, often parameterized by \(\phi\), estimates a value function \(V_\phi(s)\) that evaluates the quality of states, or more precisely the expected return from \(s\) under the current policy. The critic's estimate is used to compute an advantage function that quantifies the relative benefit of taking a specific action in a given state. A commonly used method for estimating advantages is \gls{gae}, which effectively reduces variance while introducing a manageable bias that aids learning stability. \gls{gae} computes the advantage as
\[
\hat{A}_t = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}, 
\quad
\delta_t = r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t).
\]
Here, \(r_t\) is the reward received at time \(t\), \(V_\phi(s_t)\) is the estimated value of state \(s_t\) from the critic network under the current policy, \(\gamma \in [0,1]\) is the discount factor that weighs future rewards, and \(\lambda \in [0,1]\) is a parameter that adjusts the trade-off between bias and variance in the advantage estimates.

A key contribution of \gls{ppo} is the use of a clipped surrogate objective designed to restrict the size of policy updates. We can define the probability ratio between the new and the old policies
\[
r_t(\theta) = \frac{\pi_\theta(a_t\mid s_t)}{\pi_{\theta_{\text{old}}}(a_t\mid s_t)},
\]
Now, the clipped surrogate objective is
\[
L^{\text{CLIP}}(\theta) = \mathbb{E}_t\!\Bigl[\min\!\bigl(r_t(\theta)\,\hat{A}_t,\;\text{clip}(r_t(\theta),1-\epsilon,1+\epsilon)\,\hat{A}_t\bigr)\Bigr],
\]
where \(\hat{A}_t\) is an estimator of the advantage function and \(\epsilon\) is a hyperparameter defining the clipping range. This objective penalizes overly large deviations from the previous policy, ensuring that updates remain conservative while still allowing for meaningful improvements.

In practice, \gls{ppo} combines the policy surrogate loss with additional terms, such as a value function loss, which is computed by the critic network, and an entropy bonus, yielding a composite objective:
\[
L(\theta,\phi) = \mathbb{E}_t\Bigl[L^{\text{CLIP}}(\theta)
  - c_1\,\bigl(V_\phi(s_t) - V_t^{\text{target}}\bigr)^2
  + c_2\,S\bigl[\pi_\theta\big](s_t)\Bigr],
\]
where \(c_1\) and \(c_2\) are coefficients that weight the contributions of the value function error and the entropy bonus \(S\big[\pi_\theta\big](s_t)\), respectively. The term \(\left(V_\phi(s_t) - V_t^{\text{target}}\right)^2\) is the critic's squared error against the target value \(V_t^{\text{target}}\). This final objective is optimized using stochastic gradient ascent over multiple epochs on the same batch of on-policy samples, updating both actor parameters \(\theta\) and critic parameters \(\phi\).

The overall procedure of \gls{ppo} in an actor-critic setting is summarized in Algorithm~\ref{alg:ppo}. In the algorithm, \(\theta\) denotes the current policy parameters, \(\theta_{\text{old}}\) are the parameters used for generating the on-policy data, \(N\) is the number of parallel actors, \(T\) is the number of timesteps per actor rollout (with \(NT\) total timesteps per batch), \(K\) is the number of epochs over the data, and \(M \le NT\) is the minibatch size. The critic network's parameters \(\phi\) are updated alongside \(\theta\) whenever the value loss term is backpropagated.

\begin{algorithm}[H]
\caption{Proximal Policy Optimization (Actor-Critic)}
\label{alg:ppo}
\begin{algorithmic}[1]
\For{iteration = 1, 2, \dots}
    \For{actor = 1, 2, \dots, N}
        \State Run policy \(\pi_{\theta_{\text{old}}}\) (actor network) in the environment for \(T\) timesteps.
        \State Compute advantage estimates \(\hat{A}_1, \dots, \hat{A}_T\) using the critic network's value estimates.
    \EndFor
    \State Optimize the composite loss \(L(\theta)\) with respect to \(\theta\) (for the actor) and update \(\phi\) (for the critic) using \(K\) epochs and minibatch size \(M \le NT\).
    \State Update \(\theta_{\text{old}} \leftarrow \theta\).
\EndFor
\end{algorithmic}
\end{algorithm}

Overall, \gls{ppo} strikes a favorable balance between simplicity and performance, making it one of the most widely adopted on-policy algorithms in modern reinforcement learning applications. The following sections will discuss the multi-agent extensions of \gls{ppo} that are used in this work.
\section{Multi-Agent Reinforcement Learning}
This section provides an overview of the theoretical background of \gls{marl} as applied in this work. Our work evaluates both centralized and decentralized training paradigms. In particular, we employ Proximal Policy Optimization (PPO) for centralized training, while also exploring decentralized approaches using Independent PPO (IPPO) and Multi-Agent PPO (MAPPO), which are detailed in subsequent sections.
\subsection{Multi-Agent Markov Decision Process}
A cooperative multi-agent reinforcement learning problem can be formalized as a \gls{dec-pomdp}\cite{oliehoek_concise_2016}, defined by the tuple
\[
  \bigl(\mathcal{N},\,\mathcal{S},\,\{\mathcal{A}^i\}_{i=1}^N,\,P,\,r,\,\{\Omega^i\}_{i=1}^N,\,O,\,\gamma\bigr)
\]
where $\mathcal{N}=\{1,\dots,N\}$ is the set of agents; $\mathcal{S}$ is the set of global states (with $s_0\sim\rho(s)$ denoting the initial state distribution); $\mathcal{A}^i$ is the action space of agent $i$, and the joint action space is $\mathcal{A} = \prod_{i=1}^N \mathcal{A}^i$; $P(s' \mid s, a)$ is the transition kernel, where $a=(a^1,\dots,a^N)\in\mathcal{A}$; $r(s,a)\in\mathbb{R}$ is the common team reward received by all agents; $\Omega^i$ is the observation space of agent $i$, and $O(o^1,\dots,o^N\mid s)$ is the joint observation function; and $\gamma\in[0,1)$ is the discount factor.
At each time step $t$, each agent $i$ receives a private observation $o^i_t \in \Omega^i$ sampled from $O(\cdot\mid s_t)$ and selects an action 
$a^i_t \sim \pi^i\bigl(a^i \mid \tau^i_t\bigr)$
conditioned on its action-observation history $\tau^i_t$. The joint policy 
$\pi(a\mid \tau) \;=\; \prod_{i=1}^N \pi^i\bigl(a^i\mid \tau^i\bigr)$
induces an expected discounted return
\[
  J(\pi) \;=\; \mathbb{E}\Bigl[\sum_{t=0}^\infty \gamma^t\,r(s_t, a_t)\Bigr]\,. 
\]

\subsection{Independent PPO (IPPO)}
There are two primary approaches for extending \gls{ppo} to multi-agent settings: \gls{ippo}, which employs decentralized critics, and \gls{mappo}, which uses a centralized critic.
In \gls{ippo} \cite{witt_is_2020}, each agent \(i\) treats other agents as part of its environment and optimizes its own clipped surrogate objective:
\[
L_i(\theta_i) = \mathbb{E}_t\Bigl[L^{\mathrm{CLIP}}_i(\theta_i) 
  - c_1\,\bigl(V_{\phi_i}(o^i_t)-V^{\text{target},i}_t\bigr)^2 
  + c_2\,S[\pi_{\theta_i}](o^i_t)\Bigr],
\]
where
\[
L^{\mathrm{CLIP}}_i(\theta_i) = \mathbb{E}_t\!\Bigl[\min\bigl(r^i_t(\theta_i)\,\hat{A}^i_t,\;\text{clip}(r^i_t(\theta_i),1-\epsilon,1+\epsilon)\,\hat{A}^i_t\bigr)\Bigr].
\]
Here, \(r^i_t(\theta_i)=\pi_{\theta_i}(a^i_t\mid o^i_t)/\pi_{\theta_i}^{\mathrm{old}}(a^i_t\mid o^i_t)\), and \(\hat{A}^i_t\) is estimated with \gls{gae} using agent \(i\)'s local critic \(V_{\phi_i}\). Each agent collects its own on-policy rollouts \(\{(o^i_t,a^i_t,r_t)\}\) and performs \(K\) epochs of minibatch \gls{sgd} exactly as in single agent \gls{ppo}.  While this simplicity enables straightforward scaling to many agents, it does not explicitly address the non stationarity introduced by concurrent learning: all stability and performance gains must emerge implicitly from \gls{ppo} conservative updates and entropy regularization.

\subsection{Multi-Agent PPO (MAPPO)}
\gls{mappo} \cite{yu_surprising_2022} adopts a \gls{ctde} paradigm. A shared policy \(\pi_\theta(a^i\mid o^i)\) is used by all agents at execution, while training uses a centralized critic \(V_\phi(s)\) that has access to the full state or all agents' observations. The joint objective is
\[
L(\theta,\phi) = \mathbb{E}_t\Bigl[L^{\mathrm{CLIP}}(\theta)
  - c_1\,\bigl(V_\phi(s_t)-V^{\text{target}}_t\bigr)^2
  + c_2 \sum_{i=1}^N S[\pi_\theta(\cdot\mid o^i_t)]\Bigr],
\]
where 
\[
L^{\mathrm{CLIP}}(\theta) = \mathbb{E}_t\!\Bigl[\min\bigl(r_t(\theta)\,\hat{A}_t,\;\text{clip}(r_t(\theta),1-\epsilon,1+\epsilon)\,\hat{A}_t\bigr)\Bigr],
\]
and \(r_t(\theta)=\prod_i \frac{\pi_\theta(a^i_t\mid o^i_t)}{\pi_{\theta_{\text{old}}}(a^i_t\mid o^i_t)}\). By conditioning the critic on the true global state, \gls{mappo} obtains lower-variance advantage estimates and improved credit assignment, while each agent remains decentralized at execution, relying only on its local observation \(o^i\). Shared policy parameters promote coordinated behaviors and efficient scaling.

\section{Quadrotor Dynamics}
\label{sec:quadrotor_control}
Quadrotors are rotorcraft \glspl{uav} with four fixed-pitch propellers generating upwards thrust. Their mechanical simplicity and agility make them popular for aerial surveillance, package delivery, and search-and-rescue. This section introduces coordinate frames, state representation, onboard sensors, and the continuous-time dynamics and common control action parameterizations for a quadrotor platform (illustrated in Figure~\ref{fig:quadrotor_frames}).

\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\textwidth]{quadrotor_dynamics.png}
  \caption{TODO}
  \label{fig:quadrotor_frames}
\end{figure}

\subsection{Coordinate Frames and State Representation}
\label{sec:quadrotor_state}
We define the inertial frame \(\mathcal{I}\) and the body-fixed frame \(\mathcal{B}\) as shown in Figure~\ref{fig:quadrotor_frames}. The origin of \(\mathcal{B}\) is at the quadrotor's center of mass, with its \(x\)-axis pointing forward, \(y\)-axis to the right, and \(z\)-axis upward. The quadrotor's position in \(\mathcal{I}\) is \(\mathbf{p} \in \mathbb{R}^3\), and its linear velocity is \(\mathbf{v} = \dot{\mathbf{p}}\). Its attitude is given by \(\mathbf{R} \in SO(3)\), mapping vectors from \(\mathcal{B}\) to \(\mathcal{I}\), and the angular velocity in \(\mathcal{B}\) is \(\boldsymbol{\omega} = [\omega_x,\omega_y,\omega_z]^\mathsf{T} \in \mathbb{R}^3\). We collect these into the state
\[
x = \bigl(\mathbf{p},\,\mathbf{v},\,\mathbf{R},\,\boldsymbol{\omega}\bigr).
\]

\subsection{Sensors and State Estimation}
\label{sec:quadrotor_estimation}
The quadrotor’s state estimator fuses measurements from a 9-DOF IMU and, when available, an external motion capture system. The IMU provides accelerometer and gyroscope readings:
\[
\mathbf{a}_m = \mathbf{R}^\mathsf{T}\bigl(\ddot{\mathbf{p}} - \mathbf{g}\bigr) + \boldsymbol{\nu}_a,\qquad
\boldsymbol{\omega}_m = \boldsymbol{\omega} + \boldsymbol{\nu}_\omega,
\]
where \(\mathbf{g}=[0,0,9.81]^\mathsf{T}\) and \(\boldsymbol{\nu}_a,\boldsymbol{\nu}_\omega\) are noise terms. In motion-capture environments, position and (optionally) attitude measurements are
\[
\mathbf{p}_{\mathrm{mocap}} = \mathbf{p} + \boldsymbol{\nu}_p,\qquad
\mathbf{R}_{\mathrm{mocap}} = \mathbf{R}\bigl(\mathbf{I} + \widehat{\boldsymbol{\nu}_R}\bigr),
\]
with \(\boldsymbol{\nu}_p,\boldsymbol{\nu}_R\) small errors and \(\widehat{\cdot}\) mapping \(\mathbb{R}^3\) to \(SO(3)\). Stacking these gives the measurement vector
\[
\mathbf{y} = \begin{bmatrix}
\mathbf{a}_m^\mathsf{T} & \boldsymbol{\omega}_m^\mathsf{T} & \mathbf{p}_{\mathrm{mocap}}^\mathsf{T} & \mathrm{vec}(\mathbf{R}_{\mathrm{mocap}})^\mathsf{T}
\end{bmatrix}^\mathsf{T} = h(x) + \boldsymbol{\nu}.
\]
An Extended Kalman Filter (EKF) fuses these measurements to produce \(\hat{x} = (\hat{\mathbf{p}},\,\hat{\mathbf{v}},\,\hat{\mathbf{R}},\,\hat{\boldsymbol{\omega}})\) for use by the low-level controllers.
\subsection{Quadrotor Dynamics}
\label{sec:quadrotor_dynamics}
We augment the state \(x=(\mathbf{p},\mathbf{v},\mathbf{R},\boldsymbol{\omega})\) with propeller speeds \(\boldsymbol{\Omega}=[\Omega_1,\Omega_2,\Omega_3,\Omega_4]^\mathsf{T}\), forming
\[
x_{\mathrm{full}} = (\mathbf{p},\,\mathbf{v},\,\mathbf{R},\,\boldsymbol{\omega},\,\boldsymbol{\Omega}).
\]
Let \(m\) be the mass of the quadrotor, and let \(\mathbf{I} = \mathrm{diag}(J_{x},\,J_{y},\,J_{z})\) be its moment-of-inertia matrix expressed in \(\mathcal{B}\). Gravity in \(\mathcal{I}\) is \(g\,\mathbf{e}_{3}\) with \(\mathbf{e}_{3} = [0,\,0,\,1]^{\mathsf{T}}\). Each propeller \(i\) (numbered 1–4 as in Figure~\ref{fig:quadrotor_frames}) produces an upward thrust \(f_{i} \in \mathbb{R}\) along the body-fixed \(z\)-axis and a reactive torque \(\tau_{i}\in \mathbb{R}\) about that same axis. We assume the thrust and drag torque of each propeller scale quadratically with the motor speed:
\[
f_{i}(\Omega_{i}) \;=\; c_{\ell}\,\Omega_{i}^{2}, 
\qquad 
\tau_{i}(\Omega_{i}) \;=\; c_{d}\,\Omega_{i}^{2},
\]
where \(c_{\ell}\) and \(c_{d}\) are the experimentally identified thrust and drag coefficients.

Let \(\mathbf{r}_{P,i}\in \mathbb{R}^{3}\) be the position vector from the body-frame origin to propeller \(i\). We define the total propulsive force in the body frame as
\[
\mathbf{f}_{\mathrm{prop}} = \sum_{i=1}^4 f_i\,\mathbf{e}_3^B,
\]
and the total torque is
\[
\boldsymbol{\tau}_{\mathrm{prop}} = \sum_{i=1}^4 \Bigl(\tau_i\,\mathbf{e}_3^B + \mathbf{r}_{P,i}\times(f_i\,\mathbf{e}_3^B)\Bigr).
\]
Ignoring aerodynamic drag, the continuous-time dynamics are
\[
\dot{\mathbf{p}} = \mathbf{v}, 
\qquad
\dot{\mathbf{R}} = \mathbf{R}\,\widehat{\boldsymbol{\omega}},
\]
\[
m\,\dot{\mathbf{v}} = \mathbf{R}\,\mathbf{f}_{\mathrm{prop}} - m\,g\,\mathbf{e}_3,
\]
\[
\mathbf{I}\,\dot{\boldsymbol{\omega}} = \boldsymbol{\tau}_{\mathrm{prop}} - \boldsymbol{\omega} \times (\mathbf{I}\,\boldsymbol{\omega}),
\]
\[
\dot{\boldsymbol{\Omega}} = \tfrac{1}{k_{\mathrm{mot}}}(\boldsymbol{\Omega}_{\mathrm{cmd}} - \boldsymbol{\Omega}),
\]
where \(\widehat{\boldsymbol{\omega}}\) is the skew-symmetric matrix of \(\boldsymbol{\omega}\), \(k_{\mathrm{mot}}\) is the motor time constant, and \(\boldsymbol{\Omega}_{\mathrm{cmd}}\) is the commanded motor-speed vector \cite{kaufmann_benchmark_2022}.

\subsection{Control Action Parameterizations}
\label{sec:quadrotor_actions}
Following \cite{kaufmann_benchmark_2022}, we describe three common action-space definitions for quadrotor control:

\paragraph{Linear Velocity \& Yaw Rate (LV).}  
An LV policy outputs a desired body-frame velocity \(\mathbf{v}_{\mathrm{des}}=[v_x,v_y,v_z]^\mathsf{T}\) and a yaw rate \(\omega_z\):
\[
u_{\mathrm{LV}} = \{v_x,\,v_y,\,v_z,\,\omega_z\}.
\]
A cascaded low-level controller converts these into collective thrust \(c=\|\mathbf{f}_{\mathrm{prop}}\|\) and attitude setpoints, then allocates individual propeller thrusts \(f_i\) to achieve the commanded \(\omega_z\). LV reduces sample complexity and improves sim-to-real transfer but cannot leverage full force–torque dynamics for aggressive maneuvers.

\paragraph{Collective Thrust \& Bodyrates (\gls{ctbr}).}  
A \gls{ctbr} policy outputs total thrust \(c\in\mathbb{R}\) and body-rate setpoints \(\boldsymbol{\omega}_{\mathrm{des}}=[\omega_x,\omega_y,\omega_z]^\mathsf{T}\):
\[
u_{\mathrm{CTBR}} = \{c,\,\omega_x,\,\omega_y,\,\omega_z\}.
\]
An inner-loop rate controller computes moments \(\mathbf{M}=[M_x,M_y,M_z]^\mathsf{T}\) to track \(\boldsymbol{\omega}_{\mathrm{des}}\), and the mixer solves
\[
\sum_{i=1}^4 f_i = c, 
\quad
\ell\,(f_2 - f_4) = M_x, 
\quad
\ell\,(f_3 - f_1) = M_y, 
\quad
\tau_1 - \tau_2 + \tau_3 - \tau_4 = M_z
\]
for individual thrusts \(f_i\). \gls{ctbr} achieves higher agility than LV while remaining more robust than end-to-end approaches.

\paragraph{Single-Rotor Thrust (SRT).}  
An SRT policy outputs the four individual propeller thrusts:
\[
u_{\mathrm{SRT}} = \{f_1,\,f_2,\,f_3,\,f_4\},
\]
where \(f_i = c_\ell \Omega_i^2\). This end-to-end formulation grants full authority over \(\mathbf{f}_{\mathrm{prop}}\) and \(\boldsymbol{\tau}_{\mathrm{prop}}\), enabling the most aggressive maneuvers, but it is sample-inefficient and sensitive to modeling errors.

In summary, LV actions abstract away low-level thrust allocation at the cost of dynamic expressiveness, \gls{ctbr} retains medium-level control over thrust and attitude rates and SRT provides full end-to-end control at the cost of higher learning complexity and sensitivity to modeling errors.
\section{Quadrotors with Cable-Suspended Payloads}
\label{sec:quadrotor_with_payloads}
\begin{figure}

  \centering
  \includegraphics[width=0.6\textwidth]{cable_models.png}
  \caption{TODO Different approaches to modeling a cable-suspended payload. (a) A simple pendulum model with a rigid link and point-mass payload. (b) A multi-link pendulum model with \(N\) serial links. (c) A tendon approximation in MuJoCo}
  \label{fig:cable_models}
\end{figure}
When a quadrotor carries a cable-suspended payload, its state must include cable and payload variables. Figure~\ref{fig:cable_models} illustrates three common modeling approaches. A simple pendulum model treats the cable as a massless, inextensible rigid link of length \(\ell\) with a point-mass payload \(m_L\). The payload position \(\mathbf{p}_L\) relates to the quadrotor position \(\mathbf{p}_Q\) via
\[
\mathbf{p}_L = \mathbf{p}_Q - \ell\,\mathbf{q}, 
\quad
\|\mathbf{q}\| = 1,
\]
where \(\mathbf{q}\in S^2\) is the unit vector along the cable \cite{estevez_review_2024}. The augmented state is
\[
x_{\mathrm{aug}} = (\mathbf{p}_Q,\,\mathbf{v}_Q,\,\mathbf{R},\,\boldsymbol{\omega},\,\mathbf{q},\,\dot{\mathbf{q}}),
\]
with \(\mathbf{v}_Q = \dot{\mathbf{p}}_Q\) and \(\dot{\mathbf{q}}\) capturing payload swing \cite{Wahba2024}.

A multi-link pendulum model represents the cable as \(N\) serial, massless links of lengths \(\{\ell_i\}\), each with orientation \(\mathbf{q}_i\in S^2\). The payload position becomes
\[
\mathbf{p}_L = \mathbf{p}_Q - \sum_{i=1}^N \ell_i\,\mathbf{q}_i,
\]
and the state is augmented by \(\{\mathbf{q}_i,\,\dot{\mathbf{q}}_i\}_{i=1}^N\). This captures cable sag and higher-order swing modes but increases the state dimension from 8 (rigid link) to \(6+4N\), where 6 corresponds to \(\mathbf{p}_Q,\,\mathbf{v}_Q,\,\mathbf{R},\,\boldsymbol{\omega}\) \cite{goodarzi_dynamics_2015}.

In MuJoCo, cables are approximated as tendon elements—massless, inelastic, high-stiffness springs enforcing \(\|\mathbf{p}_Q - \mathbf{p}_L\| = \ell\) when taut and acting as slack otherwise. The augmented state is
\[
x_{\mathrm{aug}} = (\mathbf{p}_Q,\,\mathbf{v}_Q,\,\mathbf{R},\,\boldsymbol{\omega},\,\mathbf{p}_L,\,\mathbf{v}_L),
\]
where \(\mathbf{v}_L = \dot{\mathbf{p}}_L\). Tendon tension \(\mathbf{T}\) applies \(-\mathbf{T}\) on the quadrotor and \(+\mathbf{T}\) on the payload.

For cooperative transport with \(M\) quadrotors, each quadrotor \(i\) connects to a common payload of mass \(m_P\) via its own tendon. The payload dynamics are
\[
m_P \,\ddot{\mathbf{p}}_P = -m_P\,\mathbf{g} + \sum_{i=1}^M \mathbf{T}_i,
\]
and each quadrotor's translational dynamics include \(-\mathbf{T}_i\) alongside its thrust. The joint state is
\[
x_{\mathrm{aug}} = \bigl(\{\mathbf{p}_i,\,\mathbf{v}_i,\,\mathbf{R}_i,\,\boldsymbol{\omega}_i\}_{i=1}^M,\,\mathbf{p}_P,\,\mathbf{v}_P\bigr),
\]
capturing multi-agent interactions, payload swing, tension variations, and hybrid dynamics while remaining computationally efficient for simulation and control experiments.