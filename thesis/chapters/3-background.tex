\chapter{Background}

\section{Multi-Agent Reinforcement Learning}

This section provides an overview of the theoretical background of Multi-Agent Reinforcement Learning (MARL) as applied in this work. Our work evaluates both centralized and decentralized training paradigms. In particular, we employ Proximal Policy Optimization (PPO) for centralized training, while also exploring decentralized approaches using Independent PPO (IPPO) and Multi-Agent PPO (MAPPO), which are detailed in subsequent sections.

\subsection{Fundamentals of Reinforcement Learning}

\gls{rl} is a framework in which an agent learns to make decisions by interacting with its environment. This interaction is commonly modeled as an \gls{mdp}, which is defined by the tuple
\[
(\mathcal{S}, \mathcal{A}, P, r, \gamma),
\]
where \(\mathcal{S}\) is the set of states, \(\mathcal{A}\) is the set of actions, \(P(s'\mid s,a)\) denotes the probability of transitioning from state \(s\) to state \(s'\) given action \(a\), \(r(s,a)\) represents the reward received when taking action \(a\) in state \(s\), and \(\gamma \in [0,1)\) is the discount factor, which reduces the weight of future rewards.

At every time step \(t\), the agent observes the current state \(s_t\), selects an action \(a_t\) according to a policy \(\pi(a_t\mid s_t)\), receives an immediate reward \(r(s_t,a_t)\), and transitions to the next state \(s_{t+1}\) based on the dynamics \(P\). The goal is to find the optimal policy \(\pi^*\) that maximizes the expected total discounted reward, i.e.,
\[
\pi^* = \arg\max_{\pi} J(\pi), \qquad\text{where}\qquad J(\pi) = \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t r(s_t, a_t)\right].
\]

This formulation can be explicitly stated as the following constrained optimization problem:
\begin{equation}\label{eq:RL_opt}
\begin{aligned}
\text{Find} \quad & \pi^* = \arg\max_{\pi} \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t r(s_t, a_t)\right] \\
\text{subject to} \quad & s_0 \sim \rho_0, \\
& a_t \sim \pi(\cdot \mid s_t), \quad t = 0,1,2,\ldots, \\
& s_{t+1} \sim P(\cdot \mid s_t, a_t), \quad t = 0,1,2,\ldots, \\
& \pi(a \mid s) \geq 0,\quad \forall\, s \in \mathcal{S},\, a \in \mathcal{A}, \\
& \sum_{a \in \mathcal{A}} \pi(a \mid s) = 1,\quad \forall\, s \in \mathcal{S}.
\end{aligned}
\end{equation}

Many \gls{rl} algorithms operate through the following iterative procedure:

\begin{enumerate}
    \item \textbf{Data Collection:} Execute the current policy \(\pi\) to collect trajectories of states, actions, and rewards.
    \item \textbf{Return Estimation:} For each time step, compute the cumulative future reward using:
    \begin{equation}
    R_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k}.
    \end{equation}
    \item \textbf{Value Function Computation:} Estimate the following:
    \begin{itemize}
        \item \emph{State-value function:}
        \begin{equation}
        V^\pi(s) = \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t r_t \,\bigg|\, s_0 = s\right].
        \end{equation}
        \item \emph{Action-value function (Q-function):}
        \begin{equation}
        Q^\pi(s, a) = \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t r_t \,\bigg|\, s_0 = s,\, a_0 = a\right].
        \end{equation}
    \end{itemize}
    \item \textbf{Advantage Estimation:} Determine the relative benefit of taking an action \(a_t\) in state \(s_t\) by computing:
    \begin{equation}
    \hat{A}_t = Q^\pi(s_t, a_t) - V^\pi(s_t).
    \end{equation}
    \item \textbf{Policy Update:} Adjust the policy parameters \(\theta\) by ascending the gradient:
    \begin{equation}
    \hat{g} = \mathbb{E}_t\left[\nabla_\theta \log \pi_\theta(a_t \mid s_t) \, \hat{A}_t\right].
    \end{equation}
    \item \textbf{Iteration:} With the updated policy, collect new data and repeat the process until the performance meets the desired criteria.
\end{enumerate}

The above steps outline the general framework of \gls{rl} algorithms. However, the specific implementation details can vary significantly between different algorithms. For instance, some methods may utilize function approximation to represent the policy and value functions, while others may rely on tabular representations. Additionally, the choice of exploration strategy, reward shaping, and experience replay can also influence the performance of the algorithm.

For a thorough treatment of these fundamentals, see \cite{SuttonBarto2018}.



\subsection{PPO}
Proximal Policy Optimization (PPO) was introduced by Schulman et al. in \cite{schulman2017proximal} as a method to achieve stable and reliable policy updates. PPO addresses the shortcomings of standard policy gradient methods by incorporating multiple epochs of minibatch updates on a surrogate objective.

Central to PPO is the probability ratio
\[
r_t(\theta) = \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)},
\]
which compares the new policy \(\pi_\theta\) with the old policy \(\pi_{\theta_{\text{old}}}\). To prevent large, destabilizing policy updates, PPO employs a clipped surrogate objective:
\begin{equation}
    L^{\text{CLIP}}(\theta) = \mathbb{E}_t\left[ \min\left(r_t(\theta)\hat{A}_t,\, \text{clip}\left(r_t(\theta), 1-\epsilon, 1+\epsilon\right)\hat{A}_t\right) \right],
\end{equation}
where \(\hat{A}_t\) is an advantage estimate and \(\epsilon\) is a hyperparameter that determines the clipping range.

PPO alternates between data collection using the current policy and performing several epochs of stochastic gradient ascent on the surrogate loss. This approach combines improved sample efficiency with reliable learning dynamics.
\subsection{IPPO}
\subsection{MAPPO}

\section{Quadrotor Control}
\subsection{Quadrotor Dynamics}
\subsection{Quadrotor Simulation}
\subsection{Quadrotor Classic Control}
\subsection{Quadrotor With Payloads}

\section{Problem Formulation}


