@Article{Lillicrap2015ContinuousCW,
  author       = {Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Manfred Otto Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
  date         = {2015},
  journaltitle = {CoRR},
  title        = {Continuous control with deep reinforcement learning},
  doi          = {10.48550/arxiv.1509.02971},
  eprint       = {1509.02971},
  volume       = {abs/1509.02971},
}

@InProceedings{Papoudakis2020BenchmarkingMD,
  author    = {Georgios Papoudakis and Filippos Christianos and Lukas Sch{\"a}fer and Stefano V. Albrecht},
  booktitle = {NeurIPS Datasets and Benchmarks},
  date      = {2020},
  title     = {Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Tasks},
}

@Article{villa_cooperative_2021,
  author       = {Villa, Daniel Khede Dourado and Brandão, Alexandre Santos and Carelli, Ricardo and Sarcinelli-Filho, Mário},
  date         = {2021},
  journaltitle = {{IEEE} Access},
  title        = {Cooperative Load Transportation With Two Quadrotors Using Adaptive Control},
  doi          = {10.1109/ACCESS.2021.3113466},
  issn         = {2169-3536},
  pages        = {129148--129160},
  volume       = {9},
  abstract     = {The problem of carrying a bar-shaped payload suspended by flexible cables attached to two quadrotors is analyzed in this work. The aerial vehicles and the load are dealt with as a single system, whose kinematics is described as a multi-robot formation using the virtual structure approach. The dynamic effects caused by the tethered load over the quadrotors, as well as those caused by each quadrotor over the other, are treated by an adaptive dynamic compensator. To validate the proposal, experiments were run testing the system in adverse conditions: transportation far from quasi-static motion, high payload-to-quadrotor weight ratio, 20\% of error in the robot model parameters, transportation under wind disturbances, and payload weight changes during flight. The good performance of the proposed control system in all these tests allows concluding that the proposed system is able to accomplish payload positioning, orientation, and trajectory tracking under adverse conditions, with accelerations up to 1.6 m/s2.},
  file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/TZSTFIV7/Villa et al. - 2021 - Cooperative Load Transportation With Two Quadrotors Using Adaptive Control.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/S2TGNJMA/9540662.html:text/html},
}

@Article{gabellieri_equilibria_2023,
  author       = {Gabellieri, Chiara and Tognon, Marco and Sanalitro, Dario and Franchi, Antonio},
  date         = {2023-10},
  journaltitle = {{IEEE} Transactions on Robotics},
  title        = {Equilibria, Stability, and Sensitivity for the Aerial Suspended Beam Robotic System Subject to Parameter Uncertainty},
  doi          = {10.1109/TRO.2023.3279033},
  eprint       = {2302.07031},
  issn         = {1941-0468},
  pages        = {3977--3993},
  volume       = {39},
  abstract     = {This article studies how parametric uncertainties affect the cooperative manipulation of a cable-suspended beam-shaped load by means of two aerial robots not explicitly communicating with each other. In particular, this article sheds light on the impact of the uncertain knowledge of the model parameters available to an established communicationless force-based controller. First, we find the closed-loop equilibrium configurations in the presence of the aforementioned uncertainties, and then, we study their stability. Hence, we show the fundamental role played in the robustness of the load attitude control by the internal force induced in the manipulated object by nonvertical cables. Furthermore, we formally study the sensitivity of the attitude error to such parametric variations, and we provide a method to act on the load position error in the presence of uncertainties. Eventually, we validate the results through an extensive set of numerical tests in a realistic simulation environment, including underactuated aerial vehicles and sagging-prone cables, and through hardware experiments.},
  file         = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/3D74UXU3/10149811.html:text/html;Submitted Version:/Users/viktorlorentz/Zotero/storage/3TAI7ZA5/Gabellieri et al. - 2023 - Equilibria, Stability, and Sensitivity for the Aerial Suspended Beam Robotic System Subject to Param.pdf:application/pdf},
}

@Article{Lin2024PayloadTW,
  author       = {Dasheng Lin and Jianda Han and Kun Li and Jianlei Zhang and Chun-yan Zhang},
  date         = {2024},
  journaltitle = {IEEE Transactions on Aerospace and Electronic Systems},
  title        = {Payload Transporting With Two Quadrotors by Centralized Reinforcement Learning Method},
  doi          = {10.1109/taes.2023.3321260},
  pages        = {239--251},
  volume       = {60},
}

@InProceedings{sun_nonlinear_2023,
  author     = {Sun, Sihao and Franchi, Antonio},
  booktitle  = {2023 International Conference on Unmanned Aircraft Systems ({ICUAS})},
  date       = {2023-06},
  title      = {Nonlinear {MPC} for Full-Pose Manipulation of a Cable-Suspended Load using Multiple {UAVs}},
  doi        = {10.1109/ICUAS57906.2023.10156031},
  eventtitle = {2023 International Conference on Unmanned Aircraft Systems ({ICUAS})},
  pages      = {969--975},
  abstract   = {In this work, we propose a centralized control method based on nonlinear model predictive control to let multiple {UAVs} manipulate the full pose of an object via cables. At the best of the authors knowledge this is the first method that takes into account the full nonlinear model of the load-{UAV} system, and ensures all the feasibility constraints concerning the {UAV} maximumum and minimum thrusts, the collision avoidance between the {UAVs}, cables and load, and the tautness and maximum tension of the cables. By taking into account the above factors, the proposed control algorithm can fully exploit the performance of {UAVs} and facilitate the speed of operation. Simulations are conducted to validate the algorithm to achieve fast and safe manipulation of the pose of a rigid-body payload using multiple {UAVs}. We demonstrate that the computational time of the proposed method is sufficiently small ({\textless}100 ms) for {UAV} teams composed by up to 10 units, which makes it suitable for a huge variety of future industrial applications, such as autonomous building construction and heavy-load transportation.},
  file       = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/UX7W8J7P/10156031.html:text/html;Submitted Version:/Users/viktorlorentz/Zotero/storage/DFYSACLS/Sun and Franchi - 2023 - Nonlinear MPC for Full-Pose Manipulation of a Cable-Suspended Load using Multiple UAVs.pdf:application/pdf},
}

@InProceedings{Abbaraju2018SensingASA,
  author    = {Abbaraju, Praveen and Voyles, Richard and others},
  booktitle = {Proceedings of the WM2018 Symposium Conference, Phoenix, AZ, USA},
  date      = {2018},
  title     = {Sensing and sampling of trace contaminations by a dexterous hexrotor UAV at nuclear facilities-18600},
  pages     = {18--22},
}

@InProceedings{gronauer_using_2022,
  author     = {Gronauer, Sven and Kissel, Matthias and Sacchetto, Luca and Korte, Mathias and Diepold, Klaus},
  booktitle  = {2022 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
  date       = {2022-10},
  title      = {Using Simulation Optimization to Improve Zero-shot Policy Transfer of Quadrotors},
  doi        = {10.1109/IROS47612.2022.9981229},
  eprint     = {2201.01369},
  eventtitle = {2022 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
  pages      = {10170--10176},
  abstract   = {In this work, we propose a data-driven approach to optimize the parameters of a simulation such that control policies can be directly transferred from simulation to a real-world quadrotor. Our neural network-based policies take only onboard sensor data as input and run entirely on the embed-ded hardware. In real-world experiments, we compare low-level Pulse-Width Modulated control with higher-level control structures such as Attitude Rate and Attitude, which utilize Proportional-Integral-Derivative controllers to output motor commands. Our experiments show that low-level controllers trained with Reinforcement Learning require a more accurate simulation than higher-level control policies at the expense of being less robust towards parameter uncertainties.},
  file       = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/XTCQ8T9D/9981229.html:text/html;Submitted Version:/Users/viktorlorentz/Zotero/storage/9NVQPD6L/Gronauer et al. - 2022 - Using Simulation Optimization to Improve Zero-shot Policy Transfer of Quadrotors.pdf:application/pdf},
}

@InProceedings{goodarzi_dynamics_2015,
  author     = {Goodarzi, Farhad A. and Lee, Taeyoung},
  booktitle  = {2015 American Control Conference ({ACC})},
  date       = {2015-07},
  title      = {Dynamics and control of quadrotor {UAVs} transporting a rigid body connected via flexible cables},
  doi        = {10.1109/ACC.2015.7172066},
  eventtitle = {2015 American Control Conference ({ACC})},
  pages      = {4677--4682},
  abstract   = {This paper is focused on the dynamics and control of arbitrary number of quadrotor {UAVs} transporting a rigid body payload. The rigid body payload is connected to quadrotors via flexible cables where each flexible cable is modeled as a system of serially-connected links. It is shown that a coordinate-free form of equations of motion can be derived for arbitrary numbers of quadrotors and links according to Lagrangian mechanics on a manifold. A geometric nonlinear controller is presented to transport the rigid body to a fixed desired position while aligning all of the links along the vertical direction. Numerical results are provided to illustrate the desirable features of the proposed control system.},
  file       = {Snapshot:/Users/viktorlorentz/Zotero/storage/SW9FQLBI/7172066.html:text/html;Submitted Version:/Users/viktorlorentz/Zotero/storage/QMHG5P44/Goodarzi and Lee - 2015 - Dynamics and control of quadrotor UAVs transporting a rigid body connected via flexible cables.pdf:application/pdf},
}

@Article{Ji2021ReinforcementLF,
  author       = {Yandong Ji and Bike Zhang and Koushil Sreenath},
  date         = {2021},
  journaltitle = {2021 IEEE 17th International Conference on Automation Science and Engineering (CASE)},
  title        = {Reinforcement Learning for Collaborative Quadrupedal Manipulation of a Payload over Challenging Terrain},
  doi          = {10.1109/case49439.2021.9551481},
  pages        = {899--904},
}

@InProceedings{li_nonlinear_2023,
  author     = {Li, Guanrui and Loianno, Giuseppe},
  booktitle  = {2023 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
  date       = {2023-10},
  title      = {Nonlinear Model Predictive Control for Cooperative Transportation and Manipulation of Cable Suspended Payloads with Multiple Quadrotors},
  doi        = {10.1109/IROS55552.2023.10341785},
  eprint     = {2303.06165},
  eventtitle = {2023 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
  pages      = {5034--5041},
  abstract   = {Autonomous Micro Aerial Vehicles ({MAVs}) such as quadrotors equipped with manipulation mechanisms have the potential to assist humans in tasks such as construction and package delivery. Cables are a promising option for manipulation mechanisms due to their low weight, low cost, and simple design. However, designing control and planning strategies for cable mechanisms presents challenges due to indirect load actuation, nonlinear configuration space, and highly coupled system dynamics. In this paper, we propose a novel Nonlinear Model Predictive Control ({NMPC}) method that enables a team of quadrotors to manipulate a rigid-body payload in all 6 degrees of freedom via suspended cables. Our approach can concurrently exploit, as part of the receding horizon optimization, the available mechanical system redundancies to perform additional tasks such as inter-robot separation and obstacle avoidance while respecting payload dynamics and actuator constraints. To address real-time computational requirements and scalability, we employ a lightweight state vector parametrization that includes only payload states in all six degrees of freedom. This also enables the planning of trajectories on the {SE} (3) manifold load configuration space, thereby also reducing planning complexity. We validate the proposed approach through simulation and real-world experiments.},
  file       = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/PUNS9G3F/10341785.html:text/html;Submitted Version:/Users/viktorlorentz/Zotero/storage/CMRNZFHI/Li and Loianno - 2023 - Nonlinear Model Predictive Control for Cooperative Transportation and Manipulation of Cable Suspende.pdf:application/pdf},
}

@InProceedings{huang_collision_2024,
  author     = {Huang, Zhehui and Yang, Zhaojing and Krupani, Rahul and Şenbaşlar, Baskın and Batra, Sumeet and Sukhatme, Gaurav S.},
  booktitle  = {2024 {IEEE} International Conference on Robotics and Automation ({ICRA})},
  date       = {2024-05},
  title      = {Collision Avoidance and Navigation for a Quadrotor Swarm Using End-to-end Deep Reinforcement Learning},
  doi        = {10.1109/ICRA57147.2024.10611499},
  eventtitle = {2024 {IEEE} International Conference on Robotics and Automation ({ICRA})},
  pages      = {300--306},
  abstract   = {End-to-end deep reinforcement learning ({DRL}) for quadrotor control promises many benefits – easy deployment, task generalization and real-time execution capability. Prior end-to-end {DRL}-based methods have showcased the ability to deploy learned controllers onto single quadrotors or quadrotor teams maneuvering in simple, obstacle-free environments. However, the addition of obstacles increases the number of possible interactions exponentially, thereby increasing the difficulty of training {RL} policies. In this work, we propose an end-to-end {DRL} approach to control quadrotor swarms in environments with obstacles. We provide our agents a curriculum and a replay buffer of the clipped collision episodes to improve performance in obstacle-rich environments. We implement an attention mechanism to attend to the neighbor robots and obstacle interactions - the first successful demonstration of this mechanism on policies for swarm behavior deployed on severely compute-constrained hardware. Our work is the first work that demonstrates the possibility of learning neighbor-avoiding and obstacle-avoiding control policies trained with end-to-end {DRL} that transfers zero-shot to real quadrotors. Our approach scales to 32 robots with 80\% obstacle density in simulation and 8 robots with 20\% obstacle density in physical deployment. Website: https://sites.google.com/view/obst-avoid-swarm-rl},
  file       = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/QSVAIBQD/10611499.html:text/html},
}

@InProceedings{batra_decentralized_2022,
  author     = {Batra, Sumeet and Huang, Zhehui and Petrenko, Aleksei and Kumar, Tushar and Molchanov, Artem and Sukhatme, Gaurav S.},
  booktitle  = {Proceedings of the 5th Conference on Robot Learning},
  date       = {2022-01-11},
  title      = {Decentralized Control of Quadrotor Swarms with End-to-end Deep Reinforcement Learning},
  doi        = {10.48550/arxiv.2109.07735},
  eprint     = {2109.07735},
  eventtitle = {Conference on Robot Learning},
  pages      = {576--586},
  abstract   = {We demonstrate the possibility of learning drone swarm controllers that are zero-shot transferable to real quadrotors via large-scale multi-agent end-to-end reinforcement learning. We train policies parameterized by neural networks that are capable of controlling individual drones in a swarm in a fully decentralized manner. Our policies, trained in simulated environments with realistic quadrotor physics, demonstrate advanced flocking behaviors, perform aggressive maneuvers in tight formations while avoiding collisions with each other, break and re-establish formations to avoid collisions with moving obstacles, and efficiently coordinate in pursuit-evasion tasks. We analyze, in simulation, how different model architectures and parameters of the training regime influence the final performance of neural swarms. We demonstrate the successful deployment of the model learned in simulation to highly resource-constrained physical quadrotors performing station keeping and goal swapping behaviors. Video demonstrations and source code are available at the project website https://sites.google.com/view/swarm-rl.},
  file       = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/D9K6Q2FE/Batra et al. - 2022 - Decentralized Control of Quadrotor Swarms with End-to-end Deep Reinforcement Learning.pdf:application/pdf},
  langid     = {english},
}

@Article{zhao_deep_2024,
  author       = {Zhao, Zipeng and Wan, Yu and Chen, Yong},
  date         = {2024-09},
  journaltitle = {Drones},
  title        = {Deep Reinforcement Learning-Driven Collaborative Rounding-Up for Multiple Unmanned Aerial Vehicles in Obstacle Environments},
  doi          = {10.3390/drones8090464},
  issn         = {2504-446X},
  pages        = {464},
  volume       = {8},
  abstract     = {With the rapid advancement of {UAV} technology, the utilization of multi-{UAV} cooperative operations has become increasingly prevalent in various domains, including military and civilian applications. However, achieving efficient coordinated rounding-up of multiple {UAVs} remains a challenging problem. This paper addresses the issue of collaborative drone hunting by proposing a decision-making control model based on deep reinforcement learning. Additionally, a shared experience data pool is established to facilitate communication between drones. Each drone possesses independent decision-making and control capabilities while also considering the presence of other drones in the environment to collaboratively accomplish obstacle avoidance and rounding-up tasks. Furthermore, we redefine and design the reward function of reinforcement learning to achieve precise control of drone swarms in diverse environments. Simulation experiments demonstrate the feasibility of the proposed method, showcasing its successful completion of obstacle avoidance, tracking, and rounding-up tasks in an obstacle environment.},
  file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/YR38DQEG/Zhao et al. - 2024 - Deep Reinforcement Learning-Driven Collaborative Rounding-Up for Multiple Unmanned Aerial Vehicles i.pdf:application/pdf},
  langid       = {english},
  rights       = {http://creativecommons.org/licenses/by/3.0/},
}

@InCollection{littman1994markov,
  author    = {Littman, Michael L},
  booktitle = {Machine learning proceedings 1994},
  date      = {1994},
  title     = {Markov games as a framework for multi-agent reinforcement learning},
  pages     = {157--163},
}

@Article{kaufmann_champion-level_2023,
  author       = {Kaufmann, Elia and Bauersfeld, Leonard and Loquercio, Antonio and Müller, Matthias and Koltun, Vladlen and Scaramuzza, Davide},
  date         = {2023-08},
  journaltitle = {Nature},
  title        = {Champion-level drone racing using deep reinforcement learning},
  doi          = {10.1038/s41586-023-06419-4},
  issn         = {1476-4687},
  pages        = {982--987},
  volume       = {620},
  abstract     = {First-person view ({FPV}) drone racing is a televised sport in which professional competitors pilot high-speed aircraft through a 3D circuit. Each pilot sees the environment from the perspective of their drone by means of video streamed from an onboard camera. Reaching the level of professional pilots with an autonomous drone is challenging because the robot needs to fly at its physical limits while estimating its speed and location in the circuit exclusively from onboard sensors1. Here we introduce Swift, an autonomous system that can race physical vehicles at the level of the human world champions. The system combines deep reinforcement learning ({RL}) in simulation with data collected in the physical world. Swift competed against three human champions, including the world champions of two international leagues, in real-world head-to-head races. Swift won several races against each of the human champions and demonstrated the fastest recorded race time. This work represents a milestone for mobile robotics and machine intelligence2, which may inspire the deployment of hybrid learning-based solutions in other physical systems.},
  file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/BKZSL3KB/Kaufmann et al. - 2023 - Champion-level drone racing using deep reinforcement learning.pdf:application/pdf},
  langid       = {english},
  rights       = {2023 The Author(s)},
}

@Article{Li2022RotorTMAF,
  author       = {Guanrui Li and Xinyang Liu and Giuseppe Loianno},
  date         = {2022},
  journaltitle = {IEEE Transactions on Robotics},
  title        = {RotorTM: A Flexible Simulator for Aerial Transportation and Manipulation},
  pages        = {831--850},
  volume       = {40},
}

@Software{jax2018github,
  author  = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  date    = {2018},
  title   = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url     = {http://github.com/jax-ml/jax},
  version = {0.3.13},
}

@Article{mnih2015human,
  author       = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  date         = {2015},
  journaltitle = {nature},
  title        = {Human-level control through deep reinforcement learning},
  pages        = {529--533},
  volume       = {518},
}

@Article{estevez_review_2024,
  author       = {Estevez, Julian and Garate, Gorka and Lopez-Guede, Jose Manuel and Larrea, Mikel},
  date         = {2024-02},
  journaltitle = {Drones},
  title        = {Review of Aerial Transportation of Suspended-Cable Payloads with Quadrotors},
  doi          = {10.3390/drones8020035},
  issn         = {2504-446X},
  pages        = {35},
  volume       = {8},
  abstract     = {Payload transportation and manipulation by rotorcraft drones are receiving a lot of attention from the military, industrial and logistics research areas. The interactions between the {UAV} and the payload, plus the means of object attachment or manipulation (such as cables or anthropomorphic robotic arms), may be nonlinear, introducing difficulties in the overall system performance. In this paper, we focus on the current state of the art of aerial transportation systems with suspended loads by a single {UAV} and a team of them and present a review of different dynamic cable models and control systems. We cover the last sixteen years of the existing literature, and we add a discussion for evaluating the main trends in the referenced research works.},
  file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/RZZDVQHM/Estevez et al. - 2024 - Review of Aerial Transportation of Suspended-Cable Payloads with Quadrotors.pdf:application/pdf},
  langid       = {english},
  rights       = {http://creativecommons.org/licenses/by/3.0/},
}

@InProceedings{molchanov_sim--multi-real_2019,
  author     = {Molchanov, Artem and Chen, Tao and Hönig, Wolfgang and Preiss, James A. and Ayanian, Nora and Sukhatme, Gaurav S.},
  booktitle  = {2019 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
  date       = {2019-11},
  title      = {Sim-to-(Multi)-Real: Transfer of Low-Level Robust Control Policies to Multiple Quadrotors},
  doi        = {10.1109/IROS40897.2019.8967695},
  eventtitle = {2019 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
  pages      = {59--66},
  abstract   = {Quadrotor stabilizing controllers often require careful, model-specific tuning for safe operation. We use reinforcement learning to train policies in simulation that transfer remarkably well to multiple different physical quadrotors. Our policies are low-level, i.e., we map the rotorcrafts' state directly to the motor outputs. The trained control policies are very robust to external disturbances and can withstand harsh initial conditions such as throws. We show how different training methodologies (change of the cost function, modeling of noise, use of domain randomization) might affect flight performance. To the best of our knowledge, this is the first work that demonstrates that a simple neural network can learn a robust stabilizing low-level quadrotor controller (without the use of a stabilizing {PD} controller) that is shown to generalize to multiple quadrotors. The video of our experiments can be found at https://sites.google.com/view/sim-to-multi-quad.},
  file       = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/5MMNKWRE/8967695.html:text/html;Submitted Version:/Users/viktorlorentz/Zotero/storage/GGY77L2R/Molchanov et al. - 2019 - Sim-to-(Multi)-Real Transfer of Low-Level Robust Control Policies to Multiple Quadrotors.pdf:application/pdf},
  shorttitle = {Sim-to-(Multi)-Real},
}

@Article{xing_multi-task_2024,
  author       = {Xing, Jiaxu and Geles, Ismail and Song, Yunlong and Aljalbout, Elie and Scaramuzza, Davide},
  date         = {2024},
  journaltitle = {{IEEE} Robotics and Automation Letters},
  title        = {Multi-Task Reinforcement Learning for Quadrotors},
  doi          = {10.1109/LRA.2024.3520894},
  issn         = {2377-3766},
  pages        = {1--8},
  abstract     = {Reinforcement learning ({RL}) has shown great effectiveness in quadrotor control, enabling specialized policies to develop even human-champion-level performance in single-task scenarios. However, these specialized policies often struggle with novel tasks, requiring a complete retraining of the policy from scratch. To address this limitation, this paper presents a novel multi-task reinforcement learning ({MTRL}) framework tailored for quadrotor control, leveraging the shared physical dynamics of the platform to enhance sample efficiency and task performance. By employing a multi-critic architecture and shared task encoders, our framework facilitates knowledge transfer across tasks, enabling a single policy to execute diverse maneuvers, including high-speed stabilization, velocity tracking, and autonomous racing. Our experimental results, validated both in simulation and real-world scenarios, demonstrate that our framework outperforms baseline approaches in terms of sample efficiency and overall task performance. Video: https://youtu.be/{HfK}9UT1OVnY.},
  file         = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/HWLYWRMH/10812062.html:text/html},
}

@InProceedings{kaufmann_benchmark_2022,
  author     = {Kaufmann, Elia and Bauersfeld, Leonard and Scaramuzza, Davide},
  booktitle  = {2022 International Conference on Robotics and Automation ({ICRA})},
  date       = {2022-05},
  title      = {A Benchmark Comparison of Learned Control Policies for Agile Quadrotor Flight},
  doi        = {10.1109/ICRA46639.2022.9811564},
  eventtitle = {2022 International Conference on Robotics and Automation ({ICRA})},
  pages      = {10504--10510},
  abstract   = {Quadrotors are highly nonlinear dynamical systems that require carefully tuned controllers to be pushed to their physical limits. Recently, learning-based control policies have been proposed for quadrotors, as they would potentially allow learning direct mappings from high-dimensional raw sensory observations to actions. Due to sample inefficiency, training such learned controllers on the real platform is impractical or even impossible. Training in simulation is attractive but requires to transfer policies between domains, which demands trained policies to be robust to such domain gap. In this work, we make two contributions: (i) we perform the first benchmark comparison of existing learned control policies for agile quadrotor flight and show that training a control policy that commands body-rates and thrust results in more robust sim-to-real transfer compared to a policy that directly specifies individual rotor thrusts, (ii) we demonstrate for the first time that such a control policy trained via deep reinforcement learning can control a quadrotor in real-world experiments at speeds over 45 km/h.},
  file       = {Accepted Version:/Users/viktorlorentz/Zotero/storage/KJH9PQJ7/Kaufmann et al. - 2022 - A Benchmark Comparison of Learned Control Policies for Agile Quadrotor Flight.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/BGUIK98J/9811564.html:text/html},
}

@Article{belkhale_model-based_2021,
  author       = {Belkhale, Suneel and Li, Rachel and Kahn, Gregory and {McAllister}, Rowan and Calandra, Roberto and Levine, Sergey},
  date         = {2021-04},
  journaltitle = {{IEEE} Robotics and Automation Letters},
  title        = {Model-Based Meta-Reinforcement Learning for Flight with Suspended Payloads},
  doi          = {10.1109/LRA.2021.3057046},
  eprint       = {2004.11345},
  issn         = {2377-3766, 2377-3774},
  pages        = {1471--1478},
  volume       = {6},
  abstract     = {Transporting suspended payloads is challenging for autonomous aerial vehicles because the payload can cause significant and unpredictable changes to the robot's dynamics. These changes can lead to suboptimal flight performance or even catastrophic failure. Although adaptive control and learning-based methods can in principle adapt to changes in these hybrid robot-payload systems, rapid mid-flight adaptation to payloads that have a priori unknown physical properties remains an open problem. We propose a meta-learning approach that "learns how to learn" models of altered dynamics within seconds of post-connection flight data. Our experiments demonstrate that our online adaptation approach outperforms non-adaptive methods on a series of challenging suspended payload transportation tasks. Videos and other supplemental material are available on our website: https://sites.google.com/view/meta-rl-for-flight},
  file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/EB6VM7KN/Belkhale et al. - 2021 - Model-Based Meta-Reinforcement Learning for Flight with Suspended Payloads.pdf:application/pdf;Snapshot:/Users/viktorlorentz/Zotero/storage/3F9IBRDX/2004.html:text/html},
  shortjournal = {{IEEE} Robot. Autom. Lett.},
}

@Article{hua_new_2022,
  author       = {Hua, Hean and Fang, Yongchun and Zhang, Xuetao and Qian, Chen},
  date         = {2022-04},
  journaltitle = {{IEEE}/{ASME} Transactions on Mechatronics},
  title        = {A New Nonlinear Control Strategy Embedded With Reinforcement Learning for a Multirotor Transporting a Suspended Payload},
  doi          = {10.1109/TMECH.2021.3082897},
  issn         = {1941-014X},
  pages        = {1174--1184},
  volume       = {27},
  abstract     = {In this article, a reinforcement learning ({RL})-based controller is proposed for a multirotor-based transportation system, guaranteeing that the trained {RL} controller is effective in both simulation and practical experiments. The main novelty lies in that, as far as we know, this is the first attempt of combining the advantages of nonlinear and intelligent control techniques to derive a practice-oriented {RL} controller for the multirotor-based transportation system, where the high-dimensional complicated dynamics are fully considered in the framework. Specifically, inspired by the physical insight of the system, a new nonlinear control approach is proposed, in which the underactuated properties and the nontrivial couplings are well handled. On this basis, an {RL} network is proposed to parameterize the nonlinear controller, where the obtained algorithm presents the features of strong reliability and fast convergence even in complicated working conditions (e.g., model uncertainties, parameters drift, external disturbances, and so on). Subsequently, the states are proven to asymptotically converge to the equilibrium point by Lyapunov analysis and {RL} techniques. A series of simulation and real world experiments are implemented to verify satisfactory positioning accuracy and robustness of the proposed algorithm.},
  file         = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/5RXNKV7S/9439850.html:text/html},
}

@Article{Koch2018ReinforcementLF,
  author       = {William Koch and Renato Mancuso and Richard West and Azer Bestavros},
  date         = {2018},
  journaltitle = {ACM Transactions on Cyber-Physical Systems},
  title        = {Reinforcement Learning for UAV Attitude Control},
  doi          = {10.48550/arxiv.1804.04154},
  eprint       = {1804.04154},
  pages        = {1--21},
  volume       = {3},
}

@Article{Chen2021FromUSA,
  author       = {Jie Chen and Jian Sun and Gang Wang},
  date         = {2021},
  journaltitle = {Engineering},
  title        = {From Unmanned Systems to Autonomous Intelligent Systems},
}

@Article{aerial_gym_simulator,
  author       = {Kulkarni, Mihir and Rehberg, Welf and Alexis, Kostas},
  date         = {2025},
  journaltitle = {IEEE Robotics and Automation Letters},
  title        = {Aerial Gym Simulator: A Framework for Highly Parallelized Simulation of Aerial Robots},
  doi          = {10.1109/LRA.2025.3548507},
  pages        = {4093--4100},
  volume       = {10},
}

@Article{Idrissi2022AROA,
  author       = {Moad Idrissi and M. Salami and F. Annaz},
  date         = {2022},
  journaltitle = {Journal of Intelligent \& Robotic Systems},
  title        = {A Review of Quadrotor Unmanned Aerial Vehicles: Applications, Architectural Design and Control Algorithms},
  pages        = {1--33},
  volume       = {104},
}

@Article{xu_omnidrones_2024,
  author       = {Xu, Botian and Gao, Feng and Yu, Chao and Zhang, Ruize and Wu, Yi and Wang, Yu},
  date         = {2024-03},
  journaltitle = {{IEEE} Robotics and Automation Letters},
  title        = {{OmniDrones}: An Efficient and Flexible Platform for Reinforcement Learning in Drone Control},
  doi          = {10.1109/LRA.2024.3356168},
  issn         = {2377-3766},
  pages        = {2838--2844},
  volume       = {9},
  abstract     = {In this letter, we introduce {OmniDrones}, an efficient and flexible platform tailored for reinforcement learning in drone control, built on Nvidia's Omniverse Isaac Sim. It employs a bottom-up design approach that allows users to easily design and experiment with various application scenarios on top of {GPU}-parallelized simulations. It also offers a range of benchmark tasks, presenting challenges ranging from single-drone hovering to over-actuated system tracking. In summary, we propose an open-sourced drone simulation platform, equipped with an extensive suite of tools for drone learning. It includes 4 drone models, 5 sensor modalities, 4 control modes, over 10 benchmark tasks, and a selection of widely used {RL} baselines. To showcase the capabilities of {OmniDrones} and to support future research, we also provide preliminary results on these benchmark tasks. We hope this platform will encourage further studies on applying {RL} to practical drone systems.},
  file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/TFDXMY3J/Xu et al. - 2024 - OmniDrones An Efficient and Flexible Platform for Reinforcement Learning in Drone Control.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/VFVBN5GE/10409589.html:text/html},
  shorttitle   = {{OmniDrones}},
}

@Article{tognon_aerial_2018,
  author       = {Tognon, Marco and Gabellieri, Chiara and Pallottino, Lucia and Franchi, Antonio},
  date         = {2018-07},
  journaltitle = {{IEEE} Robotics and Automation Letters},
  title        = {Aerial Co-Manipulation With Cables: The Role of Internal Force for Equilibria, Stability, and Passivity},
  doi          = {10.1109/LRA.2018.2803811},
  issn         = {2377-3766},
  pages        = {2577--2583},
  volume       = {3},
  abstract     = {This letter considers the cooperative manipulation of a cable-suspended load with two generic aerial robots without the need of explicit communication. The role of the internal force for the asymptotic stability of the beam position-and-attitude equilibria is analyzed in depth. Using a nonlinear Lyapunov-based approach, we prove that if a nonzero internal force is chosen, then the asymptotic stabilization of any desired beam attitude can be achieved with a decentralized and communicationless master–slave admittance controller. If, conversely, a zero internal force is chosen, as done in the majority of the state-of-the-art algorithms, the attitude of the beam is not controllable without communication. Furthermore, we formally proof the output-strictly passivity of the system with respect to an energy-like storage function and a certain input–output pair. This proves the stability and the robustness of the method during motion and in nonideal. The theoretical findings are validated through extensive simulations.},
  file         = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/VK5YJM2L/8286868.html:text/html;Submitted Version:/Users/viktorlorentz/Zotero/storage/4DEWYJ6I/Tognon et al. - 2018 - Aerial Co-Manipulation With Cables The Role of Internal Force for Equilibria, Stability, and Passiv.pdf:application/pdf},
  shorttitle   = {Aerial Co-Manipulation With Cables},
}

@Article{Zhou2020EGOSwarmAF,
  author       = {Xiaoxia Zhou and Jiangchao Zhu and Hongyu Zhou and Chao Xu and Fei Gao},
  date         = {2020},
  journaltitle = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
  title        = {EGO-Swarm: A Fully Autonomous and Decentralized Quadrotor Swarm System in Cluttered Environments},
  pages        = {4101--4107},
}

@Software{gymnax2022github,
  author  = {Robert Tjarko Lange},
  date    = {2022},
  title   = {{gymnax}: A {JAX}-based Reinforcement Learning Environment Library},
  url     = {http://github.com/RobertTLange/gymnax},
  version = {0.0.4},
}

@Article{Estevez2024Reinforcement,
  author       = {Estevez, Julian and Manuel Lopez-Guede, Jose and del Valle-Echavarri, Javier and Graña, Manuel},
  date         = {2024},
  journaltitle = {IEEE Access},
  title        = {Reinforcement Learning Based Trajectory Planning for Multi-UAV Load Transportation},
  doi          = {10.1109/ACCESS.2024.3470509},
  pages        = {144009--144016},
  volume       = {12},
}

@Article{jackson_scalable_2020,
  author       = {Jackson, Brian E. and Howell, Taylor A. and Shah, Kunal and Schwager, Mac and Manchester, Zachary},
  date         = {2020-04},
  journaltitle = {{IEEE} Robotics and Automation Letters},
  title        = {Scalable Cooperative Transport of Cable-Suspended Loads With {UAVs} Using Distributed Trajectory Optimization},
  doi          = {10.1109/LRA.2020.2975956},
  issn         = {2377-3766},
  pages        = {3368--3374},
  volume       = {5},
  abstract     = {Most approaches to multi-robot control either rely on local decentralized control policies that scale well in the number of agents, or on centralized methods that can handle constraints and produce rich system-level behavior, but are typically computationally expensive and scale poorly in the number of agents, relegating them to offline planning. This work presents a scalable approach that uses distributed trajectory optimization to parallelize computation over a group of computationally-limited agents while handling general nonlinear dynamics and non-convex constraints. The approach, including near-real-time onboard trajectory generation, is demonstrated in hardware on a cable-suspended load problem with a team of quadrotors automatically reconfiguring to transport a heavy load through a doorway.},
  file         = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/QI9MW8ZA/9007450.html:text/html},
}

@InProceedings{Pandit2024LearningDM,
  author    = {Bikram Pandit and Ashutosh Gupta and Mohitvishnu S. Gadde and Addison Johnson and Aayam Shrestha and Helei Duan and Jeremy Dao and Alan Fern},
  booktitle = {Conference on Robot Learning},
  date      = {2024},
  title     = {Learning Decentralized Multi-Biped Control for Payload Transport},
  doi       = {10.48550/arxiv.2406.17279},
  eprint    = {2406.17279},
}

@Article{ma2024skilltransfer,
  author       = {Ma, Haitong and Ren, Zhaolin and Dai, Bo and Li, Na},
  date         = {2024},
  journaltitle = {technical report},
  title        = {Skill Transfer and Discovery for Sim-to-Real Learning: A Representation-Based Viewpoint},
  doi          = {10.48550/arxiv.2404.05051},
  eprint       = {2404.05051},
}

@Article{lee_geometric_2018,
  author       = {Lee, Taeyoung},
  date         = {2018-01},
  journaltitle = {{IEEE} Transactions on Control Systems Technology},
  title        = {Geometric Control of Quadrotor {UAVs} Transporting a Cable-Suspended Rigid Body},
  doi          = {10.1109/TCST.2017.2656060},
  issn         = {1558-0865},
  pages        = {255--264},
  volume       = {26},
  abstract     = {This paper is focused on tracking control for a rigid body payload that is connected to an arbitrary number of quadrotor unmanned aerial vehicles via rigid links. An intrinsic form of the equations of motion is derived on the nonlinear configuration manifold, and a geometric controller is constructed such that the payload asymptotically follows a given desired trajectory for its position and attitude in the presence of uncertainties. The unique feature is that the coupled dynamics between the rigid body payload, links, and quadrotors are explicitly incorporated into control system design and stability analysis. These are developed in a coordinate-free fashion to avoid singularities and complexities that are associated with local parameterizations. The desirable features of the proposed control system are illustrated by a numerical example.},
  file         = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/ZCJJ2A9J/7843619.html:text/html},
}

@Article{Song2023ReachingTL,
  author       = {Yunlong Song and Angel Romero and Matthias M{\"u}ller and Vladlen Koltun and Davide Scaramuzza},
  date         = {2023},
  journaltitle = {Science Robotics},
  title        = {Reaching the limit in autonomous racing: Optimal control versus reinforcement learning},
  volume       = {8},
}

@Article{Lyu2023UnmannedAVA,
  author       = {Mingyang Lyu and Yibo Zhao and Chao Huang and Hailong Huang},
  date         = {2023},
  journaltitle = {Remote. Sens.},
  title        = {Unmanned Aerial Vehicles for Search and Rescue: A Survey},
  pages        = {3266},
  volume       = {15},
}


@InProceedings{sreenath_dynamics_2013,
  author     = {Sreenath, Koushil and Kumar, Vijay},
  booktitle  = {Robotics: Science and Systems {IX}},
  date       = {2013-06-23},
  title      = {Dynamics, Control and Planning for Cooperative Manipulation of Payloads Suspended by Cables from Multiple Quadrotor Robots},
  doi        = {10.15607/RSS.2013.IX.011},
  eventtitle = {Robotics: Science and Systems 2013},
  isbn       = {978-981-07-3937-9},
  abstract   = {We address the problem of cooperative transportation of a cable-suspended payload by multiple quadrotors. In previous work, quasi-static models have been used to study this problem. However, these approaches are severely limited because they ignore the payload dynamics, and do not explicitly model the underactuation in the control problem. Thus, there are no guarantees on the payload trajectory or the cable tensions, which must be non negative. In this paper, we develop a complete dynamic model for the case when payload is a point load and for the case when the payload is a rigid body. We show in both cases the resulting system is differentially flat when the cable tensions are strictly positive. We also consider the case where the tensions are non negative (including the case with zero tensions) and establish that these systems are differentially flat hybrid systems by considering the switching dynamics induced by the unilateral tension constraints. We use the differential flatness property to find dynamically feasible trajectories for the payload+quadrotors system. We show using numerical and experimental methods that these trajectories are superior to those obtained by quasi-static models.},
  file       = {PDF:/Users/viktorlorentz/Zotero/storage/YMUPSRPT/Sreenath and Kumar - 2013 - Dynamics, Control and Planning for Cooperative Manipulation of Payloads Suspended by Cables from Mul.pdf:application/pdf},
  langid     = {english},
}

@InProceedings{wahba_kinodynamic_2024,
  author     = {Wahba, Khaled and Ortiz-Haro, Joaquim and Toussaint, Marc and Hönig, Wolfgang},
  booktitle  = {2024 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
  date       = {2024-10},
  title      = {Kinodynamic Motion Planning for a Team of Multirotors Transporting a Cable-Suspended Payload in Cluttered Environments},
  doi        = {10.1109/IROS58592.2024.10802794},
  eventtitle = {2024 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
  pages      = {12750--12757},
  abstract   = {We propose a motion planner for cable-driven payload transportation using multiple unmanned aerial vehicles ({UAVs}) in an environment cluttered with obstacles. Our planner is kinodynamic, i.e., it considers the full dynamics model of the transporting system including actuation constraints. Due to the high dimensionality of the planning problem, we use a hierarchical approach where we first solve for the geometric motion using a sampling-based method with a novel sampler, followed by constrained trajectory optimization that considers the full dynamics of the system. Both planning stages consider inter-robot and robot/obstacle collisions. We demonstrate in a software-in-the-loop simulation and real flight experiments that there is a significant benefit in kinodynamic motion planning for such payload transport systems with respect to payload tracking error and energy consumption compared to the standard methods of planning for the payload alone. Notably, we observe a significantly higher success rate in scenarios where the team formation changes are needed to move through tight spaces.},
  file       = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/JZ75CYKF/10802794.html:text/html;PDF:/Users/viktorlorentz/Zotero/storage/SV6U36FI/Wahba et al. - 2024 - Kinodynamic Motion Planning for a Team of Multirotors Transporting a Cable-Suspended Payload in Clut.pdf:application/pdf},
}

@InProceedings{todorov2012mujoco,
  author     = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle  = {2012 IEEE/RSJ international conference on intelligent robots and systems},
  date       = {2012-10},
  title      = {Mujoco: A physics engine for model-based control},
  doi        = {10.1109/IROS.2012.6386109},
  eventtitle = {2012 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems},
  pages      = {5026--5033},
  abstract   = {We describe a new physics engine tailored to model-based control. Multi-joint dynamics are represented in generalized coordinates and computed via recursive algorithms. Contact responses are computed via efficient new algorithms we have developed, based on the modern velocity-stepping approach which avoids the difficulties with spring-dampers. Models are specified using either a high-level C++ {API} or an intuitive {XML} file format. A built-in compiler transforms the user model into an optimized data structure used for runtime computation. The engine can compute both forward and inverse dynamics. The latter are well-defined even in the presence of contacts and equality constraints. The model can include tendon wrapping as well as actuator activation states (e.g. pneumatic cylinders or muscles). To facilitate optimal control applications and in particular sampling and finite differencing, the dynamics can be evaluated for different states and controls in parallel. Around 400,000 dynamics evaluations per second are possible on a 12-core machine, for a 3D homanoid with 18 dofs and 6 active contacts. We have already used the engine in a number of control applications. It will soon be made publicly available.},
  shorttitle = {{MuJoCo}},
}

@Article{Hwangbo2017ControlOA,
  author       = {Jemin Hwangbo and Inkyu Sa and R. Siegwart and Marco Hutter},
  date         = {2017},
  journaltitle = {IEEE Robotics and Automation Letters},
  title        = {Control of a Quadrotor With Reinforcement Learning},
  doi          = {10.48550/arxiv.1707.05110},
  eprint       = {1707.05110},
  pages        = {2096--2103},
  volume       = {2},
}

@Article{chen_what_2024,
  author       = {Chen, Jiayu and Yu, Chao and Xie, Yuqing and Gao, Feng and Chen, Yinuo and Yu, Shu’ang and Tang, Wenhao and Ji, Shilong and Mu, Mo and Wu, Yi and Yang, Huazhong and Wang, Yu},
  date         = {2025-07},
  journaltitle = {IEEE Robotics and Automation Letters},
  title        = {What Matters in Learning a Zero-Shot Sim-to-Real RL Policy for Quadrotor Control? A Comprehensive Study},
  doi          = {10.1109/lra.2025.3575011},
  issn         = {2377-3774},
  number       = {7},
  pages        = {7134--7141},
  volume       = {10},
  publisher    = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@Article{schulman2017proximal,
  author       = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  date         = {2017},
  journaltitle = {CoRR},
  title        = {Proximal Policy Optimization Algorithms},
  doi          = {10.48550/arxiv.1707.06347},
  eprint       = {1707.06347},
  eprinttype   = {arXiv},
  url          = {http://arxiv.org/abs/1707.06347},
  volume       = {abs/1707.06347},
  abstract     = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization ({PPO}), have some of the benefits of trust region policy optimization ({TRPO}), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test {PPO} on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that {PPO} outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  biburl       = {https://dblp.org/rec/journals/corr/SchulmanWDRK17.bib},
}

@Article{Chen2022TowardsHBA,
  author       = {Yuanpei Chen and Yaodong Yang and Tianhao Wu and Shengjie Wang and Xidong Feng and Jiechuang Jiang and Stephen Marcus McAleer and Hao Dong and Zongqing Lu and Song{-}Chun Zhu},
  date         = {2022},
  journaltitle = {CoRR},
  title        = {Towards Human-Level Bimanual Dexterous Manipulation with Reinforcement Learning},
  doi          = {10.48550/ARXIV.2206.08686},
  eprint       = {2206.08686},
  eprinttype   = {arXiv},
  volume       = {abs/2206.08686},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2206-08686.bib},
}

@InProceedings{flair2023jaxmarl,
  author    = {Alexander Rutherford and Benjamin Ellis and Matteo Gallici and Jonathan Cook and Andrei Lupu and Gar{\dh}ar Ingvarsson and Timon Willi and Ravi Hammond and Akbir Khan and Christian Schr{\"{o}}der de Witt and Alexandra Souly and Saptarashmi Bandyopadhyay and Mikayel Samvelyan and Minqi Jiang and Robert T. Lange and Shimon Whiteson and Bruno Lacerda and Nick Hawes and Tim Rockt{\"{a}}schel and Chris Lu and Jakob N. Foerster},
  booktitle = {Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024},
  date      = {2024},
  title     = {JaxMARL: Multi-Agent {RL} Environments and Algorithms in {JAX}},
  editor    = {Amir Globersons and Lester Mackey and Danielle Belgrave and Angela Fan and Ulrich Paquet and Jakub M. Tomczak and Cheng Zhang},
  url       = {http://papers.nips.cc/paper_files/paper/2024/hash/5aee125f052c90e326dcf6f380df94f6-Abstract-Datasets_and_Benchmarks_Track.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/nips/RutherfordEG0LI24.bib},
}

@Article{Schulman2015TrustRP,
  author       = {John Schulman and Sergey Levine and Philipp Moritz and Michael I. Jordan and Pieter Abbeel},
  date         = {2015},
  journaltitle = {CoRR},
  title        = {Trust Region Policy Optimization},
  doi          = {10.48550/arxiv.1502.05477},
  eprint       = {1502.05477},
  eprinttype   = {arXiv},
  url          = {http://arxiv.org/abs/1502.05477},
  volume       = {abs/1502.05477},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  biburl       = {https://dblp.org/rec/journals/corr/SchulmanLMJA15.bib},
}

@Article{Dimmig2023SurveyOS,
  author       = {Cora A. Dimmig and Giuseppe Silano and Kimberly McGuire and Chiara Gabellieri and Wolfgang H{\"{o}}nig and Joseph L. Moore and Marin Kobilarov},
  date         = {2025},
  journaltitle = {{IEEE} Robotics Autom. Mag.},
  title        = {Survey of Simulators for Aerial Robots: An Overview and In-Depth Systematic Comparisons [Survey]},
  doi          = {10.1109/MRA.2024.3433171},
  eprint       = {2311.02296},
  eprinttype   = {arXiv},
  number       = {2},
  pages        = {153--166},
  volume       = {32},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  biburl       = {https://dblp.org/rec/journals/ram/DimmigSMGHMK25.bib},
}

@Article{Lowe2017MultiAgentAF,
  author       = {Ryan Lowe and Yi Wu and Aviv Tamar and Jean Harb and Pieter Abbeel and Igor Mordatch},
  date         = {2017},
  journaltitle = {CoRR},
  title        = {Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},
  doi          = {10.48550/arxiv.1706.02275},
  eprint       = {1706.02275},
  eprinttype   = {arXiv},
  url          = {http://arxiv.org/abs/1706.02275},
  volume       = {abs/1706.02275},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  biburl       = {https://dblp.org/rec/journals/corr/LoweWTHAM17.bib},
}

@Article{Wang2025SafeAA,
  author       = {Yongchao Wang and Junjie Wang and Xiaobin Zhou and Tiankai Yang and Chao Xu and Fei Gao},
  date         = {2025},
  journaltitle = {CoRR},
  title        = {Safe and Agile Transportation of Cable-Suspended Payload via Multiple Aerial Robots},
  doi          = {10.48550/ARXIV.2501.15272},
  eprint       = {2501.15272},
  eprinttype   = {arXiv},
  volume       = {abs/2501.15272},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2501-15272.bib},
}

@Article{Chen2025DecentralizedNO,
  author       = {Wen{-}Tse Chen and Minh Nguyen and Zhongyu Li and Guo Ning Sue and Koushil Sreenath},
  date         = {2025},
  journaltitle = {CoRR},
  title        = {Decentralized Navigation of a Cable-Towed Load using Quadrupedal Robot Team via {MARL}},
  doi          = {10.48550/ARXIV.2503.18221},
  eprint       = {2503.18221},
  eprinttype   = {arXiv},
  volume       = {abs/2503.18221},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2503-18221.bib},
}

@Article{diao_efficient_2024,
  author       = {Xingrong Diao and Jiankun Wang},
  date         = {2024},
  journaltitle = {CoRR},
  title        = {Efficient Multi-agent Navigation with Lightweight {DRL} Policy},
  doi          = {10.48550/ARXIV.2408.16370},
  eprint       = {2408.16370},
  eprinttype   = {arXiv},
  volume       = {abs/2408.16370},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2408-16370.bib},
}

@Article{xie_multi-uav_2024,
  author       = {Yuqing Xie and Chao Yu and Hongzhi Zang and Feng Gao and Wenhao Tang and Jingyi Huang and Jiayu Chen and Botian Xu and Yi Wu and Yu Wang},
  date         = {2024},
  journaltitle = {CoRR},
  title        = {Multi-UAV Behavior-based Formation with Static and Dynamic Obstacles Avoidance via Reinforcement Learning},
  doi          = {10.48550/ARXIV.2410.18495},
  eprint       = {2410.18495},
  eprinttype   = {arXiv},
  volume       = {abs/2410.18495},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2410-18495.bib},
}

@Article{Eschmann2024,
  author       = {Eschmann, Jonas and Albani, Dario and Loianno, Giuseppe},
  date         = {2024-07},
  journaltitle = {{IEEE} Robotics and Automation Letters},
  title        = {Learning to Fly in Seconds},
  doi          = {10.1109/LRA.2024.3396025},
  issn         = {2377-3766},
  number       = {7},
  pages        = {6336--6343},
  volume       = {9},
  abstract     = {Learning-based methods, particularly Reinforcement Learning ({RL}), hold great promise for streamlining deployment, enhancing performance, and achieving generalization in the control of autonomous multirotor aerial vehicles. Deep {RL} has been able to control complex systems with impressive fidelity and agility in simulation but the simulation-to-reality transfer often brings a hard-to-bridge reality gap. Moreover, {RL} is commonly plagued by prohibitively long training times. In this work, we propose a novel asymmetric actor-critic-based architecture coupled with a highly reliable {RL}-based training paradigm for end-to-end quadrotor control. We show how curriculum learning and a highly optimized simulator enhance sample complexity and lead to fast training times. To precisely discuss the challenges related to low-level/end-to-end multirotor control, we also introduce a taxonomy that classifies the existing levels of control abstractions as well as non-linearities and domain parameters. Our framework enables Simulation-to-Reality (Sim2Real) transfer for direct Revolutions Per Minute ({RPM}) control after only 18 seconds of training on a consumer-grade laptop as well as its deployment on microcontrollers to control a multirotor under real-time guarantees. Finally, our solution exhibits competitive performance in trajectory tracking, as demonstrated through various experimental comparisons with existing state-of-the-art control solutions using a real Crazyflie nano quadrotor. We open source the code including a very fast multirotor dynamics simulator that can simulate about 5 months of flight per second on a laptop {GPU}. The fast training times and deployment to a cheap, off-the-shelf quadrotor lower the barriers to entry and help democratize the research and development of these systems.},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  biburl       = {https://dblp.org/rec/journals/ral/EschmannAL24.bib},
  file         = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/94F4G94Y/10517383.html:text/html},
}

@Article{witt_is_2020,
  author       = {Christian Schr{\"{o}}der de Witt and Tarun Gupta and Denys Makoviichuk and Viktor Makoviychuk and Philip H. S. Torr and Mingfei Sun and Shimon Whiteson},
  date         = {2020},
  journaltitle = {CoRR},
  title        = {Is Independent Learning All You Need in the StarCraft Multi-Agent Challenge?},
  doi          = {10.48550/arxiv.2011.09533},
  eprint       = {2011.09533},
  eprinttype   = {arXiv},
  url          = {https://arxiv.org/abs/2011.09533},
  volume       = {abs/2011.09533},
  abstract     = {Most recently developed approaches to cooperative multi-agent reinforcement learning in the {\textbackslash}emph\{centralized training with decentralized execution\} setting involve estimating a centralized, joint value function. In this paper, we demonstrate that, despite its various theoretical shortcomings, Independent {PPO} ({IPPO}), a form of independent learning in which each agent simply estimates its local value function, can perform just as well as or better than state-of-the-art joint learning approaches on popular multi-agent benchmark suite {SMAC} with little hyperparameter tuning. We also compare {IPPO} to several variants; the results suggest that {IPPO}'s strong performance may be due to its robustness to some forms of environment non-stationarity.},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2011-09533.bib},
}

@Article{huang_quadswarm_2023,
  author       = {Zhehui Huang and Sumeet Batra and Tao Chen and Rahul Krupani and Tushar Kumar and Artem Molchanov and Aleksei Petrenko and James A. Preiss and Zhaojing Yang and Gaurav S. Sukhatme},
  date         = {2023},
  journaltitle = {CoRR},
  title        = {QuadSwarm: {A} Modular Multi-Quadrotor Simulator for Deep Reinforcement Learning with Direct Thrust Control},
  doi          = {10.48550/ARXIV.2306.09537},
  eprint       = {2306.09537},
  eprinttype   = {arXiv},
  volume       = {abs/2306.09537},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2306-09537.bib},
}

@Book{oliehoek_concise_2016,
  author    = {Oliehoek, Frans A. and Amato, Christopher},
  date      = {2016},
  title     = {A Concise Introduction to Decentralized POMDPs},
  doi       = {10.1007/978-3-319-28929-8},
  isbn      = {978-3-319-28927-4},
  location  = {Cham},
  publisher = {Springer},
  series    = {Springer Briefs in Intelligent Systems},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/series/sbis/OliehoekA16.bib},
  langid    = {english},
  rights    = {http://www.springer.com/tdm},
}

@Article{tagliabue_robust_2017,
  author       = {Andrea Tagliabue and Mina Kamel and Roland Siegwart and Juan I. Nieto},
  date         = {2019},
  journaltitle = {Int. J. Robotics Res.},
  title        = {Robust collaborative object transportation using multiple MAVs},
  doi          = {10.1177/0278364919854131},
  number       = {9},
  volume       = {38},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  biburl       = {https://dblp.org/rec/journals/ijrr/TagliabueKSN19.bib},
}

@InProceedings{yu_surprising_2022,
  author    = {Chao Yu and Akash Velu and Eugene Vinitsky and Jiaxuan Gao and Yu Wang and Alexandre M. Bayen and Yi Wu},
  booktitle = {Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022},
  date      = {2022},
  title     = {The Surprising Effectiveness of {PPO} in Cooperative Multi-Agent Games},
  editor    = {Sanmi Koyejo and S. Mohamed and A. Agarwal and Danielle Belgrave and K. Cho and A. Oh},
  url       = {http://papers.nips.cc/paper_files/paper/2022/hash/9c1535a02f0ce079433344e14d910597-Abstract-Datasets_and_Benchmarks.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/nips/YuVVGWBW22.bib},
}

@Book{SuttonBarto2018,
  author    = {Richard S. Sutton and Andrew G. Barto},
  date      = {2018},
  title     = {Reinforcement learning - an introduction, 2nd Edition},
  publisher = {{MIT} Press},
  url       = {http://www.incompleteideas.net/book/the-book-2nd.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/books/lib/SuttonB2018.bib},
}

@Comment{jabref-meta: databaseType:biblatex;}
