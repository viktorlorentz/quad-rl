@inproceedings{Wahba2024,
	title        = {Kinodynamic Motion Planning for a Team of Multirotors Transporting a Cable-Suspended Payload in Cluttered Environments},
	author       = {Wahba, Khaled and Ortiz-Haro, Joaquim and Toussaint, Marc and Hönig, Wolfgang},
	year         = 2024,
	booktitle    = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
	pages        = {12750--12757},
	doi          = {10.1109/IROS58592.2024.10802794}
}
@article{Belkhale2021,
	title        = {Model-Based Meta-Reinforcement Learning for Flight With Suspended Payloads},
	author       = {Belkhale, Suneel and Li, Rachel and Kahn, Gregory and McAllister, Rowan and Calandra, Roberto and Levine, Sergey},
	year         = 2021,
	journal      = {IEEE Robotics and Automation Letters},
	volume       = 6,
	pages        = {1471–1478},
	doi          = {10.1109/lra.2021.3057046},
	issn         = {2377-3774}
}
@article{schulman2017proximal,
	title        = {Proximal policy optimization algorithms},
	author       = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	year         = 2017,
	journal      = {arXiv preprint arXiv:1707.06347}
}
@book{SuttonBarto2018,
	title        = {Reinforcement Learning: An Introduction},
	author       = {Sutton, Richard S. and Barto, Andrew G.},
	year         = 2018,
	edition      = {2nd}
}
@article{flair2023jaxmarl,
	title        = {JaxMARL: Multi-Agent RL Environments in JAX},
	author       = {Alexander Rutherford and Benjamin Ellis and Matteo Gallici and Jonathan Cook and Andrei Lupu and Gardar Ingvarsson and Timon Willi and Akbir Khan and Christian Schroeder de Witt and Alexandra Souly and Saptarashmi Bandyopadhyay and Mikayel Samvelyan and Minqi Jiang and Robert Tjarko Lange and Shimon Whiteson and Bruno Lacerda and Nick Hawes and Tim Rocktaschel and Chris Lu and Jakob Nicolaus Foerster},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2311.10090}
}
@article{belkhale_model-based_2021,
	title        = {Model-Based Meta-Reinforcement Learning for Flight with Suspended Payloads},
	author       = {Belkhale, Suneel and Li, Rachel and Kahn, Gregory and {McAllister}, Rowan and Calandra, Roberto and Levine, Sergey},
	volume       = 6,
	pages        = {1471--1478},
	doi          = {10.1109/LRA.2021.3057046},
	issn         = {2377-3766, 2377-3774},
	abstract     = {Transporting suspended payloads is challenging for autonomous aerial vehicles because the payload can cause significant and unpredictable changes to the robot's dynamics. These changes can lead to suboptimal flight performance or even catastrophic failure. Although adaptive control and learning-based methods can in principle adapt to changes in these hybrid robot-payload systems, rapid mid-flight adaptation to payloads that have a priori unknown physical properties remains an open problem. We propose a meta-learning approach that "learns how to learn" models of altered dynamics within seconds of post-connection flight data. Our experiments demonstrate that our online adaptation approach outperforms non-adaptive methods on a series of challenging suspended payload transportation tasks. Videos and other supplemental material are available on our website: https://sites.google.com/view/meta-rl-for-flight},
	journaltitle = {{IEEE} Robotics and Automation Letters},
	shortjournal = {{IEEE} Robot. Autom. Lett.},
	date         = {2021-04},
	file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/EB6VM7KN/Belkhale et al. - 2021 - Model-Based Meta-Reinforcement Learning for Flight with Suspended Payloads.pdf:application/pdf;Snapshot:/Users/viktorlorentz/Zotero/storage/3F9IBRDX/2004.html:text/html}
}
@article{villa_cooperative_2021,
	title        = {Cooperative Load Transportation With Two Quadrotors Using Adaptive Control},
	author       = {Villa, Daniel Khede Dourado and Brandão, Alexandre Santos and Carelli, Ricardo and Sarcinelli-Filho, Mário},
	volume       = 9,
	pages        = {129148--129160},
	doi          = {10.1109/ACCESS.2021.3113466},
	issn         = {2169-3536},
	abstract     = {The problem of carrying a bar-shaped payload suspended by flexible cables attached to two quadrotors is analyzed in this work. The aerial vehicles and the load are dealt with as a single system, whose kinematics is described as a multi-robot formation using the virtual structure approach. The dynamic effects caused by the tethered load over the quadrotors, as well as those caused by each quadrotor over the other, are treated by an adaptive dynamic compensator. To validate the proposal, experiments were run testing the system in adverse conditions: transportation far from quasi-static motion, high payload-to-quadrotor weight ratio, 20\% of error in the robot model parameters, transportation under wind disturbances, and payload weight changes during flight. The good performance of the proposed control system in all these tests allows concluding that the proposed system is able to accomplish payload positioning, orientation, and trajectory tracking under adverse conditions, with accelerations up to 1.6 m/s2.},
	journaltitle = {{IEEE} Access},
	date         = 2021,
	file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/TZSTFIV7/Villa et al. - 2021 - Cooperative Load Transportation With Two Quadrotors Using Adaptive Control.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/S2TGNJMA/9540662.html:text/html}
}
@article{xu_omnidrones_2024,
	title        = {{OmniDrones}: An Efficient and Flexible Platform for Reinforcement Learning in Drone Control},
	shorttitle   = {{OmniDrones}},
	author       = {Xu, Botian and Gao, Feng and Yu, Chao and Zhang, Ruize and Wu, Yi and Wang, Yu},
	volume       = 9,
	pages        = {2838--2844},
	doi          = {10.1109/LRA.2024.3356168},
	issn         = {2377-3766},
	abstract     = {In this letter, we introduce {OmniDrones}, an efficient and flexible platform tailored for reinforcement learning in drone control, built on Nvidia's Omniverse Isaac Sim. It employs a bottom-up design approach that allows users to easily design and experiment with various application scenarios on top of {GPU}-parallelized simulations. It also offers a range of benchmark tasks, presenting challenges ranging from single-drone hovering to over-actuated system tracking. In summary, we propose an open-sourced drone simulation platform, equipped with an extensive suite of tools for drone learning. It includes 4 drone models, 5 sensor modalities, 4 control modes, over 10 benchmark tasks, and a selection of widely used {RL} baselines. To showcase the capabilities of {OmniDrones} and to support future research, we also provide preliminary results on these benchmark tasks. We hope this platform will encourage further studies on applying {RL} to practical drone systems.},
	journaltitle = {{IEEE} Robotics and Automation Letters},
	date         = {2024-03},
	file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/TFDXMY3J/Xu et al. - 2024 - OmniDrones An Efficient and Flexible Platform for Reinforcement Learning in Drone Control.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/VFVBN5GE/10409589.html:text/html}
}
@inproceedings{wahba_kinodynamic_2024,
	title        = {Kinodynamic Motion Planning for a Team of Multirotors Transporting a Cable-Suspended Payload in Cluttered Environments},
	author       = {Wahba, Khaled and Ortiz-Haro, Joaquim and Toussaint, Marc and Hönig, Wolfgang},
	booktitle    = {2024 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	pages        = {12750--12757},
	doi          = {10.1109/IROS58592.2024.10802794},
	abstract     = {We propose a motion planner for cable-driven payload transportation using multiple unmanned aerial vehicles ({UAVs}) in an environment cluttered with obstacles. Our planner is kinodynamic, i.e., it considers the full dynamics model of the transporting system including actuation constraints. Due to the high dimensionality of the planning problem, we use a hierarchical approach where we first solve for the geometric motion using a sampling-based method with a novel sampler, followed by constrained trajectory optimization that considers the full dynamics of the system. Both planning stages consider inter-robot and robot/obstacle collisions. We demonstrate in a software-in-the-loop simulation and real flight experiments that there is a significant benefit in kinodynamic motion planning for such payload transport systems with respect to payload tracking error and energy consumption compared to the standard methods of planning for the payload alone. Notably, we observe a significantly higher success rate in scenarios where the team formation changes are needed to move through tight spaces.},
	eventtitle   = {2024 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	date         = {2024-10},
	file         = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/JZ75CYKF/10802794.html:text/html;PDF:/Users/viktorlorentz/Zotero/storage/SV6U36FI/Wahba et al. - 2024 - Kinodynamic Motion Planning for a Team of Multirotors Transporting a Cable-Suspended Payload in Clut.pdf:application/pdf}
}
@article{eschmann_learning_2024,
	title        = {Learning to Fly in Seconds},
	author       = {Eschmann, Jonas and Albani, Dario and Loianno, Giuseppe},
	volume       = 9,
	pages        = {6336--6343},
	doi          = {10.1109/LRA.2024.3396025},
	issn         = {2377-3766},
	abstract     = {Learning-based methods, particularly Reinforcement Learning ({RL}), hold great promise for streamlining deployment, enhancing performance, and achieving generalization in the control of autonomous multirotor aerial vehicles. Deep {RL} has been able to control complex systems with impressive fidelity and agility in simulation but the simulation-to-reality transfer often brings a hard-to-bridge reality gap. Moreover, {RL} is commonly plagued by prohibitively long training times. In this work, we propose a novel asymmetric actor-critic-based architecture coupled with a highly reliable {RL}-based training paradigm for end-to-end quadrotor control. We show how curriculum learning and a highly optimized simulator enhance sample complexity and lead to fast training times. To precisely discuss the challenges related to low-level/end-to-end multirotor control, we also introduce a taxonomy that classifies the existing levels of control abstractions as well as non-linearities and domain parameters. Our framework enables Simulation-to-Reality (Sim2Real) transfer for direct Revolutions Per Minute ({RPM}) control after only 18 seconds of training on a consumer-grade laptop as well as its deployment on microcontrollers to control a multirotor under real-time guarantees. Finally, our solution exhibits competitive performance in trajectory tracking, as demonstrated through various experimental comparisons with existing state-of-the-art control solutions using a real Crazyflie nano quadrotor. We open source the code including a very fast multirotor dynamics simulator that can simulate about 5 months of flight per second on a laptop {GPU}. The fast training times and deployment to a cheap, off-the-shelf quadrotor lower the barriers to entry and help democratize the research and development of these systems.},
	journaltitle = {{IEEE} Robotics and Automation Letters},
	date         = {2024-07},
	file         = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/94F4G94Y/10517383.html:text/html}
}
@inproceedings{sreenath_dynamics_2013,
	title        = {Dynamics, Control and Planning for Cooperative Manipulation of Payloads Suspended by Cables from Multiple Quadrotor Robots},
	author       = {Sreenath, Koushil and Kumar, Vijay},
	booktitle    = {Robotics: Science and Systems {IX}},
	doi          = {10.15607/RSS.2013.IX.011},
	isbn         = {978-981-07-3937-9},
	abstract     = {We address the problem of cooperative transportation of a cable-suspended payload by multiple quadrotors. In previous work, quasi-static models have been used to study this problem. However, these approaches are severely limited because they ignore the payload dynamics, and do not explicitly model the underactuation in the control problem. Thus, there are no guarantees on the payload trajectory or the cable tensions, which must be non negative. In this paper, we develop a complete dynamic model for the case when payload is a point load and for the case when the payload is a rigid body. We show in both cases the resulting system is differentially ﬂat when the cable tensions are strictly positive. We also consider the case where the tensions are non negative (including the case with zero tensions) and establish that these systems are differentially ﬂat hybrid systems by considering the switching dynamics induced by the unilateral tension constraints. We use the differential ﬂatness property to ﬁnd dynamically feasible trajectories for the payload+quadrotors system. We show using numerical and experimental methods that these trajectories are superior to those obtained by quasi-static models.},
	eventtitle   = {Robotics: Science and Systems 2013},
	date         = {2013-06-23},
	langid       = {english},
	file         = {PDF:/Users/viktorlorentz/Zotero/storage/YMUPSRPT/Sreenath and Kumar - 2013 - Dynamics, Control and Planning for Cooperative Manipulation of Payloads Suspended by Cables from Mul.pdf:application/pdf}
}
@article{lee_geometric_2018,
	title        = {Geometric Control of Quadrotor {UAVs} Transporting a Cable-Suspended Rigid Body},
	author       = {Lee, Taeyoung},
	volume       = 26,
	pages        = {255--264},
	doi          = {10.1109/TCST.2017.2656060},
	issn         = {1558-0865},
	abstract     = {This paper is focused on tracking control for a rigid body payload that is connected to an arbitrary number of quadrotor unmanned aerial vehicles via rigid links. An intrinsic form of the equations of motion is derived on the nonlinear configuration manifold, and a geometric controller is constructed such that the payload asymptotically follows a given desired trajectory for its position and attitude in the presence of uncertainties. The unique feature is that the coupled dynamics between the rigid body payload, links, and quadrotors are explicitly incorporated into control system design and stability analysis. These are developed in a coordinate-free fashion to avoid singularities and complexities that are associated with local parameterizations. The desirable features of the proposed control system are illustrated by a numerical example.},
	journaltitle = {{IEEE} Transactions on Control Systems Technology},
	date         = {2018-01},
	file         = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/ZCJJ2A9J/7843619.html:text/html}
}
@inproceedings{sun_nonlinear_2023,
	title        = {Nonlinear {MPC} for Full-Pose Manipulation of a Cable-Suspended Load using Multiple {UAVs}},
	author       = {Sun, Sihao and Franchi, Antonio},
	booktitle    = {2023 International Conference on Unmanned Aircraft Systems ({ICUAS})},
	pages        = {969--975},
	doi          = {10.1109/ICUAS57906.2023.10156031},
	abstract     = {In this work, we propose a centralized control method based on nonlinear model predictive control to let multiple {UAVs} manipulate the full pose of an object via cables. At the best of the authors knowledge this is the first method that takes into account the full nonlinear model of the load-{UAV} system, and ensures all the feasibility constraints concerning the {UAV} maximumum and minimum thrusts, the collision avoidance between the {UAVs}, cables and load, and the tautness and maximum tension of the cables. By taking into account the above factors, the proposed control algorithm can fully exploit the performance of {UAVs} and facilitate the speed of operation. Simulations are conducted to validate the algorithm to achieve fast and safe manipulation of the pose of a rigid-body payload using multiple {UAVs}. We demonstrate that the computational time of the proposed method is sufficiently small ({\textless}100 ms) for {UAV} teams composed by up to 10 units, which makes it suitable for a huge variety of future industrial applications, such as autonomous building construction and heavy-load transportation.},
	eventtitle   = {2023 International Conference on Unmanned Aircraft Systems ({ICUAS})},
	date         = {2023-06},
	file         = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/UX7W8J7P/10156031.html:text/html;Submitted Version:/Users/viktorlorentz/Zotero/storage/DFYSACLS/Sun and Franchi - 2023 - Nonlinear MPC for Full-Pose Manipulation of a Cable-Suspended Load using Multiple UAVs.pdf:application/pdf}
}
@inproceedings{li_nonlinear_2023,
	title        = {Nonlinear Model Predictive Control for Cooperative Transportation and Manipulation of Cable Suspended Payloads with Multiple Quadrotors},
	author       = {Li, Guanrui and Loianno, Giuseppe},
	booktitle    = {2023 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	pages        = {5034--5041},
	doi          = {10.1109/IROS55552.2023.10341785},
	abstract     = {Autonomous Micro Aerial Vehicles ({MAVs}) such as quadrotors equipped with manipulation mechanisms have the potential to assist humans in tasks such as construction and package delivery. Cables are a promising option for manipulation mechanisms due to their low weight, low cost, and simple design. However, designing control and planning strategies for cable mechanisms presents challenges due to indirect load actuation, nonlinear configuration space, and highly coupled system dynamics. In this paper, we propose a novel Nonlinear Model Predictive Control ({NMPC}) method that enables a team of quadrotors to manipulate a rigid-body payload in all 6 degrees of freedom via suspended cables. Our approach can concurrently exploit, as part of the receding horizon optimization, the available mechanical system redundancies to perform additional tasks such as inter-robot separation and obstacle avoidance while respecting payload dynamics and actuator constraints. To address real-time computational requirements and scalability, we employ a lightweight state vector parametrization that includes only payload states in all six degrees of freedom. This also enables the planning of trajectories on the {SE} (3) manifold load configuration space, thereby also reducing planning complexity. We validate the proposed approach through simulation and real-world experiments.},
	eventtitle   = {2023 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	date         = {2023-10},
	file         = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/PUNS9G3F/10341785.html:text/html;Submitted Version:/Users/viktorlorentz/Zotero/storage/CMRNZFHI/Li and Loianno - 2023 - Nonlinear Model Predictive Control for Cooperative Transportation and Manipulation of Cable Suspende.pdf:application/pdf}
}
@article{tognon_aerial_2018,
	title        = {Aerial Co-Manipulation With Cables: The Role of Internal Force for Equilibria, Stability, and Passivity},
	shorttitle   = {Aerial Co-Manipulation With Cables},
	author       = {Tognon, Marco and Gabellieri, Chiara and Pallottino, Lucia and Franchi, Antonio},
	volume       = 3,
	pages        = {2577--2583},
	doi          = {10.1109/LRA.2018.2803811},
	issn         = {2377-3766},
	abstract     = {This letter considers the cooperative manipulation of a cable-suspended load with two generic aerial robots without the need of explicit communication. The role of the internal force for the asymptotic stability of the beam position-and-attitude equilibria is analyzed in depth. Using a nonlinear Lyapunov-based approach, we prove that if a nonzero internal force is chosen, then the asymptotic stabilization of any desired beam attitude can be achieved with a decentralized and communicationless master–slave admittance controller. If, conversely, a zero internal force is chosen, as done in the majority of the state-of-the-art algorithms, the attitude of the beam is not controllable without communication. Furthermore, we formally proof the output-strictly passivity of the system with respect to an energy-like storage function and a certain input–output pair. This proves the stability and the robustness of the method during motion and in nonideal. The theoretical findings are validated through extensive simulations.},
	journaltitle = {{IEEE} Robotics and Automation Letters},
	date         = {2018-07},
	file         = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/VK5YJM2L/8286868.html:text/html;Submitted Version:/Users/viktorlorentz/Zotero/storage/4DEWYJ6I/Tognon et al. - 2018 - Aerial Co-Manipulation With Cables The Role of Internal Force for Equilibria, Stability, and Passiv.pdf:application/pdf}
}
@article{jackson_scalable_2020,
	title        = {Scalable Cooperative Transport of Cable-Suspended Loads With {UAVs} Using Distributed Trajectory Optimization},
	author       = {Jackson, Brian E. and Howell, Taylor A. and Shah, Kunal and Schwager, Mac and Manchester, Zachary},
	volume       = 5,
	pages        = {3368--3374},
	doi          = {10.1109/LRA.2020.2975956},
	issn         = {2377-3766},
	abstract     = {Most approaches to multi-robot control either rely on local decentralized control policies that scale well in the number of agents, or on centralized methods that can handle constraints and produce rich system-level behavior, but are typically computationally expensive and scale poorly in the number of agents, relegating them to offline planning. This work presents a scalable approach that uses distributed trajectory optimization to parallelize computation over a group of computationally-limited agents while handling general nonlinear dynamics and non-convex constraints. The approach, including near-real-time onboard trajectory generation, is demonstrated in hardware on a cable-suspended load problem with a team of quadrotors automatically reconfiguring to transport a heavy load through a doorway.},
	journaltitle = {{IEEE} Robotics and Automation Letters},
	date         = {2020-04},
	file         = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/QI9MW8ZA/9007450.html:text/html}
}
@article{gabellieri_equilibria_2023,
	title        = {Equilibria, Stability, and Sensitivity for the Aerial Suspended Beam Robotic System Subject to Parameter Uncertainty},
	author       = {Gabellieri, Chiara and Tognon, Marco and Sanalitro, Dario and Franchi, Antonio},
	volume       = 39,
	pages        = {3977--3993},
	doi          = {10.1109/TRO.2023.3279033},
	issn         = {1941-0468},
	abstract     = {This article studies how parametric uncertainties affect the cooperative manipulation of a cable-suspended beam-shaped load by means of two aerial robots not explicitly communicating with each other. In particular, this article sheds light on the impact of the uncertain knowledge of the model parameters available to an established communicationless force-based controller. First, we find the closed-loop equilibrium configurations in the presence of the aforementioned uncertainties, and then, we study their stability. Hence, we show the fundamental role played in the robustness of the load attitude control by the internal force induced in the manipulated object by nonvertical cables. Furthermore, we formally study the sensitivity of the attitude error to such parametric variations, and we provide a method to act on the load position error in the presence of uncertainties. Eventually, we validate the results through an extensive set of numerical tests in a realistic simulation environment, including underactuated aerial vehicles and sagging-prone cables, and through hardware experiments.},
	journaltitle = {{IEEE} Transactions on Robotics},
	date         = {2023-10},
	file         = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/3D74UXU3/10149811.html:text/html;Submitted Version:/Users/viktorlorentz/Zotero/storage/3TAI7ZA5/Gabellieri et al. - 2023 - Equilibria, Stability, and Sensitivity for the Aerial Suspended Beam Robotic System Subject to Param.pdf:application/pdf}
}
@misc{tagliabue_robust_2017,
	title        = {Robust Collaborative Object Transportation Using Multiple {MAVs}},
	author       = {Tagliabue, Andrea and Kamel, Mina and Siegwart, Roland and Nieto, Juan},
	doi          = {10.48550/arXiv.1711.08753},
	abstract     = {Collaborative object transportation using multiple Micro Aerial Vehicles ({MAVs}) with limited communication is a challenging problem. In this paper we address the problem of multiple {MAVs} mechanically coupled to a bulky object for transportation purposes without explicit communication between agents. The apparent physical properties of each agent are reshaped to achieve robustly stable transportation. Parametric uncertainties and unmodeled dynamics of each agent are quantified and techniques from robust control theory are employed to choose the physical parameters of each agent to guarantee stability. Extensive simulation analysis and experimental results show that the proposed method guarantees stability in worst case scenarios.},
	date         = {2017-11-23},
	file         = {Preprint PDF:/Users/viktorlorentz/Zotero/storage/C87HP647/Tagliabue et al. - 2017 - Robust Collaborative Object Transportation Using Multiple MAVs.pdf:application/pdf;Snapshot:/Users/viktorlorentz/Zotero/storage/4EM78BN3/1711.html:text/html}
}
@article{song_reaching_2023,
	title        = {Reaching the Limit in Autonomous Racing: Optimal Control versus Reinforcement Learning},
	shorttitle   = {Reaching the Limit in Autonomous Racing},
	author       = {Song, Yunlong and Romero, Angel and Mueller, Matthias and Koltun, Vladlen and Scaramuzza, Davide},
	volume       = 8,
	pages        = {eadg1462},
	doi          = {10.1126/scirobotics.adg1462},
	issn         = {2470-9476},
	abstract     = {A central question in robotics is how to design a control system for an agile mobile robot. This paper studies this question systematically, focusing on a challenging setting: autonomous drone racing. We show that a neural network controller trained with reinforcement learning ({RL}) outperformed optimal control ({OC}) methods in this setting. We then investigated which fundamental factors have contributed to the success of {RL} or have limited {OC}. Our study indicates that the fundamental advantage of {RL} over {OC} is not that it optimizes its objective better but that it optimizes a better objective. {OC} decomposes the problem into planning and control with an explicit intermediate representation, such as a trajectory, that serves as an interface. This decomposition limits the range of behaviors that can be expressed by the controller, leading to inferior control performance when facing unmodeled effects. In contrast, {RL} can directly optimize a task-level objective and can leverage domain randomization to cope with model uncertainty, allowing the discovery of more robust control responses. Our findings allowed us to push an agile drone to its maximum performance, achieving a peak acceleration greater than 12 times the gravitational acceleration and a peak velocity of 108 kilometers per hour. Our policy achieved superhuman control within minutes of training on a standard workstation. This work presents a milestone in agile robotics and sheds light on the role of {RL} and {OC} in robot control.},
	journaltitle = {Science Robotics},
	shortjournal = {Sci. Robot.},
	date         = {2023-09-13},
	file         = {Preprint PDF:/Users/viktorlorentz/Zotero/storage/ZAX73XFA/Song et al. - 2023 - Reaching the Limit in Autonomous Racing Optimal Control versus Reinforcement Learning.pdf:application/pdf;Snapshot:/Users/viktorlorentz/Zotero/storage/5C2GLX7A/2310.html:text/html}
}
@inproceedings{molchanov_sim--multi-real_2019,
	title        = {Sim-to-(Multi)-Real: Transfer of Low-Level Robust Control Policies to Multiple Quadrotors},
	shorttitle   = {Sim-to-(Multi)-Real},
	author       = {Molchanov, Artem and Chen, Tao and Hönig, Wolfgang and Preiss, James A. and Ayanian, Nora and Sukhatme, Gaurav S.},
	booktitle    = {2019 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	pages        = {59--66},
	doi          = {10.1109/IROS40897.2019.8967695},
	abstract     = {Quadrotor stabilizing controllers often require careful, model-specific tuning for safe operation. We use reinforcement learning to train policies in simulation that transfer remarkably well to multiple different physical quadrotors. Our policies are low-level, i.e., we map the rotorcrafts' state directly to the motor outputs. The trained control policies are very robust to external disturbances and can withstand harsh initial conditions such as throws. We show how different training methodologies (change of the cost function, modeling of noise, use of domain randomization) might affect flight performance. To the best of our knowledge, this is the first work that demonstrates that a simple neural network can learn a robust stabilizing low-level quadrotor controller (without the use of a stabilizing {PD} controller) that is shown to generalize to multiple quadrotors. The video of our experiments can be found at https://sites.google.com/view/sim-to-multi-quad.},
	eventtitle   = {2019 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	date         = {2019-11},
	file         = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/5MMNKWRE/8967695.html:text/html;Submitted Version:/Users/viktorlorentz/Zotero/storage/GGY77L2R/Molchanov et al. - 2019 - Sim-to-(Multi)-Real Transfer of Low-Level Robust Control Policies to Multiple Quadrotors.pdf:application/pdf}
}
@inproceedings{huang_collision_2024,
	title        = {Collision Avoidance and Navigation for a Quadrotor Swarm Using End-to-end Deep Reinforcement Learning},
	author       = {Huang, Zhehui and Yang, Zhaojing and Krupani, Rahul and Şenbaşlar, Baskın and Batra, Sumeet and Sukhatme, Gaurav S.},
	booktitle    = {2024 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	pages        = {300--306},
	doi          = {10.1109/ICRA57147.2024.10611499},
	abstract     = {End-to-end deep reinforcement learning ({DRL}) for quadrotor control promises many benefits – easy deployment, task generalization and real-time execution capability. Prior end-to-end {DRL}-based methods have showcased the ability to deploy learned controllers onto single quadrotors or quadrotor teams maneuvering in simple, obstacle-free environments. However, the addition of obstacles increases the number of possible interactions exponentially, thereby increasing the difficulty of training {RL} policies. In this work, we propose an end-to-end {DRL} approach to control quadrotor swarms in environments with obstacles. We provide our agents a curriculum and a replay buffer of the clipped collision episodes to improve performance in obstacle-rich environments. We implement an attention mechanism to attend to the neighbor robots and obstacle interactions - the first successful demonstration of this mechanism on policies for swarm behavior deployed on severely compute-constrained hardware. Our work is the first work that demonstrates the possibility of learning neighbor-avoiding and obstacle-avoiding control policies trained with end-to-end {DRL} that transfers zero-shot to real quadrotors. Our approach scales to 32 robots with 80\% obstacle density in simulation and 8 robots with 20\% obstacle density in physical deployment. Website: https://sites.google.com/view/obst-avoid-swarm-rl},
	eventtitle   = {2024 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	date         = {2024-05},
	file         = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/QSVAIBQD/10611499.html:text/html}
}
@article{felzenszwalb_distance_2012,
	title        = {Distance Transforms of Sampled Functions},
	author       = {Felzenszwalb, Pedro F. and Huttenlocher, Daniel P.},
	volume       = 8,
	pages        = {415--428},
	doi          = {10.4086/toc.2012.v008a019},
	journaltitle = {Theory of Computing},
	date         = {2012-09-02},
	file         = {Theory of computing Full Text PDF:/Users/viktorlorentz/Zotero/storage/8JNJVHG7/Felzenszwalb and Huttenlocher - 2012 - Distance Transforms of Sampled Functions.pdf:application/pdf;Theory of Computing Snapshot:/Users/viktorlorentz/Zotero/storage/LIFVQX47/v008a019.html:text/html}
}
@article{riviere_glas_2020,
	title        = {{GLAS}: Global-to-Local Safe Autonomy Synthesis for Multi-Robot Motion Planning With End-to-End Learning},
	shorttitle   = {{GLAS}},
	author       = {Rivière, Benjamin and Hönig, Wolfgang and Yue, Yisong and Chung, Soon-Jo},
	volume       = 5,
	pages        = {4249--4256},
	doi          = {10.1109/LRA.2020.2994035},
	issn         = {2377-3766},
	abstract     = {We present {GLAS}: Global-to-Local Autonomy Synthesis, a provably-safe, automated distributed policy generation for multi-robot motion planning. Our approach combines the advantage of centralized planning of avoiding local minima with the advantage of decentralized controllers of scalability and distributed computation. In particular, our synthesized policies only require relative state information of nearby neighbors and obstacles, and compute a provably-safe action. Our approach has three major components: i) we generate demonstration trajectories using a global planner and extract local observations from them, ii) we use deep imitation learning to learn a decentralized policy that can run efficiently online, and iii) we introduce a novel differentiable safety module to ensure collision-free operation, thereby allowing for end-to-end policy training. Our numerical experiments demonstrate that our policies have a 20\% higher success rate than optimal reciprocal collision avoidance, {ORCA}, across a wide range of robot and obstacle densities. We demonstrate our method on an aerial swarm, executing the policy on low-end microcontrollers in real-time.},
	journaltitle = {{IEEE} Robotics and Automation Letters},
	date         = {2020-07},
	file         = {Accepted Version:/Users/viktorlorentz/Zotero/storage/3ZQWZG3E/Rivière et al. - 2020 - GLAS Global-to-Local Safe Autonomy Synthesis for Multi-Robot Motion Planning With End-to-End Learni.pdf:application/pdf}
}
@article{zhao_deep_2024,
	title        = {Deep Reinforcement Learning-Driven Collaborative Rounding-Up for Multiple Unmanned Aerial Vehicles in Obstacle Environments},
	author       = {Zhao, Zipeng and Wan, Yu and Chen, Yong},
	volume       = 8,
	pages        = 464,
	doi          = {10.3390/drones8090464},
	issn         = {2504-446X},
	rights       = {http://creativecommons.org/licenses/by/3.0/},
	abstract     = {With the rapid advancement of {UAV} technology, the utilization of multi-{UAV} cooperative operations has become increasingly prevalent in various domains, including military and civilian applications. However, achieving efficient coordinated rounding-up of multiple {UAVs} remains a challenging problem. This paper addresses the issue of collaborative drone hunting by proposing a decision-making control model based on deep reinforcement learning. Additionally, a shared experience data pool is established to facilitate communication between drones. Each drone possesses independent decision-making and control capabilities while also considering the presence of other drones in the environment to collaboratively accomplish obstacle avoidance and rounding-up tasks. Furthermore, we redefine and design the reward function of reinforcement learning to achieve precise control of drone swarms in diverse environments. Simulation experiments demonstrate the feasibility of the proposed method, showcasing its successful completion of obstacle avoidance, tracking, and rounding-up tasks in an obstacle environment.},
	journaltitle = {Drones},
	date         = {2024-09},
	langid       = {english},
	file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/YR38DQEG/Zhao et al. - 2024 - Deep Reinforcement Learning-Driven Collaborative Rounding-Up for Multiple Unmanned Aerial Vehicles i.pdf:application/pdf}
}
@article{xing_multi-task_2024,
	title        = {Multi-Task Reinforcement Learning for Quadrotors},
	author       = {Xing, Jiaxu and Geles, Ismail and Song, Yunlong and Aljalbout, Elie and Scaramuzza, Davide},
	pages        = {1--8},
	doi          = {10.1109/LRA.2024.3520894},
	issn         = {2377-3766},
	abstract     = {Reinforcement learning ({RL}) has shown great effectiveness in quadrotor control, enabling specialized policies to develop even human-champion-level performance in single-task scenarios. However, these specialized policies often struggle with novel tasks, requiring a complete retraining of the policy from scratch. To address this limitation, this paper presents a novel multi-task reinforcement learning ({MTRL}) framework tailored for quadrotor control, leveraging the shared physical dynamics of the platform to enhance sample efficiency and task performance. By employing a multi-critic architecture and shared task encoders, our framework facilitates knowledge transfer across tasks, enabling a single policy to execute diverse maneuvers, including high-speed stabilization, velocity tracking, and autonomous racing. Our experimental results, validated both in simulation and real-world scenarios, demonstrate that our framework outperforms baseline approaches in terms of sample efficiency and overall task performance. Video: https://youtu.be/{HfK}9UT1OVnY.},
	journaltitle = {{IEEE} Robotics and Automation Letters},
	date         = 2024,
	file         = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/HWLYWRMH/10812062.html:text/html}
}
@misc{xie_multi-uav_2024,
	title        = {Multi-{UAV} Behavior-based Formation with Static and Dynamic Obstacles Avoidance via Reinforcement Learning},
	author       = {Xie, Yuqing and Yu, Chao and Zang, Hongzhi and Gao, Feng and Tang, Wenhao and Huang, Jingyi and Chen, Jiayu and Xu, Botian and Wu, Yi and Wang, Yu},
	doi          = {10.48550/arXiv.2410.18495},
	abstract     = {Formation control of multiple Unmanned Aerial Vehicles ({UAVs}) is vital for practical applications. This paper tackles the task of behavior-based {UAV} formation while avoiding static and dynamic obstacles during directed flight. We present a two-stage reinforcement learning ({RL}) training pipeline to tackle the challenge of multi-objective optimization, large exploration spaces, and the sim-to-real gap. The first stage searches in a simplified scenario for a linear utility function that balances all task objectives simultaneously, whereas the second stage applies the utility function in complex scenarios, utilizing curriculum learning to navigate large exploration spaces. Additionally, we apply an attention-based observation encoder to enhance formation maintenance and manage varying obstacle quantity. Experiments in simulation and real world demonstrate that our method outperforms planning-based and {RL}-based baselines regarding collision-free rate and formation maintenance in scenarios with static, dynamic, and mixed obstacles.},
	date         = {2024-10-24},
	file         = {Preprint PDF:/Users/viktorlorentz/Zotero/storage/RN8I9ND7/Xie et al. - 2024 - Multi-UAV Behavior-based Formation with Static and Dynamic Obstacles Avoidance via Reinforcement Lea.pdf:application/pdf;Snapshot:/Users/viktorlorentz/Zotero/storage/ABKF6L2T/2410.html:text/html}
}
@misc{diao_efficient_2024,
	title        = {Efficient Multi-agent Navigation with Lightweight {DRL} Policy},
	author       = {Diao, Xingrong and Wang, Jiankun},
	doi          = {10.48550/arXiv.2408.16370},
	abstract     = {In this article, we present an end-to-end collision avoidance policy based on deep reinforcement learning ({DRL}) for multi-agent systems, demonstrating encouraging outcomes in real-world applications. In particular, our policy calculates the control commands of the agent based on the raw {LiDAR} observation. In addition, the number of parameters of the proposed basic model is 140,000, and the size of the parameter file is 3.5 {MB}, which allows the robot to calculate the actions from the {CPU} alone. We propose a multi-agent training platform based on a physics-based simulator to further bridge the gap between simulation and the real world. The policy is trained on a policy-gradients-based {RL} algorithm in a dense and messy training environment. A novel reward function is introduced to address the issue of agents choosing suboptimal actions in some common scenarios. Although the data used for training is exclusively from the simulation platform, the policy can be successfully transferred and deployed in real-world robots. Finally, our policy effectively responds to intentional obstructions and avoids collisions. The website is available at {\textbackslash}url\{https://sites.google.com/view/xingrong2024efficient/\%E9\%A6\%96\%E9\%A1\%B5\}.},
	date         = {2024-09-04},
	file         = {Preprint PDF:/Users/viktorlorentz/Zotero/storage/RF974KYN/Diao and Wang - 2024 - Efficient Multi-agent Navigation with Lightweight DRL Policy.pdf:application/pdf;Snapshot:/Users/viktorlorentz/Zotero/storage/8PE25PVB/2408.html:text/html}
}
@article{hua_new_2022,
	title        = {A New Nonlinear Control Strategy Embedded With Reinforcement Learning for a Multirotor Transporting a Suspended Payload},
	author       = {Hua, Hean and Fang, Yongchun and Zhang, Xuetao and Qian, Chen},
	volume       = 27,
	pages        = {1174--1184},
	doi          = {10.1109/TMECH.2021.3082897},
	issn         = {1941-014X},
	abstract     = {In this article, a reinforcement learning ({RL})-based controller is proposed for a multirotor-based transportation system, guaranteeing that the trained {RL} controller is effective in both simulation and practical experiments. The main novelty lies in that, as far as we know, this is the first attempt of combining the advantages of nonlinear and intelligent control techniques to derive a practice-oriented {RL} controller for the multirotor-based transportation system, where the high-dimensional complicated dynamics are fully considered in the framework. Specifically, inspired by the physical insight of the system, a new nonlinear control approach is proposed, in which the underactuated properties and the nontrivial couplings are well handled. On this basis, an {RL} network is proposed to parameterize the nonlinear controller, where the obtained algorithm presents the features of strong reliability and fast convergence even in complicated working conditions (e.g., model uncertainties, parameters drift, external disturbances, and so on). Subsequently, the states are proven to asymptotically converge to the equilibrium point by Lyapunov analysis and {RL} techniques. A series of simulation and real world experiments are implemented to verify satisfactory positioning accuracy and robustness of the proposed algorithm.},
	journaltitle = {{IEEE}/{ASME} Transactions on Mechatronics},
	date         = {2022-04},
	file         = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/5RXNKV7S/9439850.html:text/html}
}
@article{kalenberg_stargate_2024,
	title        = {Stargate: Multimodal Sensor Fusion for Autonomous Navigation on Miniaturized {UAVs}},
	shorttitle   = {Stargate},
	author       = {Kalenberg, Konstantin and Müller, Hanna and Polonelli, Tommaso and Schiaffino, Alberto and Niculescu, Vlad and Cioflan, Cristian and Magno, Michele and Benini, Luca},
	volume       = 11,
	pages        = {21372--21390},
	doi          = {10.1109/JIOT.2024.3363036},
	issn         = {2327-4662},
	abstract     = {Autonomously navigating robots need to perceive and interpret their surroundings. Currently, cameras are among the most used sensors due to their high resolution and frame rates at relatively low-energy consumption and cost. In recent years, cutting-edge sensors, such as miniaturized depth cameras, have demonstrated strong potential, specifically for nano-size unmanned aerial vehicles ({UAVs}), where low-power consumption, lightweight hardware, and low-computational demand are essential. However, cameras are limited to working under good lighting conditions, while depth cameras have a limited range. To maximize robustness, we propose to fuse a millimeter form factor 64 pixel depth sensor and a low-resolution grayscale camera. In this work, a nano-{UAV} learns to detect and fly through a gate with a lightweight autonomous navigation system based on two {tinyML} convolutional neural network models trained in simulation, running entirely onboard in 7.6 ms and with an accuracy above 91\%. Field tests are based on the Crazyflie 2.1, featuring a total mass of 39 g. We demonstrate the robustness and potential of our navigation policy in multiple application scenarios, with a failure probability down to 1.2{\textbackslash},{\textbackslash},{\textbackslash}cdot {\textbackslash},{\textbackslash},10{\textbackslash},{\textbackslash},{\textasciicircum}{\textbackslash}mathrm -3 crash/meter, experiencing only two crashes on a cumulative flight distance of 1.7 km.},
	journaltitle = {{IEEE} Internet of Things Journal},
	date         = {2024-06},
	file         = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/J2A53CHW/10423569.html:text/html}
}
@article{kalenberg_stargate_2024-1,
	title        = {Stargate: Multimodal Sensor Fusion for Autonomous Navigation on Miniaturized {UAVs}},
	shorttitle   = {Stargate},
	author       = {Kalenberg, Konstantin and Müller, Hanna and Polonelli, Tommaso and Schiaffino, Alberto and Niculescu, Vlad and Cioflan, Cristian and Magno, Michele and Benini, Luca},
	volume       = 11,
	pages        = {21372--21390},
	doi          = {10.1109/JIOT.2024.3363036},
	issn         = {2327-4662},
	abstract     = {Autonomously navigating robots need to perceive and interpret their surroundings. Currently, cameras are among the most used sensors due to their high resolution and frame rates at relatively low-energy consumption and cost. In recent years, cutting-edge sensors, such as miniaturized depth cameras, have demonstrated strong potential, specifically for nano-size unmanned aerial vehicles ({UAVs}), where low-power consumption, lightweight hardware, and low-computational demand are essential. However, cameras are limited to working under good lighting conditions, while depth cameras have a limited range. To maximize robustness, we propose to fuse a millimeter form factor 64 pixel depth sensor and a low-resolution grayscale camera. In this work, a nano-{UAV} learns to detect and fly through a gate with a lightweight autonomous navigation system based on two {tinyML} convolutional neural network models trained in simulation, running entirely onboard in 7.6 ms and with an accuracy above 91\%. Field tests are based on the Crazyflie 2.1, featuring a total mass of 39 g. We demonstrate the robustness and potential of our navigation policy in multiple application scenarios, with a failure probability down to 1.2{\textbackslash},{\textbackslash},{\textbackslash}cdot {\textbackslash},{\textbackslash},10{\textbackslash},{\textbackslash},{\textasciicircum}{\textbackslash}mathrm -3 crash/meter, experiencing only two crashes on a cumulative flight distance of 1.7 km.},
	journaltitle = {{IEEE} Internet of Things Journal},
	date         = {2024-06}
}
@misc{schulman_proximal_2017,
	title        = {Proximal Policy Optimization Algorithms},
	author       = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	doi          = {10.48550/arXiv.1707.06347},
	abstract     = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization ({PPO}), have some of the benefits of trust region policy optimization ({TRPO}), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test {PPO} on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that {PPO} outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	date         = {2017-08-28},
	file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/GDVTGNBM/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:application/pdf;Snapshot:/Users/viktorlorentz/Zotero/storage/5GHXIDIW/1707.html:text/html}
}
@misc{chen_what_2024,
	title        = {What Matters in Learning A Zero-Shot Sim-to-Real {RL} Policy for Quadrotor Control? A Comprehensive Study},
	shorttitle   = {What Matters in Learning A Zero-Shot Sim-to-Real {RL} Policy for Quadrotor Control?},
	author       = {Chen, Jiayu and Yu, Chao and Xie, Yuqing and Gao, Feng and Chen, Yinuo and Yu, Shu'ang and Tang, Wenhao and Ji, Shilong and Mu, Mo and Wu, Yi and Yang, Huazhong and Wang, Yu},
	doi          = {10.48550/arXiv.2412.11764},
	abstract     = {Executing precise and agile flight maneuvers is critical for quadrotors in various applications. Traditional quadrotor control approaches are limited by their reliance on flat trajectories or time-consuming optimization, which restricts their flexibility. Recently, {RL}-based policy has emerged as a promising alternative due to its ability to directly map observations to actions, reducing the need for detailed system knowledge and actuation constraints. However, a significant challenge remains in bridging the sim-to-real gap, where {RL}-based policies often experience instability when deployed in real world. In this paper, we investigate key factors for learning robust {RL}-based control policies that are capable of zero-shot deployment in real-world quadrotors. We identify five critical factors and we develop a {PPO}-based training framework named {SimpleFlight}, which integrates these five techniques. We validate the efficacy of {SimpleFlight} on Crazyflie quadrotor, demonstrating that it achieves more than a 50\% reduction in trajectory tracking error compared to state-of-the-art {RL} baselines. The policy derived by {SimpleFlight} consistently excels across both smooth polynominal trajectories and challenging infeasible zigzag trajectories on small thrust-to-weight quadrotors. In contrast, baseline methods struggle with high-speed or infeasible trajectories. To support further research and reproducibility, we integrate {SimpleFlight} into a {GPU}-based simulator Omnidrones and provide open-source access to the code and model checkpoints. We hope {SimpleFlight} will offer valuable insights for advancing {RL}-based quadrotor control. For more details, visit our project website at https://sites.google.com/view/simpleflight/.},
	date         = {2024-12-23},
	file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/5D3UGIUI/Chen et al. - 2024 - What Matters in Learning A Zero-Shot Sim-to-Real RL Policy for Quadrotor Control A Comprehensive St.pdf:application/pdf;Snapshot:/Users/viktorlorentz/Zotero/storage/2GCH9GQY/2412.html:text/html}
}
@inproceedings{kaufmann_benchmark_2022,
	title        = {A Benchmark Comparison of Learned Control Policies for Agile Quadrotor Flight},
	author       = {Kaufmann, Elia and Bauersfeld, Leonard and Scaramuzza, Davide},
	booktitle    = {2022 International Conference on Robotics and Automation ({ICRA})},
	pages        = {10504--10510},
	doi          = {10.1109/ICRA46639.2022.9811564},
	abstract     = {Quadrotors are highly nonlinear dynamical systems that require carefully tuned controllers to be pushed to their physical limits. Recently, learning-based control policies have been proposed for quadrotors, as they would potentially allow learning direct mappings from high-dimensional raw sensory observations to actions. Due to sample inefficiency, training such learned controllers on the real platform is impractical or even impossible. Training in simulation is attractive but requires to transfer policies between domains, which demands trained policies to be robust to such domain gap. In this work, we make two contributions: (i) we perform the first benchmark comparison of existing learned control policies for agile quadrotor flight and show that training a control policy that commands body-rates and thrust results in more robust sim-to-real transfer compared to a policy that directly specifies individual rotor thrusts, (ii) we demonstrate for the first time that such a control policy trained via deep reinforcement learning can control a quadrotor in real-world experiments at speeds over 45 km/h.},
	eventtitle   = {2022 International Conference on Robotics and Automation ({ICRA})},
	date         = {2022-05},
	file         = {Accepted Version:/Users/viktorlorentz/Zotero/storage/KJH9PQJ7/Kaufmann et al. - 2022 - A Benchmark Comparison of Learned Control Policies for Agile Quadrotor Flight.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/BGUIK98J/9811564.html:text/html}
}
@misc{pandit_learning_2024,
	title        = {Learning Decentralized Multi-Biped Control for Payload Transport},
	author       = {Pandit, Bikram and Gupta, Ashutosh and Gadde, Mohitvishnu S. and Johnson, Addison and Shrestha, Aayam Kumar and Duan, Helei and Dao, Jeremy and Fern, Alan},
	doi          = {10.48550/arXiv.2406.17279},
	abstract     = {Payload transport over flat terrain via multi-wheel robot carriers is well-understood, highly effective, and configurable. In this paper, our goal is to provide similar effectiveness and configurability for transport over rough terrain that is more suitable for legs rather than wheels. For this purpose, we consider multi-biped robot carriers, where wheels are replaced by multiple bipedal robots attached to the carrier. Our main contribution is to design a decentralized controller for such systems that can be effectively applied to varying numbers and configurations of rigidly attached bipedal robots without retraining. We present a reinforcement learning approach for training the controller in simulation that supports transfer to the real world. Our experiments in simulation provide quantitative metrics showing the effectiveness of the approach over a wide variety of simulated transport scenarios. In addition, we demonstrate the controller in the real-world for systems composed of two and three Cassie robots. To our knowledge, this is the first example of a scalable multi-biped payload transport system.},
	date         = {2024-06-25},
	file         = {Preprint PDF:/Users/viktorlorentz/Zotero/storage/C9GDS2DE/Pandit et al. - 2024 - Learning Decentralized Multi-Biped Control for Payload Transport.pdf:application/pdf;Snapshot:/Users/viktorlorentz/Zotero/storage/46KPNX5E/2406.html:text/html}
}
@article{zhou_fast_2017,
	title        = {Fast, On-line Collision Avoidance for Dynamic Vehicles Using Buffered Voronoi Cells},
	author       = {Zhou, Dingjiang and Wang, Zijian and Bandyopadhyay, Saptarshi and Schwager, Mac},
	volume       = 2,
	pages        = {1047--1054},
	doi          = {10.1109/LRA.2017.2656241},
	issn         = {2377-3766},
	abstract     = {This letter presents a distributed collision avoidance algorithm for multiple dynamic vehicles moving in arbitrary dimensions. In our algorithm, each robot continually computes its buffered Voronoi cell ({BVC}) and plans its path within the {BVC} in a receding horizon fashion. We prove that our algorithm guarantees collision avoidance for robots with single integrator dynamics. We show that our algorithm has computational complexity of O(k), which is the same as that of the optimal reciprocal collision avoidance ({ORCA}) algorithm, and is considerably faster than model predictive control ({MPC}) and sequential convex programming ({SCP}) based approaches. Moreover, {ORCA} and {MPC}-{SCP} require relative position, velocity, and even other information, to be exchanged over a communication network among the robots. Our algorithm only requires the sensed relative position, and therefore is well suited for on-line implementation as it does not require a communication network, and it works well with noisy relative position sensors. Furthermore, we provide an extension of our algorithm to robots with higher-order dynamics like quadrotors. We demonstrate the capabilities of our algorithm by comparing it to {ORCA} in multiple benchmark simulation scenarios, and we present results of over 70 experimental trials using five quadrotors in a motion capture environment.},
	journaltitle = {{IEEE} Robotics and Automation Letters},
	date         = {2017-04},
	file         = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/2AYTWVN3/7828016.html:text/html}
}
@inproceedings{senbaslar_asynchronous_2022,
	title        = {Asynchronous Real-time Decentralized Multi-Robot Trajectory Planning},
	author       = {Şenbaşlar, Baskin and Sukhatme, Gaurav S.},
	booktitle    = {2022 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	pages        = {9972--9979},
	doi          = {10.1109/IROS47612.2022.9981760},
	abstract     = {We present a novel overconstraining and constraint-discarding method for asynchronous, real-time, decentralized, multi-robot trajectory planning that ensures collision avoidance. Our approach utilizes communication between robots. The communication medium is best-effort: messages may be dropped, re-ordered or delayed. Robots conservatively constrain themselves against others assuming they may be working with outdated information, and discard constraints when they receive update messages from others. Our method can augment existing synchronized decentralized receding horizon planning algorithms that utilize separating hyperplanes for collision avoidance thereby making them applicable to asynchronous setups. As an example, we extend an existing model predictive control based, synchronized, decentralized multi-robot planner using our method. We show our method's effectiveness under asynchronous planning and imperfect communication by comparing our extension to the base version. Our extension does not result in any collisions or synchronization-induced deadlocks to which the base version is prone.},
	eventtitle   = {2022 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	date         = {2022-10},
	file         = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/AT738Q8Q/9981760.html:text/html}
}
@inproceedings{batra_decentralized_2022,
	title        = {Decentralized Control of Quadrotor Swarms with End-to-end Deep Reinforcement Learning},
	author       = {Batra, Sumeet and Huang, Zhehui and Petrenko, Aleksei and Kumar, Tushar and Molchanov, Artem and Sukhatme, Gaurav S.},
	booktitle    = {Proceedings of the 5th Conference on Robot Learning},
	pages        = {576--586},
	abstract     = {We demonstrate the possibility of learning drone swarm controllers that are zero-shot transferable to real quadrotors via large-scale multi-agent end-to-end reinforcement learning. We train policies parameterized by neural networks that are capable of controlling individual drones in a swarm in a fully decentralized manner. Our policies, trained in simulated environments with realistic quadrotor physics, demonstrate advanced flocking behaviors, perform aggressive maneuvers in tight formations while avoiding collisions with each other, break and re-establish formations to avoid collisions with moving obstacles, and efficiently coordinate in pursuit-evasion tasks. We analyze, in simulation, how different model architectures and parameters of the training regime influence the final performance of neural swarms. We demonstrate the successful deployment of the model learned in simulation to highly resource-constrained physical quadrotors performing station keeping and goal swapping behaviors. Video demonstrations and source code are available at the project website https://sites.google.com/view/swarm-rl.},
	eventtitle   = {Conference on Robot Learning},
	date         = {2022-01-11},
	langid       = {english},
	file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/D9K6Q2FE/Batra et al. - 2022 - Decentralized Control of Quadrotor Swarms with End-to-end Deep Reinforcement Learning.pdf:application/pdf}
}
@inproceedings{cui_learning_2022,
	title        = {Learning Observation-Based Certifiable Safe Policy for Decentralized Multi-Robot Navigation},
	author       = {Cui, Yuxiang and Lin, Longzhong and Huang, Xiaolong and Zhang, Dongkun and Wang, Yunkai and Jing, Wei and Chen, Junbo and Xiong, Rong and Wang, Yue},
	booktitle    = {2022 International Conference on Robotics and Automation ({ICRA})},
	pages        = {5518--5524},
	doi          = {10.1109/ICRA46639.2022.9811950},
	abstract     = {Safety is of great importance in multi-robot navigation problems. In this paper, we propose a control barrier function ({CBF}) based optimizer that ensures robot safety with both high probability and flexibility, using only sensor measurement. The optimizer takes action commands from the policy network as initial values and provides refinement to drive the potentially dangerous ones back into safe regions. With the help of a deep world model that predicts the evolution of surrounding dynamics and the consequences of different actions, the {CBF} module can guide the optimization within a reasonable time horizon. We also present a novel joint training framework that improves the cooperation between the Reinforcement Learning ({RL}) based policy and the {CBF}-based optimizer by utilizing reward feedback from the {CBF} module. We observe that our policy can achieve a higher success rate while maintaining the safety of multiple robots in significantly fewer episodes. Experiments are conducted in multiple scenarios both in simulation and the real world, the results demonstrate the effectiveness of our method in maintaining the safety of multiple robots. Code is available at https://github.com/{YuxiangCui}/{MARL}-{OCBF}.},
	eventtitle   = {2022 International Conference on Robotics and Automation ({ICRA})},
	date         = {2022-05},
	file         = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/CQ2FM379/9811950.html:text/html;Submitted Version:/Users/viktorlorentz/Zotero/storage/KPDW2HWM/Cui et al. - 2022 - Learning Observation-Based Certifiable Safe Policy for Decentralized Multi-Robot Navigation.pdf:application/pdf}
}
@misc{eschmann_learning_2024-1,
	title        = {Learning to Fly in Seconds},
	author       = {Eschmann, Jonas and Albani, Dario and Loianno, Giuseppe},
	doi          = {10.48550/arXiv.2311.13081},
	abstract     = {Learning-based methods, particularly Reinforcement Learning ({RL}), hold great promise for streamlining deployment, enhancing performance, and achieving generalization in the control of autonomous multirotor aerial vehicles. Deep {RL} has been able to control complex systems with impressive fidelity and agility in simulation but the simulation-to-reality transfer often brings a hard-to-bridge reality gap. Moreover, {RL} is commonly plagued by prohibitively long training times. In this work, we propose a novel asymmetric actor-critic-based architecture coupled with a highly reliable {RL}-based training paradigm for end-to-end quadrotor control. We show how curriculum learning and a highly optimized simulator enhance sample complexity and lead to fast training times. To precisely discuss the challenges related to low-level/end-to-end multirotor control, we also introduce a taxonomy that classifies the existing levels of control abstractions as well as non-linearities and domain parameters. Our framework enables Simulation-to-Reality (Sim2Real) transfer for direct {RPM} control after only 18 seconds of training on a consumer-grade laptop as well as its deployment on microcontrollers to control a multirotor under real-time guarantees. Finally, our solution exhibits competitive performance in trajectory tracking, as demonstrated through various experimental comparisons with existing state-of-the-art control solutions using a real Crazyflie nano quadrotor. We open source the code including a very fast multirotor dynamics simulator that can simulate about 5 months of flight per second on a laptop {GPU}. The fast training times and deployment to a cheap, off-the-shelf quadrotor lower the barriers to entry and help democratize the research and development of these systems.},
	date         = {2024-04-08},
	file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/3AEZK4TC/Eschmann et al. - 2024 - Learning to Fly in Seconds.pdf:application/pdf;Snapshot:/Users/viktorlorentz/Zotero/storage/LUAWWL42/2311.html:text/html}
}
@inproceedings{seyde_is_2021,
	title        = {Is Bang-Bang Control All You Need? Solving Continuous Control with Bernoulli Policies},
	shorttitle   = {Is Bang-Bang Control All You Need?},
	author       = {Seyde, Tim and Gilitschenski, Igor and Schwarting, Wilko and Stellato, Bartolomeo and Riedmiller, Martin and Wulfmeier, Markus and Rus, Daniela},
	booktitle    = {Advances in Neural Information Processing Systems},
	volume       = 34,
	pages        = {27209--27221},
	abstract     = {Reinforcement learning ({RL}) for continuous control typically employs distributions whose support covers the entire action space. In this work, we investigate the colloquially known phenomenon that trained agents often prefer actions at the boundaries of that space. We draw theoretical connections to the emergence of bang-bang behavior in optimal control, and provide extensive empirical evaluation across a variety of recent {RL} algorithms. We replace the normal Gaussian by a Bernoulli distribution that solely considers the extremes along each action dimension - a bang-bang controller. Surprisingly, this achieves state-of-the-art performance on several continuous control benchmarks - in contrast to robotic hardware, where energy and maintenance cost affect controller choices. Since exploration, learning, and the final solution are entangled in {RL}, we provide additional imitation learning experiments to reduce the impact of exploration on our analysis. Finally, we show that our observations generalize to environments that aim to model real-world challenges and evaluate factors to mitigate the emergence of bang-bang solutions. Our findings emphasise challenges for benchmarking continuous control algorithms, particularly in light of potential real-world applications.},
	date         = 2021,
	file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/FHK6XKL8/Seyde et al. - 2021 - Is Bang-Bang Control All You Need Solving Continuous Control with Bernoulli Policies.pdf:application/pdf}
}
@misc{bohez_value_2019,
	title        = {Value constrained model-free continuous control},
	author       = {Bohez, Steven and Abdolmaleki, Abbas and Neunert, Michael and Buchli, Jonas and Heess, Nicolas and Hadsell, Raia},
	doi          = {10.48550/arXiv.1902.04623},
	abstract     = {The naive application of Reinforcement Learning algorithms to continuous control problems -- such as locomotion and manipulation -- often results in policies which rely on high-amplitude, high-frequency control signals, known colloquially as bang-bang control. Although such solutions may indeed maximize task reward, they can be unsuitable for real world systems. Bang-bang control may lead to increased wear and tear or energy consumption, and tends to excite undesired second-order dynamics. To counteract this issue, multi-objective optimization can be used to simultaneously optimize both the reward and some auxiliary cost that discourages undesired (e.g. high-amplitude) control. In principle, such an approach can yield the sought after, smooth, control policies. It can, however, be hard to find the correct trade-off between cost and return that results in the desired behavior. In this paper we propose a new constraint-based reinforcement learning approach that ensures task success while minimizing one or more auxiliary costs (such as control effort). We employ Lagrangian relaxation to learn both (a) the parameters of a control policy that satisfies the desired constraints and (b) the Lagrangian multipliers for the optimization. Moreover, we demonstrate that we can satisfy constraints either in expectation or in a per-step fashion, and can even learn a single policy that is able to dynamically trade-off between return and cost. We demonstrate the efficacy of our approach using a number of continuous control benchmark tasks, a realistic, energy-optimized quadruped locomotion task, as well as a reaching task on a real robot arm.},
	date         = {2019-02-12},
	file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/JV483Y6E/Bohez et al. - 2019 - Value constrained model-free continuous control.pdf:application/pdf;Snapshot:/Users/viktorlorentz/Zotero/storage/M5UXWYKL/1902.html:text/html}
}
@inproceedings{gronauer_using_2022,
	title        = {Using Simulation Optimization to Improve Zero-shot Policy Transfer of Quadrotors},
	author       = {Gronauer, Sven and Kissel, Matthias and Sacchetto, Luca and Korte, Mathias and Diepold, Klaus},
	booktitle    = {2022 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	pages        = {10170--10176},
	doi          = {10.1109/IROS47612.2022.9981229},
	abstract     = {In this work, we propose a data-driven approach to optimize the parameters of a simulation such that control policies can be directly transferred from simulation to a real-world quadrotor. Our neural network-based policies take only onboard sensor data as input and run entirely on the embed-ded hardware. In real-world experiments, we compare low-level Pulse-Width Modulated control with higher-level control structures such as Attitude Rate and Attitude, which utilize Proportional-Integral-Derivative controllers to output motor commands. Our experiments show that low-level controllers trained with Reinforcement Learning require a more accurate simulation than higher-level control policies at the expense of being less robust towards parameter uncertainties.},
	eventtitle   = {2022 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	date         = {2022-10},
	file         = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/XTCQ8T9D/9981229.html:text/html;Submitted Version:/Users/viktorlorentz/Zotero/storage/9NVQPD6L/Gronauer et al. - 2022 - Using Simulation Optimization to Improve Zero-shot Policy Transfer of Quadrotors.pdf:application/pdf}
}
@article{pi_robust_2021,
	title        = {Robust Quadrotor Control through Reinforcement Learning with Disturbance Compensation},
	author       = {Pi, Chen-Huan and Ye, Wei-Yuan and Cheng, Stone},
	volume       = 11,
	pages        = 3257,
	doi          = {10.3390/app11073257},
	issn         = {2076-3417},
	rights       = {http://creativecommons.org/licenses/by/3.0/},
	abstract     = {In this paper, a novel control strategy is presented for reinforcement learning with disturbance compensation to solve the problem of quadrotor positioning under external disturbance. The proposed control scheme applies a trained neural-network-based reinforcement learning agent to control the quadrotor, and its output is directly mapped to four actuators in an end-to-end manner. The proposed control scheme constructs a disturbance observer to estimate the external forces exerted on the three axes of the quadrotor, such as wind gusts in an outdoor environment. By introducing an interference compensator into the neural network control agent, the tracking accuracy and robustness were significantly increased in indoor and outdoor experiments. The experimental results indicate that the proposed control strategy is highly robust to external disturbances. In the experiments, compensation improved control accuracy and reduced positioning error by 75\%. To the best of our knowledge, this study is the first to achieve quadrotor positioning control through low-level reinforcement learning by using a global positioning system in an outdoor environment.},
	journaltitle = {Applied Sciences},
	date         = {2021-01},
	langid       = {english},
	file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/TNHT9649/Pi et al. - 2021 - Robust Quadrotor Control through Reinforcement Learning with Disturbance Compensation.pdf:application/pdf}
}
@inproceedings{kaufmann_benchmark_2022-1,
	title        = {A Benchmark Comparison of Learned Control Policies for Agile Quadrotor Flight},
	author       = {Kaufmann, Elia and Bauersfeld, Leonard and Scaramuzza, Davide},
	booktitle    = {2022 International Conference on Robotics and Automation ({ICRA})},
	pages        = {10504--10510},
	doi          = {10.1109/ICRA46639.2022.9811564},
	abstract     = {Quadrotors are highly nonlinear dynamical systems that require carefully tuned controllers to be pushed to their physical limits. Recently, learning-based control policies have been proposed for quadrotors, as they would potentially allow learning direct mappings from high-dimensional raw sensory observations to actions. Due to sample inefficiency, training such learned controllers on the real platform is impractical or even impossible. Training in simulation is attractive but requires to transfer policies between domains, which demands trained policies to be robust to such domain gap. In this work, we make two contributions: (i) we perform the first benchmark comparison of existing learned control policies for agile quadrotor flight and show that training a control policy that commands body-rates and thrust results in more robust sim-to-real transfer compared to a policy that directly specifies individual rotor thrusts, (ii) we demonstrate for the first time that such a control policy trained via deep reinforcement learning can control a quadrotor in real-world experiments at speeds over 45 km/h.},
	eventtitle   = {2022 International Conference on Robotics and Automation ({ICRA})},
	date         = {2022-05},
	file         = {IEEE Xplore Abstract Record:/Users/viktorlorentz/Zotero/storage/SVLQRNUH/9811564.html:text/html;Submitted Version:/Users/viktorlorentz/Zotero/storage/8ZJ7N7NQ/Kaufmann et al. - 2022 - A Benchmark Comparison of Learned Control Policies for Agile Quadrotor Flight.pdf:application/pdf}
}
@misc{xing_bootstrapping_2024,
	title        = {Bootstrapping Reinforcement Learning with Imitation for Vision-Based Agile Flight},
	author       = {Xing, Jiaxu and Romero, Angel and Bauersfeld, Leonard and Scaramuzza, Davide},
	doi          = {10.48550/arXiv.2403.12203},
	abstract     = {Learning visuomotor policies for agile quadrotor flight presents significant difficulties, primarily from inefficient policy exploration caused by high-dimensional visual inputs and the need for precise and low-latency control. To address these challenges, we propose a novel approach that combines the performance of Reinforcement Learning ({RL}) and the sample efficiency of Imitation Learning ({IL}) in the task of vision-based autonomous drone racing. While {RL} provides a framework for learning high-performance controllers through trial and error, it faces challenges with sample efficiency and computational demands due to the high dimensionality of visual inputs. Conversely, {IL} efficiently learns from visual expert demonstrations, but it remains limited by the expert's performance and state distribution. To overcome these limitations, our policy learning framework integrates the strengths of both approaches. Our framework contains three phases: training a teacher policy using {RL} with privileged state information, distilling it into a student policy via {IL}, and adaptive fine-tuning via {RL}. Testing in both simulated and real-world scenarios shows our approach can not only learn in scenarios where {RL} from scratch fails but also outperforms existing {IL} methods in both robustness and performance, successfully navigating a quadrotor through a race course using only visual information. Videos of the experiments are available at https://rpg.ifi.uzh.ch/bootstrap-rl-with-il/index.html.},
	date         = {2024-11-12},
	file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/ZMRNV3GM/Xing et al. - 2024 - Bootstrapping Reinforcement Learning with Imitation for Vision-Based Agile Flight.pdf:application/pdf;Snapshot:/Users/viktorlorentz/Zotero/storage/6WX7V4MN/2403.html:text/html}
}
@misc{song_autonomous_2021,
	title        = {Autonomous Drone Racing with Deep Reinforcement Learning},
	author       = {Song, Yunlong and Steinweg, Mats and Kaufmann, Elia and Scaramuzza, Davide},
	doi          = {10.48550/arXiv.2103.08624},
	abstract     = {In many robotic tasks, such as autonomous drone racing, the goal is to travel through a set of waypoints as fast as possible. A key challenge for this task is planning the time-optimal trajectory, which is typically solved by assuming perfect knowledge of the waypoints to pass in advance. The resulting solution is either highly specialized for a single-track layout, or suboptimal due to simplifying assumptions about the platform dynamics. In this work, a new approach to near-time-optimal trajectory generation for quadrotors is presented. Leveraging deep reinforcement learning and relative gate observations, our approach can compute near-time-optimal trajectories and adapt the trajectory to environment changes. Our method exhibits computational advantages over approaches based on trajectory optimization for non-trivial track configurations. The proposed approach is evaluated on a set of race tracks in simulation and the real world, achieving speeds of up to 60 km/h with a physical quadrotor.},
	date         = {2021-08-02},
	file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/JN2P3EM2/Song et al. - 2021 - Autonomous Drone Racing with Deep Reinforcement Learning.pdf:application/pdf;Snapshot:/Users/viktorlorentz/Zotero/storage/XFHYZCYF/2103.html:text/html}
}
@misc{patterson_empirical_2024,
	title        = {Empirical Design in Reinforcement Learning},
	author       = {Patterson, Andrew and Neumann, Samuel and White, Martha and White, Adam},
	doi          = {10.48550/arXiv.2304.01315},
	abstract     = {Empirical design in reinforcement learning is no small task. Running good experiments requires attention to detail and at times significant computational resources. While compute resources available per dollar have continued to grow rapidly, so have the scale of typical experiments in reinforcement learning. It is now common to benchmark agents with millions of parameters against dozens of tasks, each using the equivalent of 30 days of experience. The scale of these experiments often conflict with the need for proper statistical evidence, especially when comparing algorithms. Recent studies have highlighted how popular algorithms are sensitive to hyper-parameter settings and implementation details, and that common empirical practice leads to weak statistical evidence (Machado et al., 2018; Henderson et al., 2018). Here we take this one step further. This manuscript represents both a call to action, and a comprehensive resource for how to do good experiments in reinforcement learning. In particular, we cover: the statistical assumptions underlying common performance measures, how to properly characterize performance variation and stability, hypothesis testing, special considerations for comparing multiple agents, baseline and illustrative example construction, and how to deal with hyper-parameters and experimenter bias. Throughout we highlight common mistakes found in the literature and the statistical consequences of those in example experiments. The objective of this document is to provide answers on how we can use our unprecedented compute to do good science in reinforcement learning, as well as stay alert to potential pitfalls in our empirical design.},
	date         = {2024-10-29},
	file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/GPVPIJ2Z/Patterson et al. - 2024 - Empirical Design in Reinforcement Learning.pdf:application/pdf;Snapshot:/Users/viktorlorentz/Zotero/storage/JLV37DCM/2304.html:text/html}
}
@misc{hu_flare_2024,
	title        = {{FLaRe}: Achieving Masterful and Adaptive Robot Policies with Large-Scale Reinforcement Learning Fine-Tuning},
	shorttitle   = {{FLaRe}},
	author       = {Hu, Jiaheng and Hendrix, Rose and Farhadi, Ali and Kembhavi, Aniruddha and Martin-Martin, Roberto and Stone, Peter and Zeng, Kuo-Hao and Ehsani, Kiana},
	doi          = {10.48550/arXiv.2409.16578},
	abstract     = {In recent years, the Robotics field has initiated several efforts toward building generalist robot policies through large-scale multi-task Behavior Cloning. However, direct deployments of these policies have led to unsatisfactory performance, where the policy struggles with unseen states and tasks. How can we break through the performance plateau of these models and elevate their capabilities to new heights? In this paper, we propose {FLaRe}, a large-scale Reinforcement Learning fine-tuning framework that integrates robust pre-trained representations, large-scale training, and gradient stabilization techniques. Our method aligns pre-trained policies towards task completion, achieving state-of-the-art ({SoTA}) performance both on previously demonstrated and on entirely novel tasks and embodiments. Specifically, on a set of long-horizon mobile manipulation tasks, {FLaRe} achieves an average success rate of 79.5\% in unseen environments, with absolute improvements of +23.6\% in simulation and +30.7\% on real robots over prior {SoTA} methods. By utilizing only sparse rewards, our approach can enable generalizing to new capabilities beyond the pretraining data with minimal human effort. Moreover, we demonstrate rapid adaptation to new embodiments and behaviors with less than a day of fine-tuning. Videos can be found on the project website at https://robot-flare.github.io/},
	date         = {2024-09-30},
	file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/FSBSMS43/Hu et al. - 2024 - FLaRe Achieving Masterful and Adaptive Robot Policies with Large-Scale Reinforcement Learning Fine-.pdf:application/pdf;Snapshot:/Users/viktorlorentz/Zotero/storage/TGHT3DH4/2409.html:text/html}
}
@article{loquercio_learning_2021,
	title        = {Learning High-Speed Flight in the Wild},
	author       = {Loquercio, Antonio and Kaufmann, Elia and Ranftl, René and Müller, Matthias and Koltun, Vladlen and Scaramuzza, Davide},
	volume       = 6,
	pages        = {eabg5810},
	doi          = {10.1126/scirobotics.abg5810},
	issn         = {2470-9476},
	abstract     = {Quadrotors are agile. Unlike most other machines, they can traverse extremely complex environments at high speeds. To date, only expert human pilots have been able to fully exploit their capabilities. Autonomous operation with on-board sensing and computation has been limited to low speeds. State-of-the-art methods generally separate the navigation problem into subtasks: sensing, mapping, and planning. While this approach has proven successful at low speeds, the separation it builds upon can be problematic for high-speed navigation in cluttered environments. Indeed, the subtasks are executed sequentially, leading to increased processing latency and a compounding of errors through the pipeline. Here we propose an end-to-end approach that can autonomously fly quadrotors through complex natural and man-made environments at high speeds, with purely onboard sensing and computation. The key principle is to directly map noisy sensory observations to collision-free trajectories in a receding-horizon fashion. This direct mapping drastically reduces processing latency and increases robustness to noisy and incomplete perception. The sensorimotor mapping is performed by a convolutional network that is trained exclusively in simulation via privileged learning: imitating an expert with access to privileged information. By simulating realistic sensor noise, our approach achieves zero-shot transfer from simulation to challenging real-world environments that were never experienced during training: dense forests, snow-covered terrain, derailed trains, and collapsed buildings. Our work demonstrates that end-to-end policies trained in simulation enable high-speed autonomous flight through challenging environments, outperforming traditional obstacle avoidance pipelines.},
	journaltitle = {Science Robotics},
	shortjournal = {Sci. Robot.},
	date         = {2021-10-13},
	file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/SEWE4QSJ/Loquercio et al. - 2021 - Learning High-Speed Flight in the Wild.pdf:application/pdf;Snapshot:/Users/viktorlorentz/Zotero/storage/S6XHK2QU/2110.html:text/html}
}
@article{kaufmann_champion-level_2023,
	title        = {Champion-level drone racing using deep reinforcement learning},
	author       = {Kaufmann, Elia and Bauersfeld, Leonard and Loquercio, Antonio and Müller, Matthias and Koltun, Vladlen and Scaramuzza, Davide},
	volume       = 620,
	pages        = {982--987},
	doi          = {10.1038/s41586-023-06419-4},
	issn         = {1476-4687},
	rights       = {2023 The Author(s)},
	abstract     = {First-person view ({FPV}) drone racing is a televised sport in which professional competitors pilot high-speed aircraft through a 3D circuit. Each pilot sees the environment from the perspective of their drone by means of video streamed from an onboard camera. Reaching the level of professional pilots with an autonomous drone is challenging because the robot needs to fly at its physical limits while estimating its speed and location in the circuit exclusively from onboard sensors1. Here we introduce Swift, an autonomous system that can race physical vehicles at the level of the human world champions. The system combines deep reinforcement learning ({RL}) in simulation with data collected in the physical world. Swift competed against three human champions, including the world champions of two international leagues, in real-world head-to-head races. Swift won several races against each of the human champions and demonstrated the fastest recorded race time. This work represents a milestone for mobile robotics and machine intelligence2, which may inspire the deployment of hybrid learning-based solutions in other physical systems.},
	journaltitle = {Nature},
	date         = {2023-08},
	langid       = {english},
	file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/BKZSL3KB/Kaufmann et al. - 2023 - Champion-level drone racing using deep reinforcement learning.pdf:application/pdf}
}
@misc{chen_what_2024-1,
	title        = {What Matters in Learning A Zero-Shot Sim-to-Real {RL} Policy for Quadrotor Control? A Comprehensive Study},
	shorttitle   = {What Matters in Learning A Zero-Shot Sim-to-Real {RL} Policy for Quadrotor Control?},
	author       = {Chen, Jiayu and Yu, Chao and Xie, Yuqing and Gao, Feng and Chen, Yinuo and Yu, Shu'ang and Tang, Wenhao and Ji, Shilong and Mu, Mo and Wu, Yi and Yang, Huazhong and Wang, Yu},
	doi          = {10.48550/arXiv.2412.11764},
	abstract     = {Executing precise and agile flight maneuvers is critical for quadrotors in various applications. Traditional quadrotor control approaches are limited by their reliance on flat trajectories or time-consuming optimization, which restricts their flexibility. Recently, {RL}-based policy has emerged as a promising alternative due to its ability to directly map observations to actions, reducing the need for detailed system knowledge and actuation constraints. However, a significant challenge remains in bridging the sim-to-real gap, where {RL}-based policies often experience instability when deployed in real world. In this paper, we investigate key factors for learning robust {RL}-based control policies that are capable of zero-shot deployment in real-world quadrotors. We identify five critical factors and we develop a {PPO}-based training framework named {SimpleFlight}, which integrates these five techniques. We validate the efficacy of {SimpleFlight} on Crazyflie quadrotor, demonstrating that it achieves more than a 50\% reduction in trajectory tracking error compared to state-of-the-art {RL} baselines. The policy derived by {SimpleFlight} consistently excels across both smooth polynominal trajectories and challenging infeasible zigzag trajectories on small thrust-to-weight quadrotors. In contrast, baseline methods struggle with high-speed or infeasible trajectories. To support further research and reproducibility, we integrate {SimpleFlight} into a {GPU}-based simulator Omnidrones and provide open-source access to the code and model checkpoints. We hope {SimpleFlight} will offer valuable insights for advancing {RL}-based quadrotor control. For more details, visit our project website at https://sites.google.com/view/simpleflight/.},
	date         = {2024-12-23},
	file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/34Z7XC55/Chen et al. - 2024 - What Matters in Learning A Zero-Shot Sim-to-Real RL Policy for Quadrotor Control A Comprehensive St.pdf:application/pdf;Snapshot:/Users/viktorlorentz/Zotero/storage/47WC34EZ/2412.html:text/html}
}
@article{felten_multi-objective_2024,
	title        = {Multi-Objective Reinforcement Learning},
	author       = {Felten, Florian},
	abstract     = {
		The recent surge in artificial intelligence ({AI}) agents assisting us in daily tasks suggests that these agents possess the ability to comprehend key aspects of our environment, thereby facilitating better decision-making. Presently, this understanding is predominantly acquired through data-driven learning methods. Notably, reinforcement learning ({RL}) stands out as a natural framework for agents to acquire behaviors by interacting with their environment and learning from feedback. However, despite the effectiveness of {RL} in training agents to optimize a single objective, such as minimizing cost or maximizing performance, it overlooks the inherent complexity of decision-making in real-world scenarios where multiple objectives may need to be considered simultaneously. Indeed, an essential aspect that remains understudied is the human tendency to make compromises in various situations, influenced by values, circumstances, or mood. This limitation underscores the need for advancements in {AI} methodologies to address the nuanced trade-offs inherent to human decision-making. Thus, this work aims to explore the extension of {RL} principles into multi-objective settings, where agents can learn behaviors that balance competing objectives, thereby enabling more adaptable and personalized {AI} systems.

		In the first part of this thesis, we explore the domain of multi-objective reinforcement learning ({MORL}), a recent technique aimed at enabling {AI} agents to acquire diverse behaviors associated with different trade-offs from multiple feedback signals. While {MORL} is relatively recent, works in this field often rely on existing knowledge coming from older fields such as multi-objective optimization ({MOO}) and {RL}. Our initial contribution involves a comprehensive analysis of the relationships between {RL}, {MOO}, and {MORL}. This examination culminates in the development of a taxonomy for categorizing {MORL} algorithms, drawing on concepts derived from preceding fields. Building upon this foundational understanding, we proceed to investigate the feasibility of leveraging techniques from {MOO} and {RL} to enhance {MORL} methodologies. This exploration yields several contributions. Among these, we introduce the utilization of metaheuristics to address the exploration-exploitation dilemma in {MORL}. Additionally, we introduce a versatile framework rooted in the derived taxonomy, facilitating the creation of novel {MORL} algorithms based on techniques coming from {MOO} and {RL}. Furthermore, our efforts extend towards improving the scientific rigor and practical applicability of {MORL} in real-world scenarios. To this end, we introduce methods and a suite of open-source tools that have become the standard in {MORL}.

		Many real-world situations also involve collaboration among multiple agents to accomplish tasks efficiently. Therefore, the second part of this thesis transitions to settings involving multiple agents, leading to the nascent field of multi-objective multi-agent reinforcement learning ({MOMARL}). In this domain, as an initial contribution, we release a comprehensive set of open-source utilities aimed to accelerate and establish a robust foundation for research within this evolving domain. Furthermore, we perform an initial study exploring the transferability of knowledge and methodologies from both {MORL} and multi-agent {RL} to the {MOMARL} settings.  Finally, we validate our approach in a real-world application. Specifically, we aim to automatically learn the coordination of multiple drones having different objectives, harnessing the {MOMARL} framework to orchestrate their actions effectively. This empirical validation serves as evidence of the viability and versatility of the proposed methodologies in addressing complex real-world challenges.
	},
	date         = {2024-06-25},
	file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/UXJ3WPZD/Felten - 2024 - Multi-Objective Reinforcement Learning.pdf:application/pdf}
}
@inproceedings{palunko_reinforcement_2013,
	title        = {A reinforcement learning approach towards autonomous suspended load manipulation using aerial robots},
	author       = {Palunko, Ivana and Faust, Aleksandra and Cruz, Patricio and Tapia, Lydia and Fierro, Rafael},
	booktitle    = {2013 {IEEE} International Conference on Robotics and Automation},
	location     = {Karlsruhe, Germany},
	pages        = {4896--4901},
	doi          = {10.1109/ICRA.2013.6631276},
	isbn         = {978-1-4673-5643-5 978-1-4673-5641-1},
	abstract     = {In this paper, we present a problem where a suspended load, carried by a rotorcraft aerial robot, performs trajectory tracking. We want to accomplish this by specifying the reference trajectory for the suspended load only. The aerial robot needs to discover/learn its own trajectory which ensures that the suspended load tracks the reference trajectory. As a solution, we propose a method based on least-square policy iteration ({LSPI}) which is a type of reinforcement learning algorithm. The proposed method is veriﬁed through simulation and experiments.},
	eventtitle   = {2013 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	date         = {2013-05},
	langid       = {english},
	file         = {PDF:/Users/viktorlorentz/Zotero/storage/NN46FIMF/Palunko et al. - 2013 - A reinforcement learning approach towards autonomous suspended load manipulation using aerial robots.pdf:application/pdf}
}
@online{noauthor_reinforcement_nodate,
	title        = {Reinforcement Learning Based Trajectory Planning for Multi-{UAV} Load Transportation {\textbar} {IEEE} Journals \& Magazine {\textbar} {IEEE} Xplore},
	file         = {Reinforcement Learning Based Trajectory Planning for Multi-UAV Load Transportation | IEEE Journals & Magazine | IEEE Xplore:/Users/viktorlorentz/Zotero/storage/DIZH33DL/10699338.html:text/html}
}
@online{weng_reward_2024,
	title        = {Reward Hacking in Reinforcement Learning},
	author       = {Weng, Lilian},
	abstract     = {Reward hacking occurs when a reinforcement learning ({RL}) agent exploits flaws or ambiguities in the reward function to achieve high rewards, without genuinely learning or completing the intended task. Reward hacking exists because {RL} environments are often imperfect, and it is fundamentally challenging to accurately specify a reward function. With the rise of language models generalizing to a broad spectrum of tasks and {RLHF} becomes a de facto method for alignment training, reward hacking in {RL} training of language models has become a critical practical challenge. Instances where the model learns to modify unit tests to pass coding tasks, or where responses contain biases that mimic a user’s preference, are pretty concerning and are likely one of the major blockers for real-world deployment of more autonomous use cases of {AI} models.},
	date         = {2024-11-28},
	langid       = {english},
	file         = {Snapshot:/Users/viktorlorentz/Zotero/storage/8ZDCN3DF/2024-11-28-reward-hacking.html:text/html}
}
@misc{pinto_asymmetric_2017,
	title        = {Asymmetric Actor Critic for Image-Based Robot Learning},
	author       = {Pinto, Lerrel and Andrychowicz, Marcin and Welinder, Peter and Zaremba, Wojciech and Abbeel, Pieter},
	doi          = {10.48550/arXiv.1710.06542},
	abstract     = {Deep reinforcement learning ({RL}) has proven a powerful technique in many sequential decision making domains. However, Robotics poses many challenges for {RL}, most notably training on a physical system can be expensive and dangerous, which has sparked significant interest in learning control policies using a physics simulator. While several recent works have shown promising results in transferring policies trained in simulation to the real world, they often do not fully utilize the advantage of working with a simulator. In this work, we exploit the full state observability in the simulator to train better policies which take as input only partial observations ({RGBD} images). We do this by employing an actor-critic training algorithm in which the critic is trained on full states while the actor (or policy) gets rendered images as input. We show experimentally on a range of simulated tasks that using these asymmetric inputs significantly improves performance. Finally, we combine this method with domain randomization and show real robot experiments for several tasks like picking, pushing, and moving a block. We achieve this simulation to real world transfer without training on any real world data.},
	date         = {2017-10-18},
	file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/LSS437F8/Pinto et al. - 2017 - Asymmetric Actor Critic for Image-Based Robot Learning.pdf:application/pdf;Snapshot:/Users/viktorlorentz/Zotero/storage/PUP2XEQW/1710.html:text/html}
}
@inproceedings{huang_collision_2024-1,
	title        = {Collision Avoidance and Navigation for a Quadrotor Swarm Using End-to-end Deep Reinforcement Learning},
	author       = {Huang, Zhehui and Yang, Zhaojing and Krupani, Rahul and Şenbaşlar, Baskın and Batra, Sumeet and Sukhatme, Gaurav S.},
	booktitle    = {2024 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	pages        = {300--306},
	doi          = {10.1109/ICRA57147.2024.10611499},
	abstract     = {End-to-end deep reinforcement learning ({DRL}) for quadrotor control promises many benefits – easy deployment, task generalization and real-time execution capability. Prior end-to-end {DRL}-based methods have showcased the ability to deploy learned controllers onto single quadrotors or quadrotor teams maneuvering in simple, obstacle-free environments. However, the addition of obstacles increases the number of possible interactions exponentially, thereby increasing the difficulty of training {RL} policies. In this work, we propose an end-to-end {DRL} approach to control quadrotor swarms in environments with obstacles. We provide our agents a curriculum and a replay buffer of the clipped collision episodes to improve performance in obstacle-rich environments. We implement an attention mechanism to attend to the neighbor robots and obstacle interactions - the first successful demonstration of this mechanism on policies for swarm behavior deployed on severely compute-constrained hardware. Our work is the first work that demonstrates the possibility of learning neighbor-avoiding and obstacle-avoiding control policies trained with end-to-end {DRL} that transfers zero-shot to real quadrotors. Our approach scales to 32 robots with 80\% obstacle density in simulation and 8 robots with 20\% obstacle density in physical deployment. Website: https://sites.google.com/view/obst-avoid-swarm-rl},
	eventtitle   = {2024 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	date         = {2024-05}
}
@misc{schulman_proximal_2017-1,
	title        = {Proximal Policy Optimization Algorithms},
	author       = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	doi          = {10.48550/arXiv.1707.06347},
	abstract     = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization ({PPO}), have some of the benefits of trust region policy optimization ({TRPO}), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test {PPO} on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that {PPO} outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	date         = {2017-08-28},
	file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/4YTV8TGY/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:application/pdf;Snapshot:/Users/viktorlorentz/Zotero/storage/ARVYL2VA/1707.html:text/html}
}
@article{sutton_reinforcement_nodate,
	title        = {Reinforcement Learning: An Introduction},
	author       = {Sutton, Richard S and Barto, Andrew G},
	langid       = {english},
	file         = {PDF:/Users/viktorlorentz/Zotero/storage/PDZZFGDV/Sutton and Barto - Reinforcement Learning An Introduction.pdf:application/pdf}
}
@online{noauthor_reaching_nodate,
	title        = {Reaching the limit in autonomous racing: Optimal control versus reinforcement learning},
	shorttitle   = {Reaching the limit in autonomous racing},
	doi          = {10.1126/scirobotics.adg1462},
	langid       = {english},
	file         = {Accepted Version:/Users/viktorlorentz/Zotero/storage/8ZEP5I6Q/Reaching the limit in autonomous racing Optimal control versus reinforcement learning.pdf:application/pdf;Snapshot:/Users/viktorlorentz/Zotero/storage/B7X6J6JS/scirobotics.html:text/html}
}
@article{bauersfeld_robotics_2025,
	title        = {Robotics Meets Fluid Dynamics: A Characterization of the Induced Airflow Below a Quadrotor as a Turbulent Jet},
	shorttitle   = {Robotics Meets Fluid Dynamics},
	author       = {Bauersfeld, Leonard and Muller, Koen and Ziegler, Dominic and Coletti, Filippo and Scaramuzza, Davide},
	volume       = 10,
	pages        = {1241--1248},
	doi          = {10.1109/LRA.2024.3518835},
	issn         = {2377-3766, 2377-3774},
	rights       = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	abstract     = {The widespread adoption of quadrotors for diverse applications, from agriculture to public safety, necessitates an understanding of the aerodynamic disturbances they create. This paper introduces a computationally lightweight model for estimating the time-averaged magnitude of the induced ﬂow below quadrotors in hover. Unlike related approaches that rely on expensive computational ﬂuid dynamics ({CFD}) simulations or drone speciﬁc time-consuming empirical measurements, our method leverages classical theory from turbulent ﬂows. By analyzing over 16 hours of ﬂight data from drones of varying sizes within a large motion capture system, we show for the ﬁrst time that the combined ﬂow from all drone propellers is well-approximated by a turbulent jet after 2.5 drone-diameters below the vehicle. Using a novel normalization and scaling, we experimentally identify model parameters that describe a uniﬁed mean velocity ﬁeld below differently sized quadrotors. The model, which requires only the drone’s mass, propeller size, and drone size for calculations, accurately describes the farﬁeld airﬂow over a long-range in a very large volume which is impractical to simulate using {CFD}. Our model offers a practical tool for ensuring safer operations near humans, optimizing sensor placements and drone control in multi-agent scenarios. We demonstrate the latter by designing a controller that compensates for the downwash of another drone, leading to a four times lower altitude deviation when passing below.},
	journaltitle = {{IEEE} Robotics and Automation Letters},
	shortjournal = {{IEEE} Robot. Autom. Lett.},
	date         = {2025-02},
	langid       = {english},
	file         = {PDF:/Users/viktorlorentz/Zotero/storage/QTJFKS2S/Bauersfeld et al. - 2025 - Robotics Meets Fluid Dynamics A Characterization of the Induced Airflow Below a Quadrotor as a Turb.pdf:application/pdf}
}
@inproceedings{gassner_dynamic_2017,
	title        = {Dynamic collaboration without communication: Vision-based cable-suspended load transport with two quadrotors},
	shorttitle   = {Dynamic collaboration without communication},
	author       = {Gassner, Michael and Cieslewski, Titus and Scaramuzza, Davide},
	booktitle    = {2017 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	location     = {Singapore, Singapore},
	pages        = {5196--5202},
	doi          = {10.1109/ICRA.2017.7989609},
	isbn         = {978-1-5090-4633-1},
	abstract     = {Transport of objects is a major application in robotics nowadays. While ground robots can carry heavy payloads for long distances, they are limited in rugged terrains. Aerial robots can deliver objects in arbitrary terrains; however they tend to be limited in payload. It has been previously shown that, for heavy payloads, it can be beneﬁcial to carry them using multiple ﬂying robots. In this paper, we propose a novel collaborative transport scheme, in which two quadrotors transport a cable-suspended payload at accelerations that exceed the capabilities of previous collaborative approaches, which make quasi-static assumptions. Furthermore, this is achieved completely without explicit communication between the collaborating robots, making our system robust to communication failures and making consensus on a common reference frame unnecessary. Instead, they only rely on visual and inertial cues obtained from on-board sensors. We implement and validate the proposed method on a real system.},
	eventtitle   = {2017 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	date         = {2017-05},
	langid       = {english},
	file         = {PDF:/Users/viktorlorentz/Zotero/storage/L3EVPVKS/Gassner et al. - 2017 - Dynamic collaboration without communication Vision-based cable-suspended load transport with two qu.pdf:application/pdf}
}
@misc{huang_quadswarm_2023,
	title        = {{QuadSwarm}: A Modular Multi-Quadrotor Simulator for Deep Reinforcement Learning with Direct Thrust Control},
	shorttitle   = {{QuadSwarm}},
	author       = {Huang, Zhehui and Batra, Sumeet and Chen, Tao and Krupani, Rahul and Kumar, Tushar and Molchanov, Artem and Petrenko, Aleksei and Preiss, James A. and Yang, Zhaojing and Sukhatme, Gaurav S.},
	doi          = {10.48550/arXiv.2306.09537},
	abstract     = {Reinforcement learning ({RL}) has shown promise in creating robust policies for robotics tasks. However, contemporary {RL} algorithms are data-hungry, often requiring billions of environment transitions to train successful policies. This necessitates the use of fast and highly-parallelizable simulators. In addition to speed, such simulators need to model the physics of the robots and their interaction with the environment to a level acceptable for transferring policies learned in simulation to reality. We present {QuadSwarm}, a fast, reliable simulator for research in single and multi-robot {RL} for quadrotors that addresses both issues. {QuadSwarm}, with fast forward-dynamics propagation decoupled from rendering, is designed to be highly parallelizable such that throughput scales linearly with additional compute. It provides multiple components tailored toward multi-robot {RL}, including diverse training scenarios, and provides domain randomization to facilitate the development and sim2real transfer of multi-quadrotor control policies. Initial experiments suggest that {QuadSwarm} achieves over 48,500 simulation samples per second ({SPS}) on a single quadrotor and over 62,000 {SPS} on eight quadrotors on a 16-core {CPU}. The code can be found in https://github.com/Zhehui-Huang/quad-swarm-rl.},
	date         = {2023-06-15},
	file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/55GZYJQH/Huang et al. - 2023 - QuadSwarm A Modular Multi-Quadrotor Simulator for Deep Reinforcement Learning with Direct Thrust Co.pdf:application/pdf;Snapshot:/Users/viktorlorentz/Zotero/storage/YYRA9SBS/2306.html:text/html}
}
@article{ma2024skilltransfer,
	title        = {Skill Transfer and Discovery for Sim-to-Real Learning: A Representation-Based Viewpoint},
	author       = {Ma, Haitong and Ren, Zhaolin and Dai, Bo and Li, Na},
	year         = 2024,
	journal      = {technical report}
}
@article{estevez_review_2024,
	title        = {Review of Aerial Transportation of Suspended-Cable Payloads with Quadrotors},
	author       = {Estevez, Julian and Garate, Gorka and Lopez-Guede, Jose Manuel and Larrea, Mikel},
	volume       = 8,
	pages        = 35,
	doi          = {10.3390/drones8020035},
	issn         = {2504-446X},
	rights       = {http://creativecommons.org/licenses/by/3.0/},
	abstract     = {Payload transportation and manipulation by rotorcraft drones are receiving a lot of attention from the military, industrial and logistics research areas. The interactions between the {UAV} and the payload, plus the means of object attachment or manipulation (such as cables or anthropomorphic robotic arms), may be nonlinear, introducing difficulties in the overall system performance. In this paper, we focus on the current state of the art of aerial transportation systems with suspended loads by a single {UAV} and a team of them and present a review of different dynamic cable models and control systems. We cover the last sixteen years of the existing literature, and we add a discussion for evaluating the main trends in the referenced research works.},
	journaltitle = {Drones},
	date         = {2024-02},
	langid       = {english},
	file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/RZZDVQHM/Estevez et al. - 2024 - Review of Aerial Transportation of Suspended-Cable Payloads with Quadrotors.pdf:application/pdf}
}
@misc{witt_is_2020,
	title        = {Is Independent Learning All You Need in the {StarCraft} Multi-Agent Challenge?},
	author       = {Witt, Christian Schroeder de and Gupta, Tarun and Makoviichuk, Denys and Makoviychuk, Viktor and Torr, Philip H. S. and Sun, Mingfei and Whiteson, Shimon},
	doi          = {10.48550/arXiv.2011.09533},
	abstract     = {Most recently developed approaches to cooperative multi-agent reinforcement learning in the {\textbackslash}emph\{centralized training with decentralized execution\} setting involve estimating a centralized, joint value function. In this paper, we demonstrate that, despite its various theoretical shortcomings, Independent {PPO} ({IPPO}), a form of independent learning in which each agent simply estimates its local value function, can perform just as well as or better than state-of-the-art joint learning approaches on popular multi-agent benchmark suite {SMAC} with little hyperparameter tuning. We also compare {IPPO} to several variants; the results suggest that {IPPO}'s strong performance may be due to its robustness to some forms of environment non-stationarity.},
	date         = {2020-11-18},
	file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/BSZB2BW8/Witt et al. - 2020 - Is Independent Learning All You Need in the StarCraft Multi-Agent Challenge.pdf:application/pdf;Snapshot:/Users/viktorlorentz/Zotero/storage/S8FWA9FV/2011.html:text/html}
}
@misc{yu_surprising_2022,
	title        = {The Surprising Effectiveness of {PPO} in Cooperative, Multi-Agent Games},
	author       = {Yu, Chao and Velu, Akash and Vinitsky, Eugene and Gao, Jiaxuan and Wang, Yu and Bayen, Alexandre and Wu, Yi},
	doi          = {10.48550/arXiv.2103.01955},
	abstract     = {Proximal Policy Optimization ({PPO}) is a ubiquitous on-policy reinforcement learning algorithm but is significantly less utilized than off-policy learning algorithms in multi-agent settings. This is often due to the belief that {PPO} is significantly less sample efficient than off-policy methods in multi-agent systems. In this work, we carefully study the performance of {PPO} in cooperative multi-agent settings. We show that {PPO}-based multi-agent algorithms achieve surprisingly strong performance in four popular multi-agent testbeds: the particle-world environments, the {StarCraft} multi-agent challenge, Google Research Football, and the Hanabi challenge, with minimal hyperparameter tuning and without any domain-specific algorithmic modifications or architectures. Importantly, compared to competitive off-policy methods, {PPO} often achieves competitive or superior results in both final returns and sample efficiency. Finally, through ablation studies, we analyze implementation and hyperparameter factors that are critical to {PPO}'s empirical performance, and give concrete practical suggestions regarding these factors. Our results show that when using these practices, simple {PPO}-based methods can be a strong baseline in cooperative multi-agent reinforcement learning. Source code is released at {\textbackslash}url\{https://github.com/marlbenchmark/on-policy\}.},
	date         = {2022-11-04},
	file         = {Full Text PDF:/Users/viktorlorentz/Zotero/storage/YT2YMAEN/Yu et al. - 2022 - The Surprising Effectiveness of PPO in Cooperative, Multi-Agent Games.pdf:application/pdf;Snapshot:/Users/viktorlorentz/Zotero/storage/24AZWNBS/2103.html:text/html}
}
@book{oliehoek_concise_2016,
	title        = {A Concise Introduction to Decentralized {POMDPs}},
	author       = {Oliehoek, Frans A. and Amato, Christopher},
	location     = {Cham},
	series       = {{SpringerBriefs} in Intelligent Systems},
	doi          = {10.1007/978-3-319-28929-8},
	isbn         = {978-3-319-28927-4 978-3-319-28929-8},
	rights       = {http://www.springer.com/tdm},
	date         = 2016,
	langid       = {english},
	file         = {PDF:/Users/viktorlorentz/Zotero/storage/Y95CGILI/Oliehoek and Amato - 2016 - A Concise Introduction to Decentralized POMDPs.pdf:application/pdf}
}
@software{gymnax2022github,
	title        = {{gymnax}: A {JAX}-based Reinforcement Learning Environment Library},
	author       = {Robert Tjarko Lange},
	year         = 2022,
	version      = {0.0.4}
}
@inproceedings{goodarzi_dynamics_2015,
	title        = {Dynamics and control of quadrotor {UAVs} transporting a rigid body connected via flexible cables},
	author       = {Goodarzi, Farhad A. and Lee, Taeyoung},
	booktitle    = {2015 American Control Conference ({ACC})},
	pages        = {4677--4682},
	doi          = {10.1109/ACC.2015.7172066},
	abstract     = {This paper is focused on the dynamics and control of arbitrary number of quadrotor {UAVs} transporting a rigid body payload. The rigid body payload is connected to quadrotors via flexible cables where each flexible cable is modeled as a system of serially-connected links. It is shown that a coordinate-free form of equations of motion can be derived for arbitrary numbers of quadrotors and links according to Lagrangian mechanics on a manifold. A geometric nonlinear controller is presented to transport the rigid body to a fixed desired position while aligning all of the links along the vertical direction. Numerical results are provided to illustrate the desirable features of the proposed control system.},
	eventtitle   = {2015 American Control Conference ({ACC})},
	date         = {2015-07},
	file         = {Snapshot:/Users/viktorlorentz/Zotero/storage/SW9FQLBI/7172066.html:text/html;Submitted Version:/Users/viktorlorentz/Zotero/storage/QMHG5P44/Goodarzi and Lee - 2015 - Dynamics and control of quadrotor UAVs transporting a rigid body connected via flexible cables.pdf:application/pdf}
}
@inproceedings{todorov_mujoco_2012,
	title        = {{MuJoCo}: A physics engine for model-based control},
	shorttitle   = {{MuJoCo}},
	author       = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
	booktitle    = {2012 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems},
	pages        = {5026--5033},
	doi          = {10.1109/IROS.2012.6386109},
	abstract     = {We describe a new physics engine tailored to model-based control. Multi-joint dynamics are represented in generalized coordinates and computed via recursive algorithms. Contact responses are computed via efficient new algorithms we have developed, based on the modern velocity-stepping approach which avoids the difficulties with spring-dampers. Models are specified using either a high-level C++ {API} or an intuitive {XML} file format. A built-in compiler transforms the user model into an optimized data structure used for runtime computation. The engine can compute both forward and inverse dynamics. The latter are well-defined even in the presence of contacts and equality constraints. The model can include tendon wrapping as well as actuator activation states (e.g. pneumatic cylinders or muscles). To facilitate optimal control applications and in particular sampling and finite differencing, the dynamics can be evaluated for different states and controls in parallel. Around 400,000 dynamics evaluations per second are possible on a 12-core machine, for a 3D homanoid with 18 dofs and 6 active contacts. We have already used the engine in a number of control applications. It will soon be made publicly available.},
	eventtitle   = {2012 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems},
	date         = {2012-10}
}
@article{Idrissi2022AROA,
	title        = {A Review of Quadrotor Unmanned Aerial Vehicles: Applications, Architectural Design and Control Algorithms},
	author       = {Moad Idrissi and M. Salami and F. Annaz},
	year         = 2022,
	journal      = {Journal of Intelligent \& Robotic Systems},
	volume       = 104,
	pages        = {1--33}
}
@article{Lyu2023UnmannedAVA,
	title        = {Unmanned Aerial Vehicles for Search and Rescue: A Survey},
	author       = {Mingyang Lyu and Yibo Zhao and Chao Huang and Hailong Huang},
	year         = 2023,
	journal      = {Remote. Sens.},
	volume       = 15,
	pages        = 3266
}
@article{Chen2021FromUSA,
	title        = {From Unmanned Systems to Autonomous Intelligent Systems},
	author       = {Jie Chen and Jian Sun and Gang Wang},
	year         = 2021,
	journal      = {Engineering}
}
@inproceedings{Abbaraju2018SensingASA,
	title        = {Sensing and sampling of trace contaminations by a dexterous hexrotor UAV at nuclear facilities-18600},
	author       = {Abbaraju, Praveen and Voyles, Richard and others},
	year         = 2018,
	booktitle    = {Proceedings of the WM2018 Symposium Conference, Phoenix, AZ, USA},
	pages        = {18--22}
}
@article{Hwangbo2017ControlOAA,
	title        = {Control of a Quadrotor With Reinforcement Learning},
	author       = {Jemin Hwangbo and Inkyu Sa and R. Siegwart and Marco Hutter},
	year         = 2017,
	journal      = {IEEE Robotics and Automation Letters},
	volume       = 2,
	pages        = {2096--2103}
}
@article{Lin2024PayloadTW,
	title        = {Payload Transporting With Two Quadrotors by Centralized Reinforcement Learning Method},
	author       = {Dasheng Lin and Jianda Han and Kun Li and Jianlei Zhang and Chun-yan Zhang},
	year         = 2024,
	journal      = {IEEE Transactions on Aerospace and Electronic Systems},
	volume       = 60,
	pages        = {239--251}
}
@article{Estevez2024Reinforcement,
	title        = {Reinforcement Learning Based Trajectory Planning for Multi-UAV Load Transportation},
	author       = {Estevez, Julian and Manuel Lopez-Guede, Jose and del Valle-Echavarri, Javier and Graña, Manuel},
	year         = 2024,
	journal      = {IEEE Access},
	volume       = 12,
	pages        = {144009--144016},
	doi          = {10.1109/ACCESS.2024.3470509}
}
@article{Dimmig2023SurveyOS,
	title        = {Survey of Simulators for Aerial Robots},
	author       = {Cora A. Dimmig and Giuseppe Silano and Kimberly McGuire and Chiara Gabellieri and Wolfgang H{\"o}nig and Joseph L. Moore and Marin Kobilarov},
	year         = 2023,
	journal      = {ArXiv},
	volume       = {abs/2311.02296}
}
@article{Li2022RotorTMAF,
	title        = {RotorTM: A Flexible Simulator for Aerial Transportation and Manipulation},
	author       = {Guanrui Li and Xinyang Liu and Giuseppe Loianno},
	year         = 2022,
	journal      = {IEEE Transactions on Robotics},
	volume       = 40,
	pages        = {831--850}
}
@article{aerial_gym_simulator,
	title        = {Aerial Gym Simulator: A Framework for Highly Parallelized Simulation of Aerial Robots},
	author       = {Kulkarni, Mihir and Rehberg, Welf and Alexis, Kostas},
	year         = 2025,
	journal      = {IEEE Robotics and Automation Letters},
	volume       = 10,
	pages        = {4093--4100},
	doi          = {10.1109/LRA.2025.3548507}
}
@article{Song2023ReachingTL,
	title        = {Reaching the limit in autonomous racing: Optimal control versus reinforcement learning},
	author       = {Yunlong Song and Angel Romero and Matthias M{\"u}ller and Vladlen Koltun and Davide Scaramuzza},
	year         = 2023,
	journal      = {Science Robotics},
	volume       = 8
}
@inproceedings{Papoudakis2020BenchmarkingMD,
	title        = {Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Tasks},
	author       = {Georgios Papoudakis and Filippos Christianos and Lukas Sch{\"a}fer and Stefano V. Albrecht},
	year         = 2020,
	booktitle    = {NeurIPS Datasets and Benchmarks}
}
@article{Lowe2017MultiAgentAF,
	title        = {Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},
	author       = {Ryan Lowe and Yi Wu and Aviv Tamar and Jean Harb and P. Abbeel and Igor Mordatch},
	year         = 2017,
	journal      = {ArXiv},
	volume       = {abs/1706.02275}
}
@article{Chen2022TowardsHBA,
	title        = {Towards Human-Level Bimanual Dexterous Manipulation with Reinforcement Learning},
	author       = {Yuanpei Chen and Yaodong Yang and Tianhao Wu and Shengjie Wang and Xidong Feng and Jiechuan Jiang and S. McAleer and Yiran Geng and Hao Dong and Zongqing Lu and Song-Chun Zhu},
	year         = 2022,
	journal      = {ArXiv},
	volume       = {abs/2206.08686}
}
@inproceedings{Pandit2024LearningDM,
	title        = {Learning Decentralized Multi-Biped Control for Payload Transport},
	author       = {Bikram Pandit and Ashutosh Gupta and Mohitvishnu S. Gadde and Addison Johnson and Aayam Shrestha and Helei Duan and Jeremy Dao and Alan Fern},
	year         = 2024,
	booktitle    = {Conference on Robot Learning}
}
@article{Ji2021ReinforcementLF,
	title        = {Reinforcement Learning for Collaborative Quadrupedal Manipulation of a Payload over Challenging Terrain},
	author       = {Yandong Ji and Bike Zhang and Koushil Sreenath},
	year         = 2021,
	journal      = {2021 IEEE 17th International Conference on Automation Science and Engineering (CASE)},
	pages        = {899--904}
}
@article{Chen2025DecentralizedNO,
	title        = {Decentralized Navigation of a Cable-Towed Load using Quadrupedal Robot Team via MARL},
	author       = {Wen-Tse Chen and Minh Nguyen and Zhongyu Li and Guo Ning Sue and Koushil Sreenath},
	year         = 2025,
	journal      = {ArXiv},
	volume       = {abs/2503.18221}
}
@article{Wang2025SafeAA,
	title        = {Safe and Agile Transportation of Cable-Suspended Payload via Multiple Aerial Robots},
	author       = {Yongchao Wang and Junjie Wang and Xiaobin Zhou and Tiankai Yang and Chao Xu and Fei Gao},
	year         = 2025,
	journal      = {ArXiv},
	volume       = {abs/2501.15272}
}
@article{Koch2018ReinforcementLF,
	title        = {Reinforcement Learning for UAV Attitude Control},
	author       = {William Koch and Renato Mancuso and Richard West and Azer Bestavros},
	year         = 2018,
	journal      = {ACM Transactions on Cyber-Physical Systems},
	volume       = 3,
	pages        = {1--21}
}
@article{Hwangbo2017ControlOA,
	title        = {Control of a Quadrotor With Reinforcement Learning},
	author       = {Jemin Hwangbo and Inkyu Sa and Roland Y. Siegwart and Marco Hutter},
	year         = 2017,
	journal      = {IEEE Robotics and Automation Letters},
	volume       = 2,
	pages        = {2096--2103}
}
@article{Zhou2020EGOSwarmAF,
	title        = {EGO-Swarm: A Fully Autonomous and Decentralized Quadrotor Swarm System in Cluttered Environments},
	author       = {Xiaoxia Zhou and Jiangchao Zhu and Hongyu Zhou and Chao Xu and Fei Gao},
	year         = 2020,
	journal      = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
	pages        = {4101--4107}
}
@incollection{littman1994markov,
	title        = {Markov games as a framework for multi-agent reinforcement learning},
	author       = {Littman, Michael L},
	year         = 1994,
	booktitle    = {Machine learning proceedings 1994},
	pages        = {157--163}
}
@article{Lillicrap2015ContinuousCW,
	title        = {Continuous control with deep reinforcement learning},
	author       = {Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Manfred Otto Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
	year         = 2015,
	journal      = {CoRR},
	volume       = {abs/1509.02971}
}
@article{Schulman2015TrustRP,
	title        = {Trust Region Policy Optimization},
	author       = {John Schulman and Sergey Levine and P. Abbeel and Michael I. Jordan and Philipp Moritz},
	year         = 2015,
	journal      = {ArXiv},
	volume       = {abs/1502.05477}
}
@article{mnih2015human,
	title        = {Human-level control through deep reinforcement learning},
	author       = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
	year         = 2015,
	journal      = {nature},
	volume       = 518,
	pages        = {529--533}
}
@software{jax2018github,
	title        = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
	author       = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
	year         = 2018,
	version      = {0.3.13},
	url = {http://github.com/jax-ml/jax}
}
@inproceedings{todorov2012mujoco,
	title        = {Mujoco: A physics engine for model-based control},
	author       = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
	year         = 2012,
	booktitle    = {2012 IEEE/RSJ international conference on intelligent robots and systems},
	pages        = {5026--5033}
}
@article{Wang2025SafeAA,
	title        = {Safe and Agile Transportation of Cable-Suspended Payload via Multiple Aerial Robots},
	author       = {Yongchao Wang and Junjie Wang and Xiaobin Zhou and Tiankai Yang and Chao Xu and Fei Gao},
	year         = 2025,
	journal      = {ArXiv},
	volume       = {abs/2501.15272}
}
@article{Koch2018ReinforcementLF,
	title        = {Reinforcement Learning for UAV Attitude Control},
	author       = {William Koch and Renato Mancuso and Richard West and Azer Bestavros},
	year         = 2018,
	journal      = {ACM Transactions on Cyber-Physical Systems},
	volume       = 3,
	pages        = {1--21}
}
@article{Hwangbo2017ControlOA,
	title        = {Control of a Quadrotor With Reinforcement Learning},
	author       = {Jemin Hwangbo and Inkyu Sa and Roland Y. Siegwart and Marco Hutter},
	year         = 2017,
	journal      = {IEEE Robotics and Automation Letters},
	volume       = 2,
	pages        = {2096--2103}
}
@article{Zhou2020EGOSwarmAF,
	title        = {EGO-Swarm: A Fully Autonomous and Decentralized Quadrotor Swarm System in Cluttered Environments},
	author       = {Xiaoxia Zhou and Jiangchao Zhu and Hongyu Zhou and Chao Xu and Fei Gao},
	year         = 2020,
	journal      = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
	pages        = {4101--4107}
}
@incollection{littman1994markov,
	title        = {Markov games as a framework for multi-agent reinforcement learning},
	author       = {Littman, Michael L},
	year         = 1994,
	booktitle    = {Machine learning proceedings 1994},
	pages        = {157--163}
}
@article{Lillicrap2015ContinuousCW,
	title        = {Continuous control with deep reinforcement learning},
	author       = {Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Manfred Otto Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
	year         = 2015,
	journal      = {CoRR},
	volume       = {abs/1509.02971}
}
@article{Schulman2015TrustRP,
	title        = {Trust Region Policy Optimization},
	author       = {John Schulman and Sergey Levine and P. Abbeel and Michael I. Jordan and Philipp Moritz},
	year         = 2015,
	journal      = {ArXiv},
	volume       = {abs/1502.05477}
}
@article{mnih2015human,
	title        = {Human-level control through deep reinforcement learning},
	author       = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
	year         = 2015,
	journal      = {nature},
	volume       = 518,
	pages        = {529--533}
}
@software{jax2018github,
	title        = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
	author       = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
	year         = 2018,
	version      = {0.3.13}
}
@inproceedings{todorov2012mujoco,
	title        = {Mujoco: A physics engine for model-based control},
	author       = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
	year         = 2012,
	booktitle    = {2012 IEEE/RSJ international conference on intelligent robots and systems},
	pages        = {5026--5033}
}
@article{Lyu2023UnmannedAVA,
	title        = {Unmanned Aerial Vehicles for Search and Rescue: A Survey},
	author       = {Mingyang Lyu and Yibo Zhao and Chao Huang and Hailong Huang},
	year         = 2023,
	journal      = {Remote. Sens.},
	volume       = 15,
	pages        = 3266
}
@article{Chen2021FromUSA,
	title        = {From Unmanned Systems to Autonomous Intelligent Systems},
	author       = {Jie Chen and Jian Sun and Gang Wang},
	year         = 2021,
	journal      = {Engineering}
}
@inproceedings{Abbaraju2018SensingASA,
	title        = {Sensing and Sampling of Trace Contaminations by a Dexterous Hexrotor UAV at Nuclear Facilities-18600},
	author       = {Praveen Abbaraju and R. Voyles},
	year         = 2018,
	booktitle    = {unknown}
}
@article{Hwangbo2017ControlOAA,
	title        = {Control of a Quadrotor With Reinforcement Learning},
	author       = {Jemin Hwangbo and Inkyu Sa and R. Siegwart and Marco Hutter},
	year         = 2017,
	journal      = {IEEE Robotics and Automation Letters},
	volume       = 2,
	pages        = {2096--2103}
}
@article{Lin2024PayloadTW,
	title        = {Payload Transporting With Two Quadrotors by Centralized Reinforcement Learning Method},
	author       = {Dasheng Lin and Jianda Han and Kun Li and Jianlei Zhang and Chun-yan Zhang},
	year         = 2024,
	journal      = {IEEE Transactions on Aerospace and Electronic Systems},
	volume       = 60,
	pages        = {239--251}
}
@article{Estevez2024Reinforcement,
	title        = {Reinforcement Learning Based Trajectory Planning for Multi-UAV Load Transportation},
	author       = {Estevez, Julian and Manuel Lopez-Guede, Jose and del Valle-Echavarri, Javier and Graña, Manuel},
	year         = 2024,
	journal      = {IEEE Access},
	volume       = 12,
	pages        = {144009--144016},
	doi          = {10.1109/ACCESS.2024.3470509}
}
@article{Dimmig2023SurveyOS,
	title        = {Survey of Simulators for Aerial Robots},
	author       = {Cora A. Dimmig and Giuseppe Silano and Kimberly McGuire and Chiara Gabellieri and Wolfgang H{\"o}nig and Joseph L. Moore and Marin Kobilarov},
	year         = 2023,
	journal      = {ArXiv},
	volume       = {abs/2311.02296}
}
@article{Li2022RotorTMAF,
	title        = {RotorTM: A Flexible Simulator for Aerial Transportation and Manipulation},
	author       = {Guanrui Li and Xinyang Liu and Giuseppe Loianno},
	year         = 2022,
	journal      = {IEEE Transactions on Robotics},
	volume       = 40,
	pages        = {831--850}
}
@article{aerial_gym_simulator,
	title        = {Aerial Gym Simulator: A Framework for Highly Parallelized Simulation of Aerial Robots},
	author       = {Kulkarni, Mihir and Rehberg, Welf and Alexis, Kostas},
	year         = 2025,
	journal      = {IEEE Robotics and Automation Letters},
	volume       = 10,
	pages        = {4093--4100},
	doi          = {10.1109/LRA.2025.3548507}
}
@article{Song2023ReachingTL,
	title        = {Reaching the limit in autonomous racing: Optimal control versus reinforcement learning},
	author       = {Yunlong Song and Angel Romero and Matthias M{\"u}ller and Vladlen Koltun and Davide Scaramuzza},
	year         = 2023,
	journal      = {Science Robotics},
	volume       = 8
}
@inproceedings{Papoudakis2020BenchmarkingMD,
	title        = {Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Tasks},
	author       = {Georgios Papoudakis and Filippos Christianos and Lukas Sch{\"a}fer and Stefano V. Albrecht},
	year         = 2020,
	booktitle    = {NeurIPS Datasets and Benchmarks}
}
@article{Lowe2017MultiAgentAF,
	title        = {Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},
	author       = {Ryan Lowe and Yi Wu and Aviv Tamar and Jean Harb and P. Abbeel and Igor Mordatch},
	year         = 2017,
	journal      = {ArXiv},
	volume       = {abs/1706.02275}
}
@article{Chen2022TowardsHBA,
	title        = {Towards Human-Level Bimanual Dexterous Manipulation with Reinforcement Learning},
	author       = {Yuanpei Chen and Yaodong Yang and Tianhao Wu and Shengjie Wang and Xidong Feng and Jiechuan Jiang and S. McAleer and Yiran Geng and Hao Dong and Zongqing Lu and Song-Chun Zhu},
	year         = 2022,
	journal      = {ArXiv},
	volume       = {abs/2206.08686}
}
@inproceedings{Pandit2024LearningDM,
	title        = {Learning Decentralized Multi-Biped Control for Payload Transport},
	author       = {Bikram Pandit and Ashutosh Gupta and Mohitvishnu S. Gadde and Addison Johnson and Aayam Shrestha and Helei Duan and Jeremy Dao and Alan Fern},
	year         = 2024,
	booktitle    = {Conference on Robot Learning}
}
@article{Ji2021ReinforcementLF,
	title        = {Reinforcement Learning for Collaborative Quadrupedal Manipulation of a Payload over Challenging Terrain},
	author       = {Yandong Ji and Bike Zhang and Koushil Sreenath},
	year         = 2021,
	journal      = {2021 IEEE 17th International Conference on Automation Science and Engineering (CASE)},
	pages        = {899--904}
}
@article{Chen2025DecentralizedNO,
	title        = {Decentralized Navigation of a Cable-Towed Load using Quadrupedal Robot Team via MARL},
	author       = {Wen-Tse Chen and Minh Nguyen and Zhongyu Li and Guo Ning Sue and Koushil Sreenath},
	year         = 2025,
	journal      = {ArXiv},
	volume       = {abs/2503.18221}
}
@article{Wang2025SafeAA,
	title        = {Safe and Agile Transportation of Cable-Suspended Payload via Multiple Aerial Robots},
	author       = {Yongchao Wang and Junjie Wang and Xiaobin Zhou and Tiankai Yang and Chao Xu and Fei Gao},
	year         = 2025,
	journal      = {ArXiv},
	volume       = {abs/2501.15272}
}
@article{Koch2018ReinforcementLF,
	title        = {Reinforcement Learning for UAV Attitude Control},
	author       = {William Koch and Renato Mancuso and Richard West and Azer Bestavros},
	year         = 2018,
	journal      = {ACM Transactions on Cyber-Physical Systems},
	volume       = 3,
	pages        = {1--21}
}
@article{Hwangbo2017ControlOA,
	title        = {Control of a Quadrotor With Reinforcement Learning},
	author       = {Jemin Hwangbo and Inkyu Sa and Roland Y. Siegwart and Marco Hutter},
	year         = 2017,
	journal      = {IEEE Robotics and Automation Letters},
	volume       = 2,
	pages        = {2096--2103}
}
@article{Zhou2020EGOSwarmAF,
	title        = {EGO-Swarm: A Fully Autonomous and Decentralized Quadrotor Swarm System in Cluttered Environments},
	author       = {Xiaoxia Zhou and Jiangchao Zhu and Hongyu Zhou and Chao Xu and Fei Gao},
	year         = 2020,
	journal      = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
	pages        = {4101--4107}
}
@incollection{littman1994markov,
	title        = {Markov games as a framework for multi-agent reinforcement learning},
	author       = {Littman, Michael L},
	year         = 1994,
	booktitle    = {Machine learning proceedings 1994},
	pages        = {157--163}
}
@article{Lillicrap2015ContinuousCW,
	title        = {Continuous control with deep reinforcement learning},
	author       = {Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Manfred Otto Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
	year         = 2015,
	journal      = {CoRR},
	volume       = {abs/1509.02971}
}
@article{Schulman2015TrustRP,
	title        = {Trust Region Policy Optimization},
	author       = {John Schulman and Sergey Levine and P. Abbeel and Michael I. Jordan and Philipp Moritz},
	year         = 2015,
	journal      = {ArXiv},
	volume       = {abs/1502.05477}
}
@article{mnih2015human,
	title        = {Human-level control through deep reinforcement learning},
	author       = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
	year         = 2015,
	journal      = {nature},
	volume       = 518,
	pages        = {529--533}
}
@software{jax2018github,
	title        = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
	author       = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
	year         = 2018,
	version      = {0.3.13}
}
@inproceedings{todorov2012mujoco,
	title        = {Mujoco: A physics engine for model-based control},
	author       = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
	year         = 2012,
	booktitle    = {2012 IEEE/RSJ international conference on intelligent robots and systems},
	pages        = {5026--5033}
}
